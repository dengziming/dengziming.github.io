<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>设计高并发架构 on 数据分析师之旅</title>
    <link>https://dengziming.github.io/categories/%E8%AE%BE%E8%AE%A1%E9%AB%98%E5%B9%B6%E5%8F%91%E6%9E%B6%E6%9E%84/</link>
    <description>Recent content in 设计高并发架构 on 数据分析师之旅</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Mon, 31 Dec 2018 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://dengziming.github.io/categories/%E8%AE%BE%E8%AE%A1%E9%AB%98%E5%B9%B6%E5%8F%91%E6%9E%B6%E6%9E%84/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>分布式存储-replication</title>
      <link>https://dengziming.github.io/post/data-intensive-architecture/replication/</link>
      <pubDate>Mon, 31 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/data-intensive-architecture/replication/</guid>
      
        <description>

&lt;h1 id=&#34;一-replication&#34;&gt;一、Replication&lt;/h1&gt;

&lt;p&gt;Replication 就是将你的数据放在多个节点，这样有很多好处。数据从地理上可以隔用户更近，保持高可用，增加吞吐量。&lt;/p&gt;

&lt;p&gt;本章我们假设数据都是完整的进行副本存放，后面再讲分区。数据的副本解决难点就是数据变化的一致性，如果数据一致不变，那就太简单了，没什么可以讨论的。&lt;/p&gt;

&lt;p&gt;对于数据变化的副本存储，有很多需要注意的地方，例如每次写数据操作数据复制是同步的还是异步的，节点挂了怎么办，数据一致性问题，时效性问题。
我们的学习主要分为三种设计：single-leader, multi-leader, and leaderless&lt;/p&gt;

&lt;h2 id=&#34;1-leaders-and-followers&#34;&gt;1. Leaders and Followers&lt;/h2&gt;

&lt;p&gt;每个节点上面的副本叫做 replica ，当有 multi replica 的时候，一个问题出现了，怎么保证数据写到了每一个副本。
数据库的每一次写都必须保证被每个副本处理，否则就会出现不一致。
最常用的解决方案就是 leader-based-replication，也叫 active-passive、master-slave。&lt;/p&gt;

&lt;p&gt;在主从结构中，写只能发送给 leader，leader 写的时候会发送数据给 follower，而读操作可以读任何一个节点。&lt;/p&gt;

&lt;h3 id=&#34;1-synchronous-versus-asynchronous-replication&#34;&gt;(1) Synchronous Versus Asynchronous Replication&lt;/h3&gt;

&lt;p&gt;replicated 系统重要特点就是 同步写还是异步写，一般关系型数据库可以配置，其他的都是硬编码写死的。&lt;/p&gt;

&lt;p&gt;客户端发送数据给leader，然后leader转发给follower，最后通知client。如果 leader 收到 follower 的确认消息再回复client，这就是同步。
如果leader发送给了 follower后就直接回复client，就是异步。一般情况，系统都只有一个follower是同步，其余的都是异步。这叫 semi-synchronous。&lt;/p&gt;

&lt;p&gt;为了效率经常设置为 Asynchronous，这样如何 leader 失败了而且不恢复，那还没有被副本保存的数据就丢失了，
这样的好处就是可以提供持续服务，尽管follower很慢。即便是配置使用 Synchronous，实际上也是有一台是同步，其余的是异步。&lt;/p&gt;

&lt;p&gt;Weakening durability 可能是一个比较糟糕的 trade-off，但是 Asynchronous 还是很常用，尤其是地理上分布式的集群。&lt;/p&gt;

&lt;p&gt;当然也有很多其他的方案在研究中，例如 Azure 使用了 chain replication。&lt;/p&gt;

&lt;h3 id=&#34;2-setting-up-new-followers&#34;&gt;(2) Setting Up New Followers&lt;/h3&gt;

&lt;p&gt;系统有时候需要添加新的节点，如何保证新节点和leader的数据一致呢？直接复制数据过来肯定是不行的，leader 一直处在 flux 中，一般步骤有四步：
1. leader 在某个确定一致性的时间点拍一个 snapshot 而不用锁住系统，大部分数据库都提供了这个功能，有的能通过第三方工具提供这个功能。
2. follower 将数据复制过去。
3. follower 将从这个时间点以后的数据复制过去，这需要拍 snapshot 时候的日志位置，这个点的名字叫 Log sequence number 或者 binlog coordinates。
4. 当 follower 将所有的change都同步后，我们称之为 catch up，就可以和其他follower一样，处理 leader 发生的变化了。&lt;/p&gt;

&lt;p&gt;真正的步骤其实是有很多不同，有时候可以自动配置，有时候需要手动配置。&lt;/p&gt;

&lt;h3 id=&#34;3-handling-node-outages&#34;&gt;(3) Handling Node Outages&lt;/h3&gt;

&lt;p&gt;有时候节点会挂掉，可能是我们不知道的原因，也可能是为了安装系统模块而重启，这时候应该怎么保证集群的高可用&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;follower 挂掉&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;follower 挂点然后重启的步骤是很简单的，类似前面的添加新节点，启动以后，根据日志能找到最后一个处理的事务，然后从leader请求这个时间点以后的更改，然后进行更改，完成后就 catch up。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;leader failover&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;leader 挂掉后可能麻烦一点，必须马上选择一名新的leader，这个过程被称为 failover&lt;/p&gt;

&lt;p&gt;failover的具体步骤有4：
1. 确定leader失败了，失败的原因很多，磁盘、电源等等，目前用的最多的是 timeout 方法，节点相互 bounce message，如果突然在某个时间点收不到，就是失败了。
2. 选择新的 leader，需要在大多数的follwer 进行选举，最好是选一个数据最新的。所有的节点同意这个节点成为 consensus。
3. 告知系统使用新的leader，client 发送数据到新的 leader，旧的 leader 回复后也要认同新的 leader。&lt;/p&gt;

&lt;p&gt;failover 总是会有些不期而遇的问题：
1. 如果是 asynchronous replica ，可能遇到一些数据没有同步过来，导致数据少了，如果这时候 older leader起来了，就会发生错误，这时候一般选择 discard 这部分数据。
2. 如果系统和别的系统进行结合，discard 是很危险的，github 发生了一次 mysql leader挂掉，然后discard 了一部分，删掉了部分自增的primary key，可是这部分pk已经保存到 redis了，导致了错误
3. 有时候两个节点都认为自己是 leader，这个叫做 split brain，这时候一半需要去掉一个，但是有可能两个都被去掉了。
4. timeout 的具体值多少合适？如果太长了，可能导致发现的晚。如果太短了可能误判，尤其是系统压力大的时候，还重新换一次会造成更大的压力。&lt;/p&gt;

&lt;p&gt;这些问题解决起来还是很棘手的，所以有很多算法，我们后续会介绍。&lt;/p&gt;

&lt;h3 id=&#34;4-implementation-of-replication-logs&#34;&gt;(4) Implementation of Replication Logs&lt;/h3&gt;

&lt;p&gt;上面各种恢复，添加，都涉及到日志，日志记录了数据的更改，实际上我们学习 Hadoop 的时候，也知道里面有日志，现在我们来研究一下，日志怎么实现。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Statement-based replication&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;基于 statement 的 replication，这是最简单的方法，数据库记录每一次更新，也就是说，你的 UPDATE、INSERT、DELETE都会记录到日志中，然后发送给follower。
这样的问题是：
对于 current_timestamp、random 这样的方法返回值不一样，基于判断的例如 update &amp;hellip; where &amp;hellip; 必须按照顺序，一些触发器、存储过程等 side effects。&lt;/p&gt;

&lt;p&gt;对于这些问题，当然也有特定的解决方案，例如先计算一些可能导致不一致的函数。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;WAL-log&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;前面讲过 B-Tree 和 LSM-Tree，都是基于WAL-log，是一种只能 append 的二进制日志，leader 不仅将 wal-log 保存到磁盘，而且发送到 followers。&lt;/p&gt;

&lt;p&gt;WAL 的 disadvantage 是太底层，wal 包含了磁盘那个位置的 block 需要修改哪些 bytes，这样耦合性比较高，如果format 改变了，基本上很难让 leader和follower 使用不同版本。&lt;/p&gt;

&lt;p&gt;这个问题看起来更像是实现的问题，如果 replication 的协议支持follower 使用新的版本，就能完成 zore-downtime 的 upgrade。
首先更新 follower的版本，然后让leader failover，选择一个 follower 为 leader。如果replication 的协议不支持版本不一直，这就叫做 wal-shipping，会造成 upgrade-downtime。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Logical (row-based) log replication&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这种方式就是 replication log 和具体的 storage device 解耦，使用不同格式。 relational database 的 logical log 一般都是一系列表示行的recorders。一个改变很多行的 statement 会
产生很多行这样的日志，后面有一个transaction 完成的标识符。这种方式是解耦的，所以可以容忍不同的版本，甚至不同的存储引擎。这也让外部的系统容易获得数据，
例如数仓、二级索引，外部缓存，这个技术叫做 change data capture。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;trigger based replication&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;前面讲的都是数据库系统的，有时候我们需要更灵活的，例如你只想一部分数据，或者你想从一个数据库迁移到另一个数据库。这样你就应该将replication 提升到应用程序层面。
很多数据库提供了工具完成这个事情。许多关系型数据库提供了一项功能：triggers and stored procedures。
trigger 能让你注册用户代码，一旦数据发生改变，将会将数据放到另一给表格，外部系统可以访问。&lt;/p&gt;

&lt;h3 id=&#34;5-problems-with-replication-lag&#34;&gt;(5) Problems with Replication Lag&lt;/h3&gt;

&lt;p&gt;之前说了 Replication 是保证数据安全、忍受 node failures，实际上还可以提高 scalability。&lt;/p&gt;

&lt;p&gt;Leader-based 需要所有的 write 请求都通过 master，将 read 分发到 slave，增加节点就能增加扩展性，
节点太多了就只能使用 异步的数据同步方式，否则可用性就很差。&lt;/p&gt;

&lt;p&gt;然而一旦 slave 的数据 fallen behind 了，可能访问的数据过时了，这看起来问题不大，毕竟等等就行，这叫做  eventual consistency。&lt;/p&gt;

&lt;p&gt;这个 eventually 是一个很NB的词，并没有告诉你具体时间，只是说最终会一致。我们称时间差为 the replication lag。这个 lag导致的问题我们接下来会讨论。&lt;/p&gt;

&lt;h4 id=&#34;1-read-your-own-writes&#34;&gt;1. Read your own writes&lt;/h4&gt;

&lt;p&gt;之前的 qq 空间有这个问题，当你发表了评论后页面会刷新，但是刷新后看不到自己发表的评论。
这时候你以为评论失败了，所以重新评论一下，然后刷新发现了自己评论了两次。&lt;/p&gt;

&lt;p&gt;这种问题被叫做 “read-after-write consistency”，也叫做 “read-your-writes consistency”。有一些解决方案：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;如果是用户可以修改的信息，从 master 读。&lt;/li&gt;
&lt;li&gt;如果是一分钟之内修改过的数据，从 master 读，或者可以监控 lag 大于1分钟的follow 排除。&lt;/li&gt;
&lt;li&gt;客户端保存最近一次写的 timestamp，这样服务端保证读的时候先等 slave 的数据同步已经到了这个时间。这里的 timestamp 可以是逻辑时间。&lt;/li&gt;
&lt;li&gt;如果有多个数据中心，可能更复杂。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;上面的解决方案只是一个 client，如果用户同事有多个 client 访问，例如手机和pc，这时候要考虑更多问题。&lt;/p&gt;

&lt;h4 id=&#34;2-monotonic-reads&#34;&gt;2. Monotonic Reads&lt;/h4&gt;

&lt;p&gt;类似上面，如果连续两次访问来自于不同的 slave，可能出现时间倒退，例如看球赛的时候刚开始比分 1:1,刷新一下变成 1:0 了！&lt;/p&gt;

&lt;p&gt;Monotonic Reads 就是保证上面的异常不发生的一致性。他比 eventually consistency 更强，比 strong consistency 更弱。&lt;/p&gt;

&lt;p&gt;这种一致性的解决方案就是保证每个 userid 都从同一个 slave 读数据，例如根据 userid 的 hash。&lt;/p&gt;

&lt;h4 id=&#34;3-consistent-prefix-reads&#34;&gt;3. Consistent Prefix Reads&lt;/h4&gt;

&lt;p&gt;在一个群聊系统，如果你发现两个人的对话顺序反了，就是 Consistent Prefix Reads 没有得到保证。&lt;/p&gt;

&lt;p&gt;这个问题解决方案需要保证有相互依赖的数据都按顺序写入。后续再  causal dependencies 和 happens-before 会讨论。&lt;/p&gt;

&lt;h3 id=&#34;6-solutions-for-replication-lag&#34;&gt;(6). Solutions for Replication Lag&lt;/h3&gt;

&lt;p&gt;后续会讲解 事务 和 分布式事务。&lt;/p&gt;

&lt;h2 id=&#34;2-multi-leader-replication&#34;&gt;2. Multi-Leader Replication&lt;/h2&gt;

&lt;p&gt;一个 leader 可能会有一些问题，例如写太多了可能会有性能问题。也有一些其他的选择， 例如多个 leader 。&lt;/p&gt;

&lt;h3 id=&#34;1-use-cases-for-multi-leader-replication&#34;&gt;(1). “Use Cases for Multi-Leader Replication”&lt;/h3&gt;

&lt;p&gt;一般情况如果只有一个 datacenter 是不需要用多个 leader 的，所以在多个 datacenter 的时候会使用 multi-leader&lt;/p&gt;

&lt;h4 id=&#34;multi-datacenter-operation&#34;&gt;Multi-datacenter operation&lt;/h4&gt;

&lt;p&gt;如果有多个 datacenter ，那么每个 datacenter 一个 leader 是一个不错的选择。
我们对比一下在 multi-datacenter 中使用 single-leader 和    multi-leader 的fare&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Performance ，multi-datacenter 的性能可能好一些&lt;/li&gt;
&lt;li&gt;Tolerance of datacenter outages ，multi-datacenter 不用担心leader 没了，所以更简单一点&lt;/li&gt;
&lt;li&gt;Tolerance of network problems，由于 datacenter 之间的网络稳定性肯定不如 datacenter 内部，所以 multi-leader 更好点&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;总之，如果有多个 datacenter ，选择 multi-leader。尽管如此也会有缺点，
例如需要同时修改一份数据，可能发生冲突导致一边成功一边失败了，后面的 handle write conflict 会讨论。&lt;/p&gt;

&lt;p&gt;同时，multi-leader 可能会有一些陷阱、和数据库的其他特性放一起经常出问题，例如主键自增。所以 multi-leader 需要尽量避免。&lt;/p&gt;

&lt;h4 id=&#34;clients-with-offline-operation&#34;&gt;“Clients with offline operation”&lt;/h4&gt;

&lt;p&gt;以 印象笔记、icloud 为例，我们在手机、电脑 的笔记可以在线编辑，也可能离线编辑。
这个架构就和 Multi-Leader 类似，每个设备是一个 datacenter，服务端也是一个 datacenter，网络连接并不靠谱。&lt;/p&gt;

&lt;h4 id=&#34;collaborative-editing&#34;&gt;Collaborative editing&lt;/h4&gt;

&lt;p&gt;以 Google docs、wiki 为例，大家可以同事协同编辑，相关的算法就是 automatic conflict resolution。
你可能以为这个和 multi-leader 的 replication 不一样，实际上很类似。
一个人编辑的时候，change 马上就出现在 local replica ，然后是异步同步到服务器和其他的人。
如果你希望保证没有冲突，Application 需要在文档上面加锁，其余人获得锁之前不能编辑，这种就类似与 single-leader 的replica。
但是为了快速工作，你可能将锁分的很小，大家可以同时编辑，这就类似 multi-leader ，同时带来了 conflict resolution。&lt;/p&gt;

&lt;h4 id=&#34;handling-write-conflicts&#34;&gt;Handling Write Conflicts&lt;/h4&gt;

&lt;p&gt;如果两个人同时编辑，一个修改了标题一个修改了内容这不会有冲突，但是如果两个人都修改了标题该如何解决这个问题。
如果是 single-leader，并不会有这种问题，因为写都是有时间顺序的。后面的那个要么等待前面的修改事务成功或者失败，要么自己让让写任务失败。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;“Synchronous versus asynchronous conflict detection”&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如果两个任务修改同一个数据都成功了，然后异步发现了冲突，这时候解决已经晚了，因为不知道放弃哪个。
当然原则上也可以通过同步修改的方式避免冲突，这样就和 single-leader 没什么区别了。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Conflict avoidance&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;避免冲突是目前最好的方法了，例如用户可以修改自己信息的话，就让所有的修改都发送到同一个 datacenter 的 leader。&lt;/p&gt;

&lt;p&gt;有时候也可能某个 datacenter 挂了或者用户去了另一个地方需要 datacenter 换了，需要使用别的方法避免冲突。这时候还是需要处理冲突。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Converging toward a consistent state&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于 single-leader 的 writes 都有一个 sequential order，统一数据的最后一次写决定了数据。
对于 multi-leader 就不一样了，并不知道哪个 datacenter 的才是正确的，如果不进行修改就会出现各个 datacenter 不一致，
所以所有的更改必须 最终是 convergent ，也就是说 最终一致性。几种常见的 convergent 方法：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;简单粗暴，直接每个操作一个 uuid，例如 hash，或者时间戳，最大的那个胜出，如果使用时间戳，这种方法就叫做 Last-Write—wins（LLW）。&lt;/li&gt;
&lt;li&gt;给每个 replica 一个 id，id 最高的占主导地位，和上面的类似。&lt;/li&gt;
&lt;li&gt;合并数据，例如 一个标题是A，一个是B，最后就是 A|B&lt;/li&gt;
&lt;li&gt;用户自己实现冲突合并逻辑。&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
&lt;li&gt;Custom conflict resolution logic&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;自己实现冲突解决一般有两种方案：On Read 和 On Write ，相关细节比较简单&lt;/p&gt;

&lt;h4 id=&#34;what-is-a-conflict&#34;&gt;What is a conflict?&lt;/h4&gt;

&lt;p&gt;上面说的两个人同时修改标题的冲突很明显，所以可以通过一些程序控制，实际上也有一些不明显的的冲突。&lt;/p&gt;

&lt;p&gt;例如两个人预定会议室，同一个会议室同一时间只能有一个人预定成功，但如果是 multi-leader 可能两个人都预订成功了。
或者注册的时候要求 username 不重复，但是有可能两个人同时申请同一个 username。&lt;/p&gt;

&lt;h3 id=&#34;2-multi-leader-replication-topologies&#34;&gt;(2) Multi-Leader Replication Topologies&lt;/h3&gt;

&lt;p&gt;Replication Topologies 是用来描述数据在节点间传播的路径。例如 circular 、star、all-2-all。&lt;/p&gt;

&lt;p&gt;每一种都有优势和劣势。&lt;/p&gt;

&lt;h2 id=&#34;3-leaderless-replication&#34;&gt;3. Leaderless Replication&lt;/h2&gt;

&lt;p&gt;实际上 cassandra 采用了 leaderless 的架构，没有主节点。&lt;/p&gt;

&lt;h3 id=&#34;1-writing-to-the-database-when-a-node-is-down&#34;&gt;(1) Writing to the Database When a Node Is Down&lt;/h3&gt;

&lt;p&gt;leaderless 中，不存在 failover，因为没有主节点，因为一个节点在读数据的时候，并不需要从所有的节点都读数据，
二是给每个节点发送数据，然后每个节点都返回数据，根据 Version numbers 决定哪个是正确的。&lt;/p&gt;

&lt;h4 id=&#34;read-repair-and-anti-entropy&#34;&gt;Read repair and anti-entropy&lt;/h4&gt;

&lt;p&gt;系统需要保证每个 replica 上面的数据都是最终一致的，如果出现了节点挂掉然后恢复，如何保证它的数据是一致？&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Read repair ，当用户读书节的时候，能够发现 stale 的值并且修复&lt;/li&gt;
&lt;li&gt;Anti-entropy process 通过后台进程扫描数据发现不一致&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&#34;quorums-for-reading-and-writing&#34;&gt;Quorums for reading and writing&lt;/h4&gt;

&lt;p&gt;也就是上面所说的，读和写不需要全成功，只要保证 读成功数 r 和 写成功数 w 满足 r+w&amp;gt;n即可。这个叫做仲裁原理，类似抽屉原理，
n个抽屉，里面有 w 个抽屉放了一样的纸条，打开r个抽屉，想要看到纸条内容，只要 r + w &amp;gt;n。
一般情况下 r=w=(n+1)/2&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Limitations of Quorum Consistency&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;上面我们说了 r=w=(n+1)/2，但是如果你的数据库写的量特别小，读特别大，你也可以设置 r=n，w=1，总之只要覆盖即可。&lt;/p&gt;

&lt;p&gt;如果你对一致性要求不高，也可以使用 w+r &amp;lt;= n，这样返回值可能 stale，不过这样的可用性更高。&lt;/p&gt;

&lt;p&gt;然而尽管使 w+r &amp;gt; n，也会有一些其他的异常情况：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;sloppy quorum 下无法保证&lt;/li&gt;
&lt;li&gt;concurrently 写，和上一节类似，出现了写冲突？&lt;/li&gt;
&lt;li&gt;读和写 concurrently，这时候写只提交了一部分，其他的可能还未完全在 w 个节点成功&lt;/li&gt;
&lt;li&gt;如果写 成功不够w，但是这时候也没法 rollback，后续可能会读到失败的值，也可能读不到&lt;/li&gt;
&lt;li&gt;如果保存了 new value 的节点失败重启，从另一个保存了 old value 的节点修复数据，导致包含 new value 数据的节点少于 n&lt;/li&gt;
&lt;li&gt;还有极端情况，例如  “Linearizability and quorums”.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;因此，即便是 达到了 quorum 的条件，也没法真正的实现一致性。
特别的，至于在 single-leader 中的 “reading your writes, monotonic reads, consistent prefix reads” 等弱一致性也没法得到保障。
所以前面说的异常可能也会发生，强一致性需要 consensus 或者 transaction 来保证。&lt;/p&gt;

&lt;h4 id=&#34;monitoring-staleness&#34;&gt;Monitoring staleness&lt;/h4&gt;

&lt;p&gt;对于应用而言，是否返回最新版本的数据是很重要的。对于 leader-based 的设计，所有的问题都可以交给 replication-lag 来解决，
可以通过监控 replication-lag 监控系统
对于 leaderless 的设计，所有的数据并没有确定的顺序，监控很困难，而且如果没有 anti-entropy，数据可能过时很久都不修复。&lt;/p&gt;

&lt;h3 id=&#34;sloppy-quorums-and-hinted-handoff&#34;&gt;“Sloppy Quorums and Hinted Handoff”&lt;/h3&gt;

&lt;p&gt;简单介绍一下，例如 cassandra 中，保存三副本，读写都要成功 2 个 replica，我们写数据要成功 2个，
但是如果此时这三个节点挂了两个，只能写成功一个，可以先将另一份数据放到其他的节点上。
等对应的剩下的节点都恢复了，再将数据放回去，这个过程就是 hinted handoff。&lt;/p&gt;

&lt;h3 id=&#34;multi-datacenter-operation-1&#34;&gt;Multi-datacenter operation&lt;/h3&gt;

&lt;p&gt;这时候我们还是有N个副本，我们需要保证每在 每个 datacenter 都达到了 对应的 n/2 个副本。&lt;/p&gt;

&lt;h3 id=&#34;detecting-concurrent-writes&#34;&gt;Detecting Concurrent Writes&lt;/h3&gt;

&lt;p&gt;类似之前的 handling write conflict，也是由于顺序不一样导致，所以也需要有保证 converge 的方法，
这样的方案也就是我们前面说的一样。但是我们这里讨论的更细致一点。&lt;/p&gt;

&lt;h4 id=&#34;last-write-wins-discarding-concurrent-writes&#34;&gt;Last write wins (discarding concurrent writes)&lt;/h4&gt;

&lt;p&gt;对于不知道哪个 happens-first 的操作，我们称之为 concurrent，他们的 order 是不知道的。&lt;/p&gt;

&lt;p&gt;尽管他们没有 order，我们可以人为给一个 order，例如 timestamp，也就是 Last write wins。&lt;/p&gt;

&lt;p&gt;llw 解决了冲突问题，但是丢失了一些数据。所有使用 cassandra 的时候有人会给每个 key 后面加一个 uuid，这样能保证每个 key 多次写都不会丢失。&lt;/p&gt;

&lt;h4 id=&#34;the-happens-before-relationship-and-concurrency&#34;&gt;The “happens-before” relationship and concurrency&lt;/h4&gt;

&lt;p&gt;happens-before 的关系是决定是否并发的关键，以下情况下我们称两个操作是 concurrency：
“neither happens before the other (neither knows about the other)”&lt;/p&gt;

&lt;p&gt;因此，对于任何两个操作 A 和 B，只有两种可能，A happens-before B，B happens-before A，A 和 B 是 concurrency。&lt;/p&gt;

&lt;h4 id=&#34;capturing-the-happens-before-relationship&#34;&gt;Capturing the happens-before relationship&lt;/h4&gt;

&lt;p&gt;上面已经知道两个操作只能有 3 种关系，我们如果发现 happens-before  的关系呢？其实有算法的。&lt;/p&gt;

&lt;p&gt;首先假如 A 和 B两个人都在往同一个购物车添加商品，一开始 A添加了 商品1，B添加了商品 2，这两个是 concurrency的，
然后 A再添加 3，B添加4，这时候 这两个操作都是 3和4是 concurrency的，但是1和2 都是 happens-before 3 和4 的。&lt;/p&gt;

&lt;p&gt;发现 concurrency 关系的算法如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;server 的每个key有一个 version number，每次更新写这个 key 的时候，version number 就 +1并且跟新数据。&lt;/li&gt;
&lt;li&gt;当 client 读数据，返回所有没有 overwritten 的数据和最新的 version number。&lt;/li&gt;
&lt;li&gt;client 写数据的时候，必须包含之前读到的那个 version number，并且合并所有的数据，&lt;/li&gt;
&lt;li&gt;当 server 收到带 version number 的写请求，覆盖所有小于等于 version number 的数据，保留大于 version number 的数据。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这样的算法就能发现所有的 happens-before 关系&lt;/p&gt;

&lt;h4 id=&#34;merging-concurrently-written-values&#34;&gt;Merging concurrently written values&lt;/h4&gt;

&lt;p&gt;类似前面的 conflict resolution，上面的算法需要 client 进行数据合并，合并算法大家自己查资料，例如 CRDT 。&lt;/p&gt;

&lt;h4 id=&#34;version-vectors&#34;&gt;Version vectors&lt;/h4&gt;

&lt;p&gt;上面的算法是 single-replica 情况，对于 multi-replica 的情况，需要每个副本一个 Version number，这被叫做 Version vectors。&lt;/p&gt;

&lt;h2 id=&#34;summary&#34;&gt;SUMmary&lt;/h2&gt;

&lt;p&gt;通过上面的讨论，我们明白了一句话，其实分布式 replica 最大的问题就是发现 happens-before 的依赖关系，也就是操作的顺序。&lt;/p&gt;

&lt;p&gt;另外有大神说过，分布式只有两个问题， exactly-once 和 order。其中 exactly-once 后续会讨论，order 就是我们这里讨论最多的。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>design-data-intensive-architecture</title>
      <link>https://dengziming.github.io/post/data-intensive-architecture/design-data-intensive-architecture/</link>
      <pubDate>Sat, 22 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/data-intensive-architecture/design-data-intensive-architecture/</guid>
      
        <description>

&lt;h1 id=&#34;第一部分-前言&#34;&gt;第一部分、前言&lt;/h1&gt;

&lt;p&gt;ACID，bigdata，NoSql ，bigTable ，CAP，2PC，3PC，quorum，raft，paxos，cloud，real-time，高并发架构将会逐渐成为基础。&lt;/p&gt;

&lt;p&gt;主要内容：
（一）基础&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;“reliability, scalability, and maintainability”&lt;/li&gt;
&lt;li&gt;数据模型&lt;/li&gt;
&lt;li&gt;存储引擎&lt;/li&gt;
&lt;li&gt;数据结构&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;（二）分布式&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;replication&lt;/li&gt;
&lt;li&gt;partitioning/sharding&lt;/li&gt;
&lt;li&gt;transaction&lt;/li&gt;
&lt;li&gt;distributed system&lt;/li&gt;
&lt;li&gt;consistency and consensus&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;（三）交互&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;batch process&lt;/li&gt;
&lt;li&gt;realtime&lt;/li&gt;
&lt;li&gt;put everything together&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;（四）案例及资料&lt;/p&gt;

&lt;h1 id=&#34;第二部分-数据系统的基础&#34;&gt;第二部分、数据系统的基础&lt;/h1&gt;

&lt;h2 id=&#34;一-reliable-scalable-和-maintainable-的含义&#34;&gt;一、Reliable,Scalable 和 Maintainable 的含义&lt;/h2&gt;

&lt;p&gt;数据系统分为 data-intensive 和 compute-intensive，一般的大数据应用会采用的技术：
1. 存储，方便后续的查询 &amp;ndash; databases
2. 缓存，加速访问 &amp;ndash; caches
3. 允许用户通过关键字搜索、按照过滤  &amp;ndash; indexes
4. 给另一个进程发送消息，异步处理  &amp;ndash; streaming process
5. 周期性处理全量数据  &amp;ndash; batch process&lt;/p&gt;

&lt;p&gt;这一切都是需要数据系统，我们并不会从头开始搭建存储、计算系统，因为有很多现成的技术可以应用。数据库、消息队列功能都很相似，但是实现完全不同，导致不同的性能，所以我们用处不同，我们为什么要将他们结合到一起呢？&lt;/p&gt;

&lt;p&gt;其实许多工具功能比较重复，redis 其实也可以当消息队列，kafka 也可以做数据持久化。这些工具的界限越来越模糊。第二是我们得系统一般比较复杂，一个工具完成不了，我们必须组件一个数据系统。组件的过程
有很多需要考虑的事情，其中最重要的三点：
Reliable： 系统必须一致能够提供服务，即便是发生了一些不可预知的错误
Scalable： 系统升级、功能扩展、访问量增加，导致我们需要能够很方便扩展我们的系统
Maintainable： 随着时间推移，可能有很多其他人接受项目，必须方便别人进行维护&lt;/p&gt;

&lt;p&gt;有关这三点的介绍，我们逐渐展开：&lt;/p&gt;

&lt;h3 id=&#34;1-reliable&#34;&gt;1. Reliable&lt;/h3&gt;

&lt;h3 id=&#34;2-scalable&#34;&gt;2. Scalable&lt;/h3&gt;

&lt;h3 id=&#34;3-maintainable&#34;&gt;3. Maintainable&lt;/h3&gt;

&lt;h2 id=&#34;二-数据模型和查询语言&#34;&gt;二、数据模型和查询语言&lt;/h2&gt;

&lt;p&gt;数据模型是最重要的一部分，&lt;/p&gt;

&lt;h3 id=&#34;1-关系型与文档型&#34;&gt;1.关系型与文档型&lt;/h3&gt;

&lt;h3 id=&#34;2-nosql&#34;&gt;2.NoSQL&lt;/h3&gt;

&lt;h3 id=&#34;3-object-relational-mismatch&#34;&gt;3. Object-Relational Mismatch&lt;/h3&gt;

&lt;h3 id=&#34;4-many-to-one-and-many-to-many-relationships&#34;&gt;4. Many-to-One and Many-to-Many Relationships&lt;/h3&gt;

&lt;h3 id=&#34;5-query-languages-for-data&#34;&gt;5. Query Languages for Data&lt;/h3&gt;

&lt;h3 id=&#34;6-graph-like-data-models&#34;&gt;6.Graph-Like Data Models&lt;/h3&gt;

&lt;h2 id=&#34;三-storage-and-retrieval&#34;&gt;三、Storage and Retrieval&lt;/h2&gt;

&lt;h3 id=&#34;1-hash-indexes&#34;&gt;1. Hash Indexes&lt;/h3&gt;

&lt;h3 id=&#34;2-sstables-and-lsm-trees&#34;&gt;2. SSTables and LSM-Trees&lt;/h3&gt;

&lt;h3 id=&#34;3-b-trees&#34;&gt;3. B-Trees&lt;/h3&gt;

&lt;h3 id=&#34;4-multi-column-indexes&#34;&gt;4. Multi-column indexes&lt;/h3&gt;

&lt;h3 id=&#34;5-full-text-search-and-fuzzy-indexes&#34;&gt;5. Full-text search and fuzzy indexes&lt;/h3&gt;

&lt;h3 id=&#34;6-keeping-everything-in-memory&#34;&gt;6.Keeping everything in memory&lt;/h3&gt;

&lt;h3 id=&#34;7-olap-vs-oltp&#34;&gt;7. OLAP vs OLTP&lt;/h3&gt;

&lt;h3 id=&#34;8-数据仓库&#34;&gt;8. 数据仓库&lt;/h3&gt;

&lt;h3 id=&#34;9-stars-and-snowflakes-schemas-for-analytics&#34;&gt;9. Stars and Snowflakes: Schemas for Analytics&lt;/h3&gt;

&lt;h3 id=&#34;10-column-oriented-storage&#34;&gt;10. Column-Oriented Storage&lt;/h3&gt;

&lt;h3 id=&#34;11-column-compression&#34;&gt;11. Column Compression&lt;/h3&gt;

&lt;h3 id=&#34;12-sort-order-in-column-storage&#34;&gt;12. Sort Order in Column Storage&lt;/h3&gt;

&lt;h3 id=&#34;13-writing-to-column-oriented-storage&#34;&gt;13. Writing to Column-Oriented Storage&lt;/h3&gt;

&lt;p&gt;前面写的有关列存储的优化在数据仓库中很重要，排序、位图索引、压缩都能加速查询，但是会让写数据更加麻烦。&lt;/p&gt;

&lt;p&gt;以 BTree 为理论的写方法，在这里完全没有用，如果你要在数据中插入一条数据，你需要重写所有的数据。&lt;/p&gt;

&lt;p&gt;但是还好的是，如果是 LSM-tree 的方式，首先写进内存排序，然后写磁盘。查询的时候，也是两部分结果进行合并，Vertica 数据仓库就是这么做的。&lt;/p&gt;

&lt;h3 id=&#34;14-aggregation-data-cubes-and-materialized-views&#34;&gt;14. Aggregation: Data Cubes and Materialized Views&lt;/h3&gt;

&lt;p&gt;并不是所有的数据仓库都是使用列存储，很多其他存储方式也会用到，但是列存储确实能够很大程度加快访问，所以越来越流行。&lt;/p&gt;

&lt;p&gt;另一种值得一提的数据仓库技术就是 materialized aggregates。前面所说的，查询经常会遇到一些聚合， 例如 count，sum，如果一个查询被很多查询共用，可以将结果缓存。
其中一种方法就是 materialized view，类似查询视图，只不过这个视图的结果已经计算好保存起来了。如果数据改变了，你的 materialized view 也要改变。&lt;/p&gt;

&lt;p&gt;一种常见的案例就是 data cube 后者 OLAP cube，就是根据不同纬度就行聚合后的结果。举个列子，如果表格有 日期、商品种类、销售额三列，我们可以计算销售额在 日期、商品种类 两个维度下的聚合只，得到一个二维表格。
二维表格的两个维度分别为日期和商品种类，表格的值就是销售额，然后如果要计算每个维度下的 sum，只需要将对应维度所有值 sum 到一起。&lt;/p&gt;

&lt;p&gt;实际上，表格都不止两个维度，假如有五个维度，情况就很复杂了。但是原则不变，每个 cell 保存五个维度组合下的聚合值。&lt;/p&gt;

&lt;p&gt;通过 cube 可以加速某些查询，但是如果要计算订单额度大于 100 的比例，那就没法计算了，因为额度很难作为一个维度。所以一般只作为一个加快部分查询的工具。&lt;/p&gt;

&lt;h2 id=&#34;四-encoding-and-evolution&#34;&gt;四、Encoding and Evolution&lt;/h2&gt;

&lt;p&gt;服务总是要变的，服务变了，服务之间的代码也要变。服务端可以做滚动升级，也就是部分服务器先升级，保证没错的话，其他的继续升级。
而客户端也就是用户端，代码变化必须做到向前向后兼容，也就是说，高版本要能够处理低版本的数据，这很简单；低版本的代码，必须能从高版本的架构读数据，这是比较难的。
接下来我们将会看一些数据结构，JSON, XML, Protocol Buffers, Thrift, and Avro ，主要看他们在这些data storage and for communication系统中如何使用：
in web services, Representational State Transfer (REST), and remote procedure calls (RPC), as well as message-passing systems such as actors and message queues&lt;/p&gt;

&lt;h3 id=&#34;1-formats-for-encoding-data&#34;&gt;1. Formats for Encoding Data&lt;/h3&gt;

&lt;p&gt;一般的应用都会处理两种数据。内存主要是通过对象、数组、树、hash表等，而需要保存、传输时候，需要进行序列化，由于指针引用序列化后是没用的，所以序列化后的结构和内存结构完全不用。
从设计模式角度考虑，我们需要进行一个适配，两边都要适配的话，实际上就是做一个转换。内存到 byte sequence 的过程称为 encoding（或者 serialization、marshalling），反过来讲decoding（deserialization、parsing、unmarshalling）&lt;/p&gt;

&lt;p&gt;** 专业术语冲突
serialization 在数据库的事务中也会用到，但是完全是另外一码子事情，后续会进行介绍。这里使用的时候我们以 encoding 为主，虽然平时 serialization 用的更多。&lt;/p&gt;

&lt;p&gt;这里我们将会以一条数据为例,对它进行编码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
    &amp;quot;userName&amp;quot;: &amp;quot;Martin&amp;quot;,
    &amp;quot;favoriteNumber&amp;quot;: 1337,
    &amp;quot;interests&amp;quot;: [&amp;quot;daydreaming&amp;quot;, &amp;quot;hacking&amp;quot;]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;1-language-specific-formats&#34;&gt;(1) Language-Specific Formats&lt;/h4&gt;

&lt;p&gt;java.io.Serializable 属于java 自带的序列化机制，很多编程语言都自带了。当然也有很多第三方的，比如 Kryo for Java。这些很方便，因为你只需要直接读然后调用java对应的方法即可。但是问题也很多。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;通用性问题，不多说。&lt;/li&gt;
&lt;li&gt;数据会有类型，例如java 序列化必须只能解析为特定的类。所以会定义很多类，这样可能导致一些人得到你的类信息，然后远程植入代码。&lt;/li&gt;
&lt;li&gt;版本问题。&lt;/li&gt;
&lt;li&gt;效率问题。&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&#34;2-json-xml-and-binary-variants&#34;&gt;(2) JSON, XML, and Binary Variants&lt;/h4&gt;

&lt;p&gt;JSON, XML 一般都很熟悉，优缺点都很明显，二进制的编码在某些内网的应用中很常见，还能节约空间。例如 MassagePack 就是二进制的 json。&lt;/p&gt;

&lt;h4 id=&#34;3-thrift-and-protocol-buffers&#34;&gt;(3) Thrift and Protocol Buffers&lt;/h4&gt;

&lt;p&gt;Thrift and Protocol Buffers 很多人都知道，作为两种序列化的框架，当然还提供了别的功能，例如 RPC 通信接口。都需要定义一个schema：&lt;/p&gt;

&lt;p&gt;Thrift 的定义格式：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;struct Person {
  1: required string       userName,
  2: optional i64          favoriteNumber,
  3: optional list&amp;lt;string&amp;gt; interests
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;pb 的定义格式：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;message Person {
    required string user_name       = 1;
    optional int64  favorite_number = 2;
    repeated string interests       = 3;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后他们都有代码生成的工具，可以针对各种语言生成相应的代码，用来 encode 和 decode 数据。thrift 的格式有两种 BinaryProtocol and CompactProtocol。&lt;/p&gt;

&lt;p&gt;BinaryProtocol 表示上面的数据格式需要 59 bytes ：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;0b 00 01 00 00 00 06 4d 61 72 74 69 6e     0b 是 type代表 String，紧接着 00 01 代表 field tag 为1，然后 00 00 00 06 代表长度为6，剩下的六bytes 代表数据。
0a 00 02 00 00 00 00 00 00 05 39           和刚才一样， 0a 代表int64 ，
0f 00 03 0b 00 00 00 02                    0f 代表list，00 03代表 field tag 为3，0b 代表 item type 为 string，00 00 00 02 代表长度为2，
	00 00 00 0b 64 61 79 ....              长度和数据
	00 00 00 07 68 ..... 				   长度和数据
00											结束标志
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;CompactProtocol 和 BinaryProtocol 的结构类似，但是进行了一些压缩，需要 33 bytes&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;18 06 4d ....  							  18 是 00011000 field type and tag number 合在一起，0001 是 tag，
										  1000 是 type 是string （为什么8 代表 string，上面是11），06 长度，后面是数据
										  另外int类型并不是占了 64位，这里的 06是两位，共 8 字节，
										  最高位代表数据是否已经完整，剩下的七位是数据。如果数据不完整，需要再占8位。这样 -64 到 63 之间的数据只占了1byte
										  
16 f2 14 								  16 是 00010110，0001 代表 field tag 比上一个加一，0110 数据类型 后面的 f214 是数据，f2 最高位是1，数据没完，需要往下一位读。
19 28
    0b 64 .....
    07 68 ....
00
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ProtocolBuffer 和 Thrift 的 CompactProtocol 类似，需要 33 bytes&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;0a 06 4d 61 72 74 69 6e       			  0a 是 tag(00001) 和 type(010), 06 是长度。 后面是数据
10 b9 0a 								  10 是tag (00010) 和 type（000）,后面是长度+数据
1a 0b ........							  1a 是 tag 和 type，后面是长度+数据
1a 07 ........							  1a 是 tag 和 type，后面是长度+数据
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;和 CompactProtocol 稍微有些不一样，tag+type 为两位，另外 list 类型直接存两个。&lt;/p&gt;

&lt;p&gt;需要注意的是，前面我们的例子中，数据要么是 要么是 required or optional，实际上这对于数据的存储没有任何影响，都是一样的存储，只是在最后解析的时候影响，例如你设置的是 required ，但是没有解析出来，会失败。&lt;/p&gt;

&lt;h4 id=&#34;4-field-tags-and-schema-evolution&#34;&gt;(4) Field tags and schema evolution&lt;/h4&gt;

&lt;p&gt;前面讲过数据需要改变，那 pb 和 thrift 如何应对schema 的变化，做到向前向后兼容呢？
其实我们从上面的数据可以看出，最重要就是 field tag 了，只要 field tag 唯一，哪怕是增加了数据，减少了数据，最终的结果其实还是一样的。你可以改变变量名字，但是不能改变 tag。
首先是向后兼容，只要你新加的 field tag 不被设置为 required，就不会报错，另外删除数据也是，不能删除 required 的数据。
另外是向前兼容，如果是旧的代码读取新的数据，如果遇到不认识的 field tag ，会直接忽略。&lt;/p&gt;

&lt;h4 id=&#34;5-datatypes-and-schema-evolution&#34;&gt;(5) Datatypes and schema evolution&lt;/h4&gt;

&lt;p&gt;数据类型发生变化，一方面可能是数据精度收到影响，另一方面，我们可以看出 pb 是可以讲啊 list 类型变成 optional 的，但是 thrift 不行，因为 thrift 提供了 list 类型，但是这样可以用来写嵌套数据。&lt;/p&gt;

&lt;h4 id=&#34;6-avro&#34;&gt;(6) avro&lt;/h4&gt;

&lt;p&gt;avro 也是一个序列胡框架，他和 pb、thrift 不同点在于没有 tagNumber，这其实很适合动态生成schema，我们不介绍了。&lt;/p&gt;

&lt;h4 id=&#34;7-dynamically-generated-schemas&#34;&gt;(7) Dynamically generated schemas&lt;/h4&gt;

&lt;p&gt;如果我们的数据增加了一列，并且减少了一列，对于 avro 来说，只需要重新生成一个 schema 即可，而如果使用 pb、thrift，需要手动进行配置tag和field 的映射，&lt;/p&gt;

&lt;h4 id=&#34;8-code-generation-and-dynamically-typed-languages&#34;&gt;(8) Code generation and dynamically typed languages&lt;/h4&gt;

&lt;p&gt;代码生成，例如 pb 可以生成 java 和 python 的代码。&lt;/p&gt;

&lt;h4 id=&#34;9-the-merits-of-schemas&#34;&gt;(9) The Merits of Schemas&lt;/h4&gt;

&lt;h3 id=&#34;二-modes-of-dataflow&#34;&gt;二、 Modes of Dataflow&lt;/h3&gt;

&lt;p&gt;我们需要在进程之间发送数据，&lt;/p&gt;

&lt;h4 id=&#34;1-dataflow-through-databases&#34;&gt;1.Dataflow Through Databases&lt;/h4&gt;

&lt;h4 id=&#34;2-dataflow-through-services-rest-and-rpc&#34;&gt;2.Dataflow Through Services: REST and RPC&lt;/h4&gt;

&lt;h4 id=&#34;3-message-passing-dataflow&#34;&gt;3.  Message-Passing Dataflow&lt;/h4&gt;

&lt;h1 id=&#34;第二部分-分布式&#34;&gt;第二部分、分布式&lt;/h1&gt;

&lt;p&gt;使用分布式的原因很多，例如 Scalability，high available，latency。数据分布式有很多种方式，首先是副本、分区，也就是 Replication 和 Partition。&lt;/p&gt;

&lt;h1 id=&#34;一-replication&#34;&gt;一、Replication&lt;/h1&gt;

&lt;p&gt;Replication 就是将你的数据放在多个节点，这样有很多好处。数据从地理上可以隔用户更近，保持高可用，增加吞吐量。&lt;/p&gt;

&lt;p&gt;本章我们假设数据都是完整的进行副本存放，后面再讲分区。数据的副本解决难点就是数据变化的一致性，如果数据一致不变，那就太简单了，没什么可以讨论的。&lt;/p&gt;

&lt;p&gt;对于数据变化的副本存储，有很多需要注意的地方，例如每次写数据操作数据复制是同步的还是异步的，节点挂了怎么办，数据一致性问题，时效性问题。
我们的学习主要分为三种设计：single-leader, multi-leader, and leaderless&lt;/p&gt;

&lt;h2 id=&#34;1-leaders-and-followers&#34;&gt;1. Leaders and Followers&lt;/h2&gt;

&lt;p&gt;每个节点上面的副本叫做 replica ，当有 multi replica 的时候，一个问题出现了，怎么保证数据写到了每一个副本。
数据库的每一次写都必须保证被每个副本处理，否则就会出现不一致。
最常用的解决方案就是 leader-based-replication，也叫 active-passive、master-slave。&lt;/p&gt;

&lt;p&gt;在主从结构中，写只能发送给 leader，leader 写的时候会发送数据给 follower，而读操作可以读任何一个节点。&lt;/p&gt;

&lt;h3 id=&#34;1-synchronous-versus-asynchronous-replication&#34;&gt;(1) Synchronous Versus Asynchronous Replication&lt;/h3&gt;

&lt;p&gt;replicated 系统重要特点就是 同步写还是异步写，一般关系型数据库可以配置，其他的都是硬编码写死的。&lt;/p&gt;

&lt;p&gt;客户端发送数据给leader，然后leader转发给follower，最后通知client。如果 leader 收到 follower 的确认消息再回复client，这就是同步。
如果leader发送给了 follower后就直接回复client，就是异步。一般情况，系统都只有一个follower是同步，其余的都是异步。这叫 semi-synchronous。&lt;/p&gt;

&lt;p&gt;为了效率经常设置为 Asynchronous，这样如何 leader 失败了而且不恢复，那还没有被副本保存的数据就丢失了，
这样的好处就是可以提供持续服务，尽管follower很慢。即便是配置使用 Synchronous，实际上也是有一台是同步，其余的是异步。&lt;/p&gt;

&lt;p&gt;Weakening durability 可能是一个比较糟糕的 trade-off，但是 Asynchronous 还是很常用，尤其是地理上分布式的集群。&lt;/p&gt;

&lt;p&gt;当然也有很多其他的方案在研究中，例如 Azure 使用了 chain replication。&lt;/p&gt;

&lt;h3 id=&#34;2-setting-up-new-followers&#34;&gt;(2) Setting Up New Followers&lt;/h3&gt;

&lt;p&gt;系统有时候需要添加新的节点，如何保证新节点和leader的数据一致呢？直接复制数据过来肯定是不行的，leader 一直处在 flux 中，一般步骤有四步：
1. leader 在某个确定一致性的时间点拍一个 snapshot 而不用锁住系统，大部分数据库都提供了这个功能，有的能通过第三方工具提供这个功能。
2. follower 将数据复制过去。
3. follower 将从这个时间点以后的数据复制过去，这需要拍 snapshot 时候的日志位置，这个点的名字叫 Log sequence number 或者 binlog coordinates。
4. 当 follower 将所有的change都同步后，我们称之为 catch up，就可以和其他follower一样，处理 leader 发生的变化了。&lt;/p&gt;

&lt;p&gt;真正的步骤其实是有很多不同，有时候可以自动配置，有时候需要手动配置。&lt;/p&gt;

&lt;h3 id=&#34;3-handling-node-outages&#34;&gt;(3) Handling Node Outages&lt;/h3&gt;

&lt;p&gt;有时候节点会挂掉，可能是我们不知道的原因，也可能是为了安装系统模块而重启，这时候应该怎么保证集群的高可用&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;follower 挂掉&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;follower 挂点然后重启的步骤是很简单的，类似前面的添加新节点，启动以后，根据日志能找到最后一个处理的事务，然后从leader请求这个时间点以后的更改，然后进行更改，完成后就 catch up。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;leader failover&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;leader 挂掉后可能麻烦一点，必须马上选择一名新的leader，这个过程被称为 failover&lt;/p&gt;

&lt;p&gt;failover的具体步骤有4：
1. 确定leader失败了，失败的原因很多，磁盘、电源等等，目前用的最多的是 timeout 方法，节点相互 bounce message，如果突然在某个时间点收不到，就是失败了。
2. 选择新的 leader，需要在大多数的follwer 进行选举，最好是选一个数据最新的。所有的节点同意这个节点成为 consensus。
3. 告知系统使用新的leader，client 发送数据到新的 leader，旧的 leader 回复后也要认同新的 leader。&lt;/p&gt;

&lt;p&gt;failover 总是会有些不期而遇的问题：
1. 如果是 asynchronous replica ，可能遇到一些数据没有同步过来，导致数据少了，如果这时候 older leader起来了，就会发生错误，这时候一般选择 discard 这部分数据。
2. 如果系统和别的系统进行结合，discard 是很危险的，github 发生了一次 mysql leader挂掉，然后discard 了一部分，删掉了部分自增的primary key，可是这部分pk已经保存到 redis了，导致了错误
3. 有时候两个节点都认为自己是 leader，这个叫做 split brain，这时候一半需要去掉一个，但是有可能两个都被去掉了。
4. timeout 的具体值多少合适？如果太长了，可能导致发现的晚。如果太短了可能误判，尤其是系统压力大的时候，还重新换一次会造成更大的压力。&lt;/p&gt;

&lt;p&gt;这些问题解决起来还是很棘手的，所以有很多算法，我们后续会介绍。&lt;/p&gt;

&lt;h3 id=&#34;4-implementation-of-replication-logs&#34;&gt;(4) Implementation of Replication Logs&lt;/h3&gt;

&lt;p&gt;上面各种恢复，添加，都涉及到日志，日志记录了数据的更改，实际上我们学习 Hadoop 的时候，也知道里面有日志，现在我们来研究一下，日志怎么实现。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Statement-based replication&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;基于 statement 的 replication，这是最简单的方法，数据库记录每一次更新，也就是说，你的 UPDATE、INSERT、DELETE都会记录到日志中，然后发送给follower。
这样的问题是：
对于 current_timestamp、random 这样的方法返回值不一样，基于判断的例如 update &amp;hellip; where &amp;hellip; 必须按照顺序，一些触发器、存储过程等 side effects。&lt;/p&gt;

&lt;p&gt;对于这些问题，当然也有特定的解决方案，例如先计算一些可能导致不一致的函数。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;WAL-log&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;前面讲过 B-Tree 和 LSM-Tree，都是基于WAL-log，是一种只能 append 的二进制日志，leader 不仅将 wal-log 保存到磁盘，而且发送到 followers。&lt;/p&gt;

&lt;p&gt;WAL 的 disadvantage 是太底层，wal 包含了磁盘那个位置的 block 需要修改哪些 bytes，这样耦合性比较高，如果format 改变了，基本上很难让 leader和follower 使用不同版本。&lt;/p&gt;

&lt;p&gt;这个问题看起来更像是实现的问题，如果 replication 的协议支持follower 使用新的版本，就能完成 zore-downtime 的 upgrade。
首先更新 follower的版本，然后让leader failover，选择一个 follower 为 leader。如果replication 的协议不支持版本不一直，这就叫做 wal-shipping，会造成 upgrade-downtime。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Logical (row-based) log replication&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这种方式就是 replication log 和具体的 storage device 解耦，使用不同格式。 relational database 的 logical log 一般都是一系列表示行的recorders。一个改变很多行的 statement 会
产生很多行这样的日志，后面有一个transaction 完成的标识符。这种方式是解耦的，所以可以容忍不同的版本，甚至不同的存储引擎。这也让外部的系统容易获得数据，
例如数仓、二级索引，外部缓存，这个技术叫做 change data capture。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;trigger based replication&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;前面讲的都是数据库系统的，有时候我们需要更灵活的，例如你只想一部分数据，或者你想从一个数据库迁移到另一个数据库。这样你就应该将replication 提升到应用程序层面。
很多数据库提供了工具完成这个事情。许多关系型数据库提供了一项功能：triggers and stored procedures。
trigger 能让你注册用户代码，一旦数据发生改变，将会将数据放到另一给表格，外部系统可以访问。&lt;/p&gt;

&lt;h3 id=&#34;5-problems-with-replication-lag&#34;&gt;(5) Problems with Replication Lag&lt;/h3&gt;

&lt;p&gt;之前说了 Replication 是保证数据安全、忍受 node failures，实际上还可以提高 scalability。&lt;/p&gt;

&lt;p&gt;Leader-based 需要所有的 write 请求都通过 master，将 read 分发到 slave，增加节点就能增加扩展性，
节点太多了就只能使用 异步的数据同步方式，否则可用性就很差。&lt;/p&gt;

&lt;p&gt;然而一旦 slave 的数据 fallen behind 了，可能访问的数据过时了，这看起来问题不大，毕竟等等就行，这叫做  eventual consistency。&lt;/p&gt;

&lt;p&gt;这个 eventually 是一个很NB的词，并没有告诉你具体时间，只是说最终会一致。我们称时间差为 the replication lag。这个 lag导致的问题我们接下来会讨论。&lt;/p&gt;

&lt;h4 id=&#34;1-read-your-own-writes&#34;&gt;1. Read your own writes&lt;/h4&gt;

&lt;p&gt;之前的 qq 空间有这个问题，当你发表了评论后页面会刷新，但是刷新后看不到自己发表的评论。
这时候你以为评论失败了，所以重新评论一下，然后刷新发现了自己评论了两次。&lt;/p&gt;

&lt;p&gt;这种问题被叫做 “read-after-write consistency”，也叫做 “read-your-writes consistency”。有一些解决方案：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;如果是用户可以修改的信息，从 master 读。&lt;/li&gt;
&lt;li&gt;如果是一分钟之内修改过的数据，从 master 读，或者可以监控 lag 大于1分钟的follow 排除。&lt;/li&gt;
&lt;li&gt;客户端保存最近一次写的 timestamp，这样服务端保证读的时候先等 slave 的数据同步已经到了这个时间。这里的 timestamp 可以是逻辑时间。&lt;/li&gt;
&lt;li&gt;如果有多个数据中心，可能更复杂。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;上面的解决方案只是一个 client，如果用户同事有多个 client 访问，例如手机和pc，这时候要考虑更多问题。&lt;/p&gt;

&lt;h4 id=&#34;2-monotonic-reads&#34;&gt;2. Monotonic Reads&lt;/h4&gt;

&lt;p&gt;类似上面，如果连续两次访问来自于不同的 slave，可能出现时间倒退，例如看球赛的时候刚开始比分 1:1,刷新一下变成 1:0 了！&lt;/p&gt;

&lt;p&gt;Monotonic Reads 就是保证上面的异常不发生的一致性。他比 eventually consistency 更强，比 strong consistency 更弱。&lt;/p&gt;

&lt;p&gt;这种一致性的解决方案就是保证每个 userid 都从同一个 slave 读数据，例如根据 userid 的 hash。&lt;/p&gt;

&lt;h4 id=&#34;3-consistent-prefix-reads&#34;&gt;3. Consistent Prefix Reads&lt;/h4&gt;

&lt;p&gt;在一个群聊系统，如果你发现两个人的对话顺序反了，就是 Consistent Prefix Reads 没有得到保证。&lt;/p&gt;

&lt;p&gt;这个问题解决方案需要保证有相互依赖的数据都按顺序写入。后续再  causal dependencies 和 happens-before 会讨论。&lt;/p&gt;

&lt;h3 id=&#34;6-solutions-for-replication-lag&#34;&gt;(6). Solutions for Replication Lag&lt;/h3&gt;

&lt;p&gt;后续会讲解 事务 和 分布式事务。&lt;/p&gt;

&lt;h2 id=&#34;2-multi-leader-replication&#34;&gt;2. Multi-Leader Replication&lt;/h2&gt;

&lt;p&gt;一个 leader 可能会有一些问题，例如写太多了可能会有性能问题。也有一些其他的选择， 例如多个 leader 。&lt;/p&gt;

&lt;h3 id=&#34;1-use-cases-for-multi-leader-replication&#34;&gt;(1). “Use Cases for Multi-Leader Replication”&lt;/h3&gt;

&lt;p&gt;一般情况如果只有一个 datacenter 是不需要用多个 leader 的，所以在多个 datacenter 的时候会使用 multi-leader&lt;/p&gt;

&lt;h4 id=&#34;multi-datacenter-operation&#34;&gt;Multi-datacenter operation&lt;/h4&gt;

&lt;p&gt;如果有多个 datacenter ，那么每个 datacenter 一个 leader 是一个不错的选择。
我们对比一下在 multi-datacenter 中使用 single-leader 和    multi-leader 的fare&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Performance ，multi-datacenter 的性能可能好一些&lt;/li&gt;
&lt;li&gt;Tolerance of datacenter outages ，multi-datacenter 不用担心leader 没了，所以更简单一点&lt;/li&gt;
&lt;li&gt;Tolerance of network problems，由于 datacenter 之间的网络稳定性肯定不如 datacenter 内部，所以 multi-leader 更好点&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;总之，如果有多个 datacenter ，选择 multi-leader。尽管如此也会有缺点，
例如需要同时修改一份数据，可能发生冲突导致一边成功一边失败了，后面的 handle write conflict 会讨论。&lt;/p&gt;

&lt;p&gt;同时，multi-leader 可能会有一些陷阱、和数据库的其他特性放一起经常出问题，例如主键自增。所以 multi-leader 需要尽量避免。&lt;/p&gt;

&lt;h4 id=&#34;clients-with-offline-operation&#34;&gt;“Clients with offline operation”&lt;/h4&gt;

&lt;p&gt;以 印象笔记、icloud 为例，我们在手机、电脑 的笔记可以在线编辑，也可能离线编辑。
这个架构就和 Multi-Leader 类似，每个设备是一个 datacenter，服务端也是一个 datacenter，网络连接并不靠谱。&lt;/p&gt;

&lt;h4 id=&#34;collaborative-editing&#34;&gt;Collaborative editing&lt;/h4&gt;

&lt;p&gt;以 Google docs、wiki 为例，大家可以同事协同编辑，相关的算法就是 automatic conflict resolution。
你可能以为这个和 multi-leader 的 replication 不一样，实际上很类似。
一个人编辑的时候，change 马上就出现在 local replica ，然后是异步同步到服务器和其他的人。
如果你希望保证没有冲突，Application 需要在文档上面加锁，其余人获得锁之前不能编辑，这种就类似与 single-leader 的replica。
但是为了快速工作，你可能将锁分的很小，大家可以同时编辑，这就类似 multi-leader ，同时带来了 conflict resolution。&lt;/p&gt;

&lt;h4 id=&#34;handling-write-conflicts&#34;&gt;Handling Write Conflicts&lt;/h4&gt;

&lt;p&gt;如果两个人同时编辑，一个修改了标题一个修改了内容这不会有冲突，但是如果两个人都修改了标题该如何解决这个问题。
如果是 single-leader，并不会有这种问题，因为写都是有时间顺序的。后面的那个要么等待前面的修改事务成功或者失败，要么自己让让写任务失败。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;“Synchronous versus asynchronous conflict detection”&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如果两个任务修改同一个数据都成功了，然后异步发现了冲突，这时候解决已经晚了，因为不知道放弃哪个。
当然原则上也可以通过同步修改的方式避免冲突，这样就和 single-leader 没什么区别了。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Conflict avoidance&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;避免冲突是目前最好的方法了，例如用户可以修改自己信息的话，就让所有的修改都发送到同一个 datacenter 的 leader。&lt;/p&gt;

&lt;p&gt;有时候也可能某个 datacenter 挂了或者用户去了另一个地方需要 datacenter 换了，需要使用别的方法避免冲突。这时候还是需要处理冲突。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Converging toward a consistent state&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于 single-leader 的 writes 都有一个 sequential order，统一数据的最后一次写决定了数据。
对于 multi-leader 就不一样了，并不知道哪个 datacenter 的才是正确的，如果不进行修改就会出现各个 datacenter 不一致，
所以所有的更改必须 最终是 convergent ，也就是说 最终一致性。几种常见的 convergent 方法：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;简单粗暴，直接每个操作一个 uuid，例如 hash，或者时间戳，最大的那个胜出，如果使用时间戳，这种方法就叫做 Last-Write—wins（LLW）。&lt;/li&gt;
&lt;li&gt;给每个 replica 一个 id，id 最高的占主导地位，和上面的类似。&lt;/li&gt;
&lt;li&gt;合并数据，例如 一个标题是A，一个是B，最后就是 A|B&lt;/li&gt;
&lt;li&gt;用户自己实现冲突合并逻辑。&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
&lt;li&gt;Custom conflict resolution logic&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;自己实现冲突解决一般有两种方案：On Read 和 On Write ，相关细节比较简单&lt;/p&gt;

&lt;h4 id=&#34;what-is-a-conflict&#34;&gt;What is a conflict?&lt;/h4&gt;

&lt;p&gt;上面说的两个人同时修改标题的冲突很明显，所以可以通过一些程序控制，实际上也有一些不明显的的冲突。&lt;/p&gt;

&lt;p&gt;例如两个人预定会议室，同一个会议室同一时间只能有一个人预定成功，但如果是 multi-leader 可能两个人都预订成功了。
或者注册的时候要求 username 不重复，但是有可能两个人同时申请同一个 username。&lt;/p&gt;

&lt;h3 id=&#34;2-multi-leader-replication-topologies&#34;&gt;(2) Multi-Leader Replication Topologies&lt;/h3&gt;

&lt;p&gt;Replication Topologies 是用来描述数据在节点间传播的路径。例如 circular 、star、all-2-all。&lt;/p&gt;

&lt;p&gt;每一种都有优势和劣势。&lt;/p&gt;

&lt;h2 id=&#34;3-leaderless-replication&#34;&gt;3. Leaderless Replication&lt;/h2&gt;

&lt;p&gt;实际上 cassandra 采用了 leaderless 的架构，没有主节点。&lt;/p&gt;

&lt;h3 id=&#34;1-writing-to-the-database-when-a-node-is-down&#34;&gt;(1) Writing to the Database When a Node Is Down&lt;/h3&gt;

&lt;p&gt;leaderless 中，不存在 failover，因为没有主节点，因为一个节点在读数据的时候，并不需要从所有的节点都读数据，
二是给每个节点发送数据，然后每个节点都返回数据，根据 Version numbers 决定哪个是正确的。&lt;/p&gt;

&lt;h4 id=&#34;read-repair-and-anti-entropy&#34;&gt;Read repair and anti-entropy&lt;/h4&gt;

&lt;p&gt;系统需要保证每个 replica 上面的数据都是最终一致的，如果出现了节点挂掉然后恢复，如何保证它的数据是一致？&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Read repair ，当用户读书节的时候，能够发现 stale 的值并且修复&lt;/li&gt;
&lt;li&gt;Anti-entropy process 通过后台进程扫描数据发现不一致&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&#34;quorums-for-reading-and-writing&#34;&gt;Quorums for reading and writing&lt;/h4&gt;

&lt;p&gt;也就是上面所说的，读和写不需要全成功，只要保证 读成功数 r 和 写成功数 w 满足 r+w&amp;gt;n即可。这个叫做仲裁原理，类似抽屉原理，
n个抽屉，里面有 w 个抽屉放了一样的纸条，打开r个抽屉，想要看到纸条内容，只要 r + w &amp;gt;n。
一般情况下 r=w=(n+1)/2&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Limitations of Quorum Consistency&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;上面我们说了 r=w=(n+1)/2，但是如果你的数据库写的量特别小，读特别大，你也可以设置 r=n，w=1，总之只要覆盖即可。&lt;/p&gt;

&lt;p&gt;如果你对一致性要求不高，也可以使用 w+r &amp;lt;= n，这样返回值可能 stale，不过这样的可用性更高。&lt;/p&gt;

&lt;p&gt;然而尽管使 w+r &amp;gt; n，也会有一些其他的异常情况：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;sloppy quorum 下无法保证&lt;/li&gt;
&lt;li&gt;concurrently 写，和上一节类似，出现了写冲突？&lt;/li&gt;
&lt;li&gt;读和写 concurrently，这时候写只提交了一部分，其他的可能还未完全在 w 个节点成功&lt;/li&gt;
&lt;li&gt;如果写 成功不够w，但是这时候也没法 rollback，后续可能会读到失败的值，也可能读不到&lt;/li&gt;
&lt;li&gt;如果保存了 new value 的节点失败重启，从另一个保存了 old value 的节点修复数据，导致包含 new value 数据的节点少于 n&lt;/li&gt;
&lt;li&gt;还有极端情况，例如  “Linearizability and quorums”.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;因此，即便是 达到了 quorum 的条件，也没法真正的实现一致性。
特别的，至于在 single-leader 中的 “reading your writes, monotonic reads, consistent prefix reads” 等弱一致性也没法得到保障。
所以前面说的异常可能也会发生，强一致性需要 consensus 或者 transaction 来保证。&lt;/p&gt;

&lt;h4 id=&#34;monitoring-staleness&#34;&gt;Monitoring staleness&lt;/h4&gt;

&lt;p&gt;对于应用而言，是否返回最新版本的数据是很重要的。对于 leader-based 的设计，所有的问题都可以交给 replication-lag 来解决，
可以通过监控 replication-lag 监控系统
对于 leaderless 的设计，所有的数据并没有确定的顺序，监控很困难，而且如果没有 anti-entropy，数据可能过时很久都不修复。&lt;/p&gt;

&lt;h3 id=&#34;sloppy-quorums-and-hinted-handoff&#34;&gt;“Sloppy Quorums and Hinted Handoff”&lt;/h3&gt;

&lt;p&gt;简单介绍一下，例如 cassandra 中，保存三副本，读写都要成功 2 个 replica，我们写数据要成功 2个，
但是如果此时这三个节点挂了两个，只能写成功一个，可以先将另一份数据放到其他的节点上。
等对应的剩下的节点都恢复了，再将数据放回去，这个过程就是 hinted handoff。&lt;/p&gt;

&lt;h3 id=&#34;multi-datacenter-operation-1&#34;&gt;Multi-datacenter operation&lt;/h3&gt;

&lt;p&gt;这时候我们还是有N个副本，我们需要保证每在 每个 datacenter 都达到了 对应的 n/2 个副本。&lt;/p&gt;

&lt;h3 id=&#34;detecting-concurrent-writes&#34;&gt;Detecting Concurrent Writes&lt;/h3&gt;

&lt;p&gt;类似之前的 handling write conflict，也是由于顺序不一样导致，所以也需要有保证 converge 的方法，
这样的方案也就是我们前面说的一样。但是我们这里讨论的更细致一点。&lt;/p&gt;

&lt;h4 id=&#34;last-write-wins-discarding-concurrent-writes&#34;&gt;Last write wins (discarding concurrent writes)&lt;/h4&gt;

&lt;p&gt;对于不知道哪个 happens-first 的操作，我们称之为 concurrent，他们的 order 是不知道的。&lt;/p&gt;

&lt;p&gt;尽管他们没有 order，我们可以人为给一个 order，例如 timestamp，也就是 Last write wins。&lt;/p&gt;

&lt;p&gt;llw 解决了冲突问题，但是丢失了一些数据。所有使用 cassandra 的时候有人会给每个 key 后面加一个 uuid，这样能保证每个 key 多次写都不会丢失。&lt;/p&gt;

&lt;h4 id=&#34;the-happens-before-relationship-and-concurrency&#34;&gt;The “happens-before” relationship and concurrency&lt;/h4&gt;

&lt;p&gt;happens-before 的关系是决定是否并发的关键，以下情况下我们称两个操作是 concurrency：
“neither happens before the other (neither knows about the other)”&lt;/p&gt;

&lt;p&gt;因此，对于任何两个操作 A 和 B，只有两种可能，A happens-before B，B happens-before A，A 和 B 是 concurrency。&lt;/p&gt;

&lt;h4 id=&#34;capturing-the-happens-before-relationship&#34;&gt;Capturing the happens-before relationship&lt;/h4&gt;

&lt;p&gt;上面已经知道两个操作只能有 3 种关系，我们如果发现 happens-before  的关系呢？其实有算法的。&lt;/p&gt;

&lt;p&gt;首先假如 A 和 B两个人都在往同一个购物车添加商品，一开始 A添加了 商品1，B添加了商品 2，这两个是 concurrency的，
然后 A再添加 3，B添加4，这时候 这两个操作都是 3和4是 concurrency的，但是1和2 都是 happens-before 3 和4 的。&lt;/p&gt;

&lt;p&gt;发现 concurrency 关系的算法如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;server 的每个key有一个 version number，每次更新写这个 key 的时候，version number 就 +1并且跟新数据。&lt;/li&gt;
&lt;li&gt;当 client 读数据，返回所有没有 overwritten 的数据和最新的 version number。&lt;/li&gt;
&lt;li&gt;client 写数据的时候，必须包含之前读到的那个 version number，并且合并所有的数据，&lt;/li&gt;
&lt;li&gt;当 server 收到带 version number 的写请求，覆盖所有小于等于 version number 的数据，保留大于 version number 的数据。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这样的算法就能发现所有的 happens-before 关系&lt;/p&gt;

&lt;h4 id=&#34;merging-concurrently-written-values&#34;&gt;Merging concurrently written values&lt;/h4&gt;

&lt;p&gt;类似前面的 conflict resolution，上面的算法需要 client 进行数据合并，合并算法大家自己查资料，例如 CRDT 。&lt;/p&gt;

&lt;h4 id=&#34;version-vectors&#34;&gt;Version vectors&lt;/h4&gt;

&lt;p&gt;上面的算法是 single-replica 情况，对于 multi-replica 的情况，需要每个副本一个 Version number，这被叫做 Version vectors。&lt;/p&gt;

&lt;h2 id=&#34;summary&#34;&gt;SUMmary&lt;/h2&gt;

&lt;p&gt;通过上面的讨论，我们明白了一句话，其实分布式 replica 最大的问题就是发现 happens-before 的依赖关系，也就是操作的顺序。&lt;/p&gt;

&lt;p&gt;另外有大神说过，分布式只有两个问题， exactly-once 和 order。其中 exactly-once 后续会讨论，order 就是我们这里讨论最多的。&lt;/p&gt;

&lt;h2 id=&#34;二-partitioning&#34;&gt;二、Partitioning&lt;/h2&gt;

&lt;p&gt;分区，also known as Sharding。&lt;/p&gt;

&lt;h1 id=&#34;第五部分-案例&#34;&gt;第五部分、案例&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://medium.freecodecamp.org/how-to-system-design-dda63ed27e26&#34;&gt;https://medium.freecodecamp.org/how-to-system-design-dda63ed27e26&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://blog.gainlo.co/index.php/2015/10/22/8-things-you-need-to-know-before-system-design-interviews/&#34;&gt;http://blog.gainlo.co/index.php/2015/10/22/8-things-you-need-to-know-before-system-design-interviews/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://engineering.videoblocks.com/web-architecture-101-a3224e126947&#34;&gt;https://engineering.videoblocks.com/web-architecture-101-a3224e126947&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;第六部分-资料&#34;&gt;第六部分、资料&lt;/h1&gt;

&lt;p&gt;es设计架构，良心参考资料：
&lt;a href=&#34;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-i-7ac9a13b05db&#34;&gt;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-i-7ac9a13b05db&lt;/a&gt;
&lt;a href=&#34;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-ii-6db4e821b571&#34;&gt;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-ii-6db4e821b571&lt;/a&gt;
&lt;a href=&#34;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-iii-8bb6ac84488d&#34;&gt;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-iii-8bb6ac84488d&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;lsm b+tree 资料
&lt;a href=&#34;https://medium.com/databasss/on-disk-io-part-3-lsm-trees-8b2da218496f&#34;&gt;https://medium.com/databasss/on-disk-io-part-3-lsm-trees-8b2da218496f&lt;/a&gt;&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>design-data-intensive-architecture</title>
      <link>https://dengziming.github.io/post/data-intensive-architecture/partition/</link>
      <pubDate>Sat, 22 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/data-intensive-architecture/partition/</guid>
      
        <description>

&lt;h1 id=&#34;第一部分-前言&#34;&gt;第一部分、前言&lt;/h1&gt;

&lt;p&gt;ACID，bigdata，NoSql ，bigTable ，CAP，2PC，3PC，quorum，raft，paxos，cloud，real-time，高并发架构将会逐渐成为基础。&lt;/p&gt;

&lt;p&gt;主要内容：
（一）基础&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;“reliability, scalability, and maintainability”&lt;/li&gt;
&lt;li&gt;数据模型&lt;/li&gt;
&lt;li&gt;存储引擎&lt;/li&gt;
&lt;li&gt;数据结构&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;（二）分布式&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;replication&lt;/li&gt;
&lt;li&gt;partitioning/sharding&lt;/li&gt;
&lt;li&gt;transaction&lt;/li&gt;
&lt;li&gt;distributed system&lt;/li&gt;
&lt;li&gt;consistency and consensus&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;（三）交互&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;batch process&lt;/li&gt;
&lt;li&gt;realtime&lt;/li&gt;
&lt;li&gt;put everything together&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;（四）案例及资料&lt;/p&gt;

&lt;h1 id=&#34;第二部分-数据系统的基础&#34;&gt;第二部分、数据系统的基础&lt;/h1&gt;

&lt;h2 id=&#34;一-reliable-scalable-和-maintainable-的含义&#34;&gt;一、Reliable,Scalable 和 Maintainable 的含义&lt;/h2&gt;

&lt;p&gt;数据系统分为 data-intensive 和 compute-intensive，一般的大数据应用会采用的技术：
1. 存储，方便后续的查询 &amp;ndash; databases
2. 缓存，加速访问 &amp;ndash; caches
3. 允许用户通过关键字搜索、按照过滤  &amp;ndash; indexes
4. 给另一个进程发送消息，异步处理  &amp;ndash; streaming process
5. 周期性处理全量数据  &amp;ndash; batch process&lt;/p&gt;

&lt;p&gt;这一切都是需要数据系统，我们并不会从头开始搭建存储、计算系统，因为有很多现成的技术可以应用。数据库、消息队列功能都很相似，但是实现完全不同，导致不同的性能，所以我们用处不同，我们为什么要将他们结合到一起呢？&lt;/p&gt;

&lt;p&gt;其实许多工具功能比较重复，redis 其实也可以当消息队列，kafka 也可以做数据持久化。这些工具的界限越来越模糊。第二是我们得系统一般比较复杂，一个工具完成不了，我们必须组件一个数据系统。组件的过程
有很多需要考虑的事情，其中最重要的三点：
Reliable： 系统必须一致能够提供服务，即便是发生了一些不可预知的错误
Scalable： 系统升级、功能扩展、访问量增加，导致我们需要能够很方便扩展我们的系统
Maintainable： 随着时间推移，可能有很多其他人接受项目，必须方便别人进行维护&lt;/p&gt;

&lt;p&gt;有关这三点的介绍，我们逐渐展开：&lt;/p&gt;

&lt;h3 id=&#34;1-reliable&#34;&gt;1. Reliable&lt;/h3&gt;

&lt;h3 id=&#34;2-scalable&#34;&gt;2. Scalable&lt;/h3&gt;

&lt;h3 id=&#34;3-maintainable&#34;&gt;3. Maintainable&lt;/h3&gt;

&lt;h2 id=&#34;二-数据模型和查询语言&#34;&gt;二、数据模型和查询语言&lt;/h2&gt;

&lt;p&gt;数据模型是最重要的一部分，&lt;/p&gt;

&lt;h3 id=&#34;1-关系型与文档型&#34;&gt;1.关系型与文档型&lt;/h3&gt;

&lt;h3 id=&#34;2-nosql&#34;&gt;2.NoSQL&lt;/h3&gt;

&lt;h3 id=&#34;3-object-relational-mismatch&#34;&gt;3. Object-Relational Mismatch&lt;/h3&gt;

&lt;h3 id=&#34;4-many-to-one-and-many-to-many-relationships&#34;&gt;4. Many-to-One and Many-to-Many Relationships&lt;/h3&gt;

&lt;h3 id=&#34;5-query-languages-for-data&#34;&gt;5. Query Languages for Data&lt;/h3&gt;

&lt;h3 id=&#34;6-graph-like-data-models&#34;&gt;6.Graph-Like Data Models&lt;/h3&gt;

&lt;h2 id=&#34;三-storage-and-retrieval&#34;&gt;三、Storage and Retrieval&lt;/h2&gt;

&lt;h3 id=&#34;1-hash-indexes&#34;&gt;1. Hash Indexes&lt;/h3&gt;

&lt;h3 id=&#34;2-sstables-and-lsm-trees&#34;&gt;2. SSTables and LSM-Trees&lt;/h3&gt;

&lt;h3 id=&#34;3-b-trees&#34;&gt;3. B-Trees&lt;/h3&gt;

&lt;h3 id=&#34;4-multi-column-indexes&#34;&gt;4. Multi-column indexes&lt;/h3&gt;

&lt;h3 id=&#34;5-full-text-search-and-fuzzy-indexes&#34;&gt;5. Full-text search and fuzzy indexes&lt;/h3&gt;

&lt;h3 id=&#34;6-keeping-everything-in-memory&#34;&gt;6.Keeping everything in memory&lt;/h3&gt;

&lt;h3 id=&#34;7-olap-vs-oltp&#34;&gt;7. OLAP vs OLTP&lt;/h3&gt;

&lt;h3 id=&#34;8-数据仓库&#34;&gt;8. 数据仓库&lt;/h3&gt;

&lt;h3 id=&#34;9-stars-and-snowflakes-schemas-for-analytics&#34;&gt;9. Stars and Snowflakes: Schemas for Analytics&lt;/h3&gt;

&lt;h3 id=&#34;10-column-oriented-storage&#34;&gt;10. Column-Oriented Storage&lt;/h3&gt;

&lt;h3 id=&#34;11-column-compression&#34;&gt;11. Column Compression&lt;/h3&gt;

&lt;h3 id=&#34;12-sort-order-in-column-storage&#34;&gt;12. Sort Order in Column Storage&lt;/h3&gt;

&lt;h3 id=&#34;13-writing-to-column-oriented-storage&#34;&gt;13. Writing to Column-Oriented Storage&lt;/h3&gt;

&lt;p&gt;前面写的有关列存储的优化在数据仓库中很重要，排序、位图索引、压缩都能加速查询，但是会让写数据更加麻烦。&lt;/p&gt;

&lt;p&gt;以 BTree 为理论的写方法，在这里完全没有用，如果你要在数据中插入一条数据，你需要重写所有的数据。&lt;/p&gt;

&lt;p&gt;但是还好的是，如果是 LSM-tree 的方式，首先写进内存排序，然后写磁盘。查询的时候，也是两部分结果进行合并，Vertica 数据仓库就是这么做的。&lt;/p&gt;

&lt;h3 id=&#34;14-aggregation-data-cubes-and-materialized-views&#34;&gt;14. Aggregation: Data Cubes and Materialized Views&lt;/h3&gt;

&lt;p&gt;并不是所有的数据仓库都是使用列存储，很多其他存储方式也会用到，但是列存储确实能够很大程度加快访问，所以越来越流行。&lt;/p&gt;

&lt;p&gt;另一种值得一提的数据仓库技术就是 materialized aggregates。前面所说的，查询经常会遇到一些聚合， 例如 count，sum，如果一个查询被很多查询共用，可以将结果缓存。
其中一种方法就是 materialized view，类似查询视图，只不过这个视图的结果已经计算好保存起来了。如果数据改变了，你的 materialized view 也要改变。&lt;/p&gt;

&lt;p&gt;一种常见的案例就是 data cube 后者 OLAP cube，就是根据不同纬度就行聚合后的结果。举个列子，如果表格有 日期、商品种类、销售额三列，我们可以计算销售额在 日期、商品种类 两个维度下的聚合只，得到一个二维表格。
二维表格的两个维度分别为日期和商品种类，表格的值就是销售额，然后如果要计算每个维度下的 sum，只需要将对应维度所有值 sum 到一起。&lt;/p&gt;

&lt;p&gt;实际上，表格都不止两个维度，假如有五个维度，情况就很复杂了。但是原则不变，每个 cell 保存五个维度组合下的聚合值。&lt;/p&gt;

&lt;p&gt;通过 cube 可以加速某些查询，但是如果要计算订单额度大于 100 的比例，那就没法计算了，因为额度很难作为一个维度。所以一般只作为一个加快部分查询的工具。&lt;/p&gt;

&lt;h2 id=&#34;四-encoding-and-evolution&#34;&gt;四、Encoding and Evolution&lt;/h2&gt;

&lt;p&gt;服务总是要变的，服务变了，服务之间的代码也要变。服务端可以做滚动升级，也就是部分服务器先升级，保证没错的话，其他的继续升级。
而客户端也就是用户端，代码变化必须做到向前向后兼容，也就是说，高版本要能够处理低版本的数据，这很简单；低版本的代码，必须能从高版本的架构读数据，这是比较难的。
接下来我们将会看一些数据结构，JSON, XML, Protocol Buffers, Thrift, and Avro ，主要看他们在这些data storage and for communication系统中如何使用：
in web services, Representational State Transfer (REST), and remote procedure calls (RPC), as well as message-passing systems such as actors and message queues&lt;/p&gt;

&lt;h3 id=&#34;1-formats-for-encoding-data&#34;&gt;1. Formats for Encoding Data&lt;/h3&gt;

&lt;p&gt;一般的应用都会处理两种数据。内存主要是通过对象、数组、树、hash表等，而需要保存、传输时候，需要进行序列化，由于指针引用序列化后是没用的，所以序列化后的结构和内存结构完全不用。
从设计模式角度考虑，我们需要进行一个适配，两边都要适配的话，实际上就是做一个转换。内存到 byte sequence 的过程称为 encoding（或者 serialization、marshalling），反过来讲decoding（deserialization、parsing、unmarshalling）&lt;/p&gt;

&lt;p&gt;** 专业术语冲突
serialization 在数据库的事务中也会用到，但是完全是另外一码子事情，后续会进行介绍。这里使用的时候我们以 encoding 为主，虽然平时 serialization 用的更多。&lt;/p&gt;

&lt;p&gt;这里我们将会以一条数据为例,对它进行编码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
    &amp;quot;userName&amp;quot;: &amp;quot;Martin&amp;quot;,
    &amp;quot;favoriteNumber&amp;quot;: 1337,
    &amp;quot;interests&amp;quot;: [&amp;quot;daydreaming&amp;quot;, &amp;quot;hacking&amp;quot;]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;1-language-specific-formats&#34;&gt;(1) Language-Specific Formats&lt;/h4&gt;

&lt;p&gt;java.io.Serializable 属于java 自带的序列化机制，很多编程语言都自带了。当然也有很多第三方的，比如 Kryo for Java。这些很方便，因为你只需要直接读然后调用java对应的方法即可。但是问题也很多。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;通用性问题，不多说。&lt;/li&gt;
&lt;li&gt;数据会有类型，例如java 序列化必须只能解析为特定的类。所以会定义很多类，这样可能导致一些人得到你的类信息，然后远程植入代码。&lt;/li&gt;
&lt;li&gt;版本问题。&lt;/li&gt;
&lt;li&gt;效率问题。&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&#34;2-json-xml-and-binary-variants&#34;&gt;(2) JSON, XML, and Binary Variants&lt;/h4&gt;

&lt;p&gt;JSON, XML 一般都很熟悉，优缺点都很明显，二进制的编码在某些内网的应用中很常见，还能节约空间。例如 MassagePack 就是二进制的 json。&lt;/p&gt;

&lt;h4 id=&#34;3-thrift-and-protocol-buffers&#34;&gt;(3) Thrift and Protocol Buffers&lt;/h4&gt;

&lt;p&gt;Thrift and Protocol Buffers 很多人都知道，作为两种序列化的框架，当然还提供了别的功能，例如 RPC 通信接口。都需要定义一个schema：&lt;/p&gt;

&lt;p&gt;Thrift 的定义格式：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;struct Person {
  1: required string       userName,
  2: optional i64          favoriteNumber,
  3: optional list&amp;lt;string&amp;gt; interests
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;pb 的定义格式：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;message Person {
    required string user_name       = 1;
    optional int64  favorite_number = 2;
    repeated string interests       = 3;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后他们都有代码生成的工具，可以针对各种语言生成相应的代码，用来 encode 和 decode 数据。thrift 的格式有两种 BinaryProtocol and CompactProtocol。&lt;/p&gt;

&lt;p&gt;BinaryProtocol 表示上面的数据格式需要 59 bytes ：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;0b 00 01 00 00 00 06 4d 61 72 74 69 6e     0b 是 type代表 String，紧接着 00 01 代表 field tag 为1，然后 00 00 00 06 代表长度为6，剩下的六bytes 代表数据。
0a 00 02 00 00 00 00 00 00 05 39           和刚才一样， 0a 代表int64 ，
0f 00 03 0b 00 00 00 02                    0f 代表list，00 03代表 field tag 为3，0b 代表 item type 为 string，00 00 00 02 代表长度为2，
	00 00 00 0b 64 61 79 ....              长度和数据
	00 00 00 07 68 ..... 				   长度和数据
00											结束标志
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;CompactProtocol 和 BinaryProtocol 的结构类似，但是进行了一些压缩，需要 33 bytes&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;18 06 4d ....  							  18 是 00011000 field type and tag number 合在一起，0001 是 tag，
										  1000 是 type 是string （为什么8 代表 string，上面是11），06 长度，后面是数据
										  另外int类型并不是占了 64位，这里的 06是两位，共 8 字节，
										  最高位代表数据是否已经完整，剩下的七位是数据。如果数据不完整，需要再占8位。这样 -64 到 63 之间的数据只占了1byte
										  
16 f2 14 								  16 是 00010110，0001 代表 field tag 比上一个加一，0110 数据类型 后面的 f214 是数据，f2 最高位是1，数据没完，需要往下一位读。
19 28
    0b 64 .....
    07 68 ....
00
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ProtocolBuffer 和 Thrift 的 CompactProtocol 类似，需要 33 bytes&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;0a 06 4d 61 72 74 69 6e       			  0a 是 tag(00001) 和 type(010), 06 是长度。 后面是数据
10 b9 0a 								  10 是tag (00010) 和 type（000）,后面是长度+数据
1a 0b ........							  1a 是 tag 和 type，后面是长度+数据
1a 07 ........							  1a 是 tag 和 type，后面是长度+数据
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;和 CompactProtocol 稍微有些不一样，tag+type 为两位，另外 list 类型直接存两个。&lt;/p&gt;

&lt;p&gt;需要注意的是，前面我们的例子中，数据要么是 要么是 required or optional，实际上这对于数据的存储没有任何影响，都是一样的存储，只是在最后解析的时候影响，例如你设置的是 required ，但是没有解析出来，会失败。&lt;/p&gt;

&lt;h4 id=&#34;4-field-tags-and-schema-evolution&#34;&gt;(4) Field tags and schema evolution&lt;/h4&gt;

&lt;p&gt;前面讲过数据需要改变，那 pb 和 thrift 如何应对schema 的变化，做到向前向后兼容呢？
其实我们从上面的数据可以看出，最重要就是 field tag 了，只要 field tag 唯一，哪怕是增加了数据，减少了数据，最终的结果其实还是一样的。你可以改变变量名字，但是不能改变 tag。
首先是向后兼容，只要你新加的 field tag 不被设置为 required，就不会报错，另外删除数据也是，不能删除 required 的数据。
另外是向前兼容，如果是旧的代码读取新的数据，如果遇到不认识的 field tag ，会直接忽略。&lt;/p&gt;

&lt;h4 id=&#34;5-datatypes-and-schema-evolution&#34;&gt;(5) Datatypes and schema evolution&lt;/h4&gt;

&lt;p&gt;数据类型发生变化，一方面可能是数据精度收到影响，另一方面，我们可以看出 pb 是可以讲啊 list 类型变成 optional 的，但是 thrift 不行，因为 thrift 提供了 list 类型，但是这样可以用来写嵌套数据。&lt;/p&gt;

&lt;h4 id=&#34;6-avro&#34;&gt;(6) avro&lt;/h4&gt;

&lt;p&gt;avro 也是一个序列胡框架，他和 pb、thrift 不同点在于没有 tagNumber，这其实很适合动态生成schema，我们不介绍了。&lt;/p&gt;

&lt;h4 id=&#34;7-dynamically-generated-schemas&#34;&gt;(7) Dynamically generated schemas&lt;/h4&gt;

&lt;p&gt;如果我们的数据增加了一列，并且减少了一列，对于 avro 来说，只需要重新生成一个 schema 即可，而如果使用 pb、thrift，需要手动进行配置tag和field 的映射，&lt;/p&gt;

&lt;h4 id=&#34;8-code-generation-and-dynamically-typed-languages&#34;&gt;(8) Code generation and dynamically typed languages&lt;/h4&gt;

&lt;p&gt;代码生成，例如 pb 可以生成 java 和 python 的代码。&lt;/p&gt;

&lt;h4 id=&#34;9-the-merits-of-schemas&#34;&gt;(9) The Merits of Schemas&lt;/h4&gt;

&lt;h3 id=&#34;二-modes-of-dataflow&#34;&gt;二、 Modes of Dataflow&lt;/h3&gt;

&lt;p&gt;我们需要在进程之间发送数据，&lt;/p&gt;

&lt;h4 id=&#34;1-dataflow-through-databases&#34;&gt;1.Dataflow Through Databases&lt;/h4&gt;

&lt;h4 id=&#34;2-dataflow-through-services-rest-and-rpc&#34;&gt;2.Dataflow Through Services: REST and RPC&lt;/h4&gt;

&lt;h4 id=&#34;3-message-passing-dataflow&#34;&gt;3.  Message-Passing Dataflow&lt;/h4&gt;

&lt;h1 id=&#34;第二部分-分布式&#34;&gt;第二部分、分布式&lt;/h1&gt;

&lt;p&gt;使用分布式的原因很多，例如 Scalability，high available，latency。数据分布式有很多种方式，首先是副本、分区，也就是 Replication 和 Partition。&lt;/p&gt;

&lt;h2 id=&#34;一-replication&#34;&gt;一、Replication&lt;/h2&gt;

&lt;p&gt;Replication 就是将你的数据放在多个节点，这样有很多好处。首先是数据从地理上可以隔用户更近，保持高可用，增加吞吐量。&lt;/p&gt;

&lt;p&gt;本章我们假设数据都是完整的进行副本存放，后面再讲分区。数据的副本解决难点就是数据变化的一致性，如果数据一致不变，那就太简单了。
对于变化，有三种设计：single-leader, multi-leader, and leaderless&lt;/p&gt;

&lt;h3 id=&#34;1-leaders-and-followers&#34;&gt;1. Leaders and Followers&lt;/h3&gt;

&lt;p&gt;每个节点上面的副本叫做 replica ，当有 multi replica 的时候，一个问题出现了，怎么保证数据写到了每一个副本。数据库的每一次写都必须保证被每个副本处理，否则就会出现不一致。
最常用的解决方案就是 leader-based-replication，也叫 active-passive、master-slave。&lt;/p&gt;

&lt;p&gt;在主从结构中，写只能发送给 leader，leader 写的时候会发送数据给 follower，而读操作可以读任何一个节点。&lt;/p&gt;

&lt;h4 id=&#34;1-synchronous-versus-asynchronous-replication&#34;&gt;(1) Synchronous Versus Asynchronous Replication&lt;/h4&gt;

&lt;p&gt;replicated 系统重要特点就是 同步写还是异步写，一般关系型数据库可以配置，其他的都是硬编码写死的。&lt;/p&gt;

&lt;p&gt;客户端发送数据给leader，然后leader转发给follower，最后通知client。如果 leader 收到 follower 的确认消息再回复client，这就是同步。
如果leader发送给了 follower后就直接回复client，就是异步。一般情况，系统都只有一个follower是同步，其余的都是异步。这叫 semi-synchronous。&lt;/p&gt;

&lt;p&gt;为了效率经常设置为 Asynchronous，这样如何 leader 失败了而且不恢复，那还没有被副本保存的数据就丢失了，这样的好处就是可以提供持续服务，尽管follower很慢。&lt;/p&gt;

&lt;p&gt;Weakening durability 可能是一个比较糟糕的 trade-off，但是 Asynchronous 还是很常用，尤其是地理上分布式的集群。&lt;/p&gt;

&lt;p&gt;当然也有很多其他的方案在研究中，例如 Azure 使用了 chain replication。&lt;/p&gt;

&lt;h4 id=&#34;2-setting-up-new-followers&#34;&gt;(2) Setting Up New Followers&lt;/h4&gt;

&lt;p&gt;系统有时候需要添加新的节点，如何保证新节点和leader的数据一致呢？直接复制数据过来肯定是不行的，leader 一直处在 flux 中，一般步骤有四步：
1. leader 在某个确定一致性的时间点拍一个 snapshot 而不用锁住系统，大部分数据库都提供了这个功能，有的能通过第三方工具提供这个功能。
2. follower 将数据复制过去。
3. follower 将从这个时间点以后的数据复制过去，这需要拍 snapshot 时候的日志位置，这个点的名字叫 Log sequence number 或者 binlog coordinates。
4. 当 follower 将所有的change都同步后，我们称之为 catch up，就可以和其他follower一样，处理 leader 发生的变化了。&lt;/p&gt;

&lt;p&gt;真正的步骤其实是有很多不同，有时候可以自动配置，有时候需要手动配置。&lt;/p&gt;

&lt;h4 id=&#34;3-handling-node-outages&#34;&gt;(3) Handling Node Outages&lt;/h4&gt;

&lt;p&gt;有时候节点会挂掉，可能是我们不知道的原因，也可能是为了安装系统模块而重启，这时候应该怎么保证集群的高可用&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;follower 挂掉&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;follower 挂点然后重启的步骤是很简单的，类似前面的添加新节点，启动以后，根据日志能找到最后一个处理的事务，然后从leader请求这个时间点以后的更改，然后进行更改，完成后就 catch up。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;leader failover&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;leader 挂掉后可能麻烦一点，必须马上选择一名新的leader，这个过程被称为 failover&lt;/p&gt;

&lt;p&gt;failover的具体步骤有4：
1. 确定leader失败了，失败的原因很多，磁盘、电源等等，目前用的最多的是 timeout 方法，节点相互 bounce message，如果突然在某个时间点收不到，就是失败了。
2. 选择新的 leader，需要在大多数的follwer 进行选举，最好是选一个数据最新的。所有的节点同意这个节点成为 consensus。
3. 告知系统使用新的leader，client 发送数据到新的 leader，旧的 leader 回复后也要认同新的 leader。&lt;/p&gt;

&lt;p&gt;failover 总是会有些不期而遇的问题：
1. 如果是 asynchronous replica ，可能遇到一些数据没有同步过来，导致数据少了，如果这时候 older leader起来了，就会发生错误，这时候一般选择 discard 这部分数据。
2. 如果系统和别的系统进行结合，discard 是很危险的，github 发生了一次 mysql leader挂掉，然后discard 了一部分，删掉了部分自增的primary key，可是这部分pk已经保存到 redis了，导致了错误
3. 有时候两个节点都认为自己是 leader，这个叫做 split brain，这时候需要去掉一个，有可能两个都被去掉了。
4. timeout 的具体值多少合适？如果太长了，可能导致发现的晚。如果太短了可能误判，尤其是系统压力大的时候，还重新换一次会造成更大的压力。&lt;/p&gt;

&lt;p&gt;这些问题解决起来还是很棘手的，所以有很多算法，我们后续会介绍。&lt;/p&gt;

&lt;h4 id=&#34;4-implementation-of-replication-logs&#34;&gt;(4) Implementation of Replication Logs&lt;/h4&gt;

&lt;p&gt;上面各种恢复，添加，都涉及到日志，日志记录了数据的更改，实际上我们学习Hadoop 的时候，也知道里面有日志，现在我们来研究一下，日志怎么实现。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Statement-based replication&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;基于 statement 的 replication，这是最简单的方法，数据库记录每一次更新，也就是说，你的 UPDATE、INSERT、DELETE都会记录到日志中，然后发送给follower。
这样的问题是：
对于 current_timestamp、random 这样的方法返回值不一样，基于判断的例如 update &amp;hellip; where &amp;hellip; 必须按照顺序，一些触发器、存储过程等 side effects。&lt;/p&gt;

&lt;p&gt;对于这些问题，当然也有特定的解决方案，例如先计算一些可能导致不一致的函数。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;WAL-log&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;前面讲过 B-Tree 和 LSM-Tree，都是基于WAL-log，是一种只能 append 的二进制日志，leader 不仅将 wal-log 保存到磁盘，而且发送到 followers。&lt;/p&gt;

&lt;p&gt;WAL 的 disadvantage 是太底层，wal 包含了磁盘那个位置的 block 需要修改哪些 bytes，这样耦合性比较高，如果format 改变了，基本上很难让 leader和follower 使用不同版本。&lt;/p&gt;

&lt;p&gt;这个问题看起来更像是实现的问题，如果 replication 的协议支持follower 使用新的版本，就能完成 zore-downtime 的 upgrade。
首先更新 follower的版本，然后让leader failover，选择一个 follower 为 leader。如果replication 的协议不支持版本不一直，这就叫做 wal-shipping，会造成 upgrade-downtime。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Logical (row-based) log replication&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这种方式就是 replication log 和具体的 storage device 解耦，使用不同格式。 relational database 的 logical log 一般都是一系列表示行的recorders。一个改变很多行的 statement 会
产生很多行这样的日志，后面有一个transaction 完成的标识符。这种方式是解耦的，所以可以容忍不同的版本，甚至不同的存储引擎。这也让外部的系统容易获得数据，
例如数仓、二级索引，外部缓存，这个技术叫做 change data capture。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;trigger based replication&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;前面讲的都是数据库系统的，有时候我们需要更灵活的，例如你只想一部分数据，或者你想从一个数据库迁移到另一个数据库。这样你就应该将replication 提升到应用程序层面。
很多数据库提供了工具完成这个事情。许多关系型数据库提供了一项功能：triggers and stored procedures。trigger 能让你注册用户代码，一旦数据发生改变，将会将数据放到另一给表格，外部系统可以访问。&lt;/p&gt;

&lt;h4 id=&#34;5-problems-with-replication-lag&#34;&gt;(5) Problems with Replication Lag&lt;/h4&gt;

&lt;h2 id=&#34;二-partitioning&#34;&gt;二、Partitioning&lt;/h2&gt;

&lt;p&gt;分区，also known as Sharding。&lt;/p&gt;

&lt;h1 id=&#34;第五部分-案例&#34;&gt;第五部分、案例&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://medium.freecodecamp.org/how-to-system-design-dda63ed27e26&#34;&gt;https://medium.freecodecamp.org/how-to-system-design-dda63ed27e26&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://blog.gainlo.co/index.php/2015/10/22/8-things-you-need-to-know-before-system-design-interviews/&#34;&gt;http://blog.gainlo.co/index.php/2015/10/22/8-things-you-need-to-know-before-system-design-interviews/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://engineering.videoblocks.com/web-architecture-101-a3224e126947&#34;&gt;https://engineering.videoblocks.com/web-architecture-101-a3224e126947&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;第六部分-资料&#34;&gt;第六部分、资料&lt;/h1&gt;

&lt;p&gt;es设计架构，良心参考资料：
&lt;a href=&#34;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-i-7ac9a13b05db&#34;&gt;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-i-7ac9a13b05db&lt;/a&gt;
&lt;a href=&#34;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-ii-6db4e821b571&#34;&gt;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-ii-6db4e821b571&lt;/a&gt;
&lt;a href=&#34;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-iii-8bb6ac84488d&#34;&gt;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-iii-8bb6ac84488d&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;lsm b+tree 资料
&lt;a href=&#34;https://medium.com/databasss/on-disk-io-part-3-lsm-trees-8b2da218496f&#34;&gt;https://medium.com/databasss/on-disk-io-part-3-lsm-trees-8b2da218496f&lt;/a&gt;&lt;/p&gt;
</description>
      
    </item>
    
  </channel>
</rss>