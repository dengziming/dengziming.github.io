<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hadoop源码 on 数据分析师之旅</title>
    <link>https://dengziming.github.io/categories/hadoop%E6%BA%90%E7%A0%81/</link>
    <description>Recent content in Hadoop源码 on 数据分析师之旅</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Wed, 23 May 2018 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://dengziming.github.io/categories/hadoop%E6%BA%90%E7%A0%81/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>resourcemanager</title>
      <link>https://dengziming.github.io/post/hadoop/hadoopha/</link>
      <pubDate>Wed, 23 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/hadoop/hadoopha/</guid>
      
        <description>

&lt;p&gt;参考资料：&lt;/p&gt;

&lt;h1 id=&#34;hadoopha&#34;&gt;HadoopHa&lt;/h1&gt;

&lt;p&gt;hadoop 有两个NameNode，Active NameNode和Standby NameNode，通过 DFSZKFailoverController extends ZKFailoverController 进行切换。
ZKFailoverController通过HealthMonitor线程能及时检测到NameNode的健康状况，在主NameNode故障时借助Zookeeper实现自动的主备选举和切换。
DataNode 会同时向主NameNode和备NameNode上报数据块的位置信息，但只接收来自active namenode的读写命令。&lt;/p&gt;

&lt;p&gt;为啥把监控分开?&lt;/p&gt;

&lt;p&gt;显然，我们不能在NN进程内进行心跳等信息同步，最简单的原因，一次FullGC就可以让NN挂起十几分钟，所以，必须要有一个独立的短小精悍的watchdog来专门负责监控。这也是一个松耦合的设计，便于扩展或更改。&lt;/p&gt;

&lt;p&gt;通过隔离和Quorum Journal Manager(QJM)共享存储空间实现HDFS HA&lt;/p&gt;

&lt;h1 id=&#34;dfszkfailovercontroller&#34;&gt;DFSZKFailoverController&lt;/h1&gt;

&lt;p&gt;启动代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;  public static void main(String args[])
      throws Exception {
    if (DFSUtil.parseHelpArgument(args, 
        ZKFailoverController.USAGE, System.out, true)) {
      System.exit(0);
    }
    
    GenericOptionsParser parser = new GenericOptionsParser(
        new HdfsConfiguration(), args);
    DFSZKFailoverController zkfc = DFSZKFailoverController.create(
        parser.getConfiguration());
    {
        NNHAServiceTarget localTarget = new NNHAServiceTarget(
        localNNConf, nsId, nnId);
        return new DFSZKFailoverController(localNNConf, localTarget);
    }
    
    System.exit(zkfc.run(parser.getRemainingArgs()));
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;run 的步骤：
initZK();
formatZK(force, interactive);
initRPC();
initHM();
startRPC();
mainLoop();&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;initZK();
{
    elector = new ActiveStandbyElector(zkQuorum,
        zkTimeout, getParentZnode(), zkAcls, zkAuths,
        new ElectorCallbacks(), maxRetryNum);
    {
    	new ElectorCallbacks()
    	  // 临时节点ActiveStandbyElectorLock，用于标识锁
    	zkLockFilePath = znodeWorkingDir + &amp;quot;/&amp;quot; + LOCK_FILENAME;
    	// 永久节点ActiveBreadCrumb，用于存放active信息
    	zkBreadCrumbPath = znodeWorkingDir + &amp;quot;/&amp;quot; + BREADCRUMB_FILENAME;
    	this.maxRetryNum = maxRetryNum;
    	// createConnection for future API calls
    	// 创建zk连接
    	createConnection();
    	{
    	      // 不幸的是，zk的构造方法连接上zk之后，可能马上触发连接事件。
  			  // 因此如果构造zk之后注册watcher，可能不会捕获到连接事件。
  			  // 取而代之的方法是，先构造Watcher，在设置了zk的引用之前，使它阻塞所有的事件
  			  
  			  watcher = new WatcherWithClientRef();
  			  ZooKeeper zk = new ZooKeeper(zkHostPort, zkSessionTimeout, watcher);
  			  // 在watcher中设置zk的引用
  			  watcher.setZooKeeperRef(zk);
  			  // Wait for the asynchronous success/failure. This may throw an exception
  			  // if we don&#39;t connect within the session timeout.
  			  watcher.waitForZKConnectionEvent(zkSessionTimeout);
  			  
  			  for (ZKAuthInfo auth : zkAuthInfo) {
  			    zk.addAuthInfo(auth.getScheme(), auth.getAuth());
  			  }
  			  return zk;
    	      }
    	  }
    	}
}

formatZK(force, interactive);
initRPC();
{
    new ZKFCRpcServer(conf, bindAddr, this, getPolicyProvider());
    {
          this.zkfc = zkfc;
  			// 使用protocol buffer序列化
  			RPC.setProtocolEngine(conf, ZKFCProtocolPB.class,
  			    ProtobufRpcEngine.class);
  			ZKFCProtocolServerSideTranslatorPB translator =
  			    new ZKFCProtocolServerSideTranslatorPB(this);
  			BlockingService service = ZKFCProtocolService
  			    .newReflectiveBlockingService(translator);
  			// 使用hadoop rpc接口得到rpc server
  			// ZKFCProtocol是rpc协议，service是rpc协议的实现类
  			// ZKFCProtocolPB是protobuf rpc接口的一个过渡类
  			this.server = new RPC.Builder(conf).setProtocol(ZKFCProtocolPB.class)
  			    .setInstance(service).setBindAddress(bindAddr.getHostName())
  			    .setPort(bindAddr.getPort()).setNumHandlers(HANDLER_COUNT)
  			    .setVerbose(false).build();
    }
}
initHM();
startRPC();
mainLoop();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;WatcherWithClientRef 在构造zk时被注册为默认watcher，主要监听连接或者断开事件。当调用initZk之后，watcher.process会对事件进行处理，连接、断开、过期的状态类型都是EventType.None。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>yarn-api使用</title>
      <link>https://dengziming.github.io/post/hadoop/yarn-api%E4%BD%BF%E7%94%A8/</link>
      <pubDate>Wed, 23 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/hadoop/yarn-api%E4%BD%BF%E7%94%A8/</guid>
      
        <description>

&lt;p&gt;参考： &lt;a href=&#34;https://ieevee.com/tech/2015/05/05/yarn-dist-shell.html&#34;&gt;https://ieevee.com/tech/2015/05/05/yarn-dist-shell.html&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;distributeshell&#34;&gt;distributeshell&lt;/h1&gt;

&lt;h2 id=&#34;client解析&#34;&gt;Client解析&lt;/h2&gt;

&lt;p&gt;distShell主要有2个类组成，Client和ApplicationMaster。两个类都带有main入口。Client的主要工作是启动AM，真正要做的任务由AM来调度。 Client的简化框架如下。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public static void main(String[] args) {
    boolean result = false;
    try {
      Client client = new Client();  //1 创建Client对象
      try {
        boolean doRun = client.init(args);  //2 初始化
        if (!doRun) {
          System.exit(0);
        }
      }
      result = client.run();   //3 运行
    }
    if (result) {
      System.exit(0);
    }
    System.exit(2);
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;1-创建client对象&#34;&gt;1 创建Client对象&lt;/h3&gt;

&lt;p&gt;创建时会指定本Client要用到的AM。 创建yarnClient。yarn将client与RM的交互抽象出了编程库YarnClient，用以应用程序提交、状态查询和控制等，简化应用程序。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;  public Client(Configuration conf) throws Exception  {
    this(		//指定AM
      &amp;quot;org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster&amp;quot;,
      conf);
  Client(String appMasterMainClass, Configuration conf) {
    this.conf = conf;
    this.appMasterMainClass = appMasterMainClass;
    yarnClient = YarnClient.createYarnClient();		//创建yarnClient
    yarnClient.init(conf);
    opts = new Options();	//创建opts，后面解析参数的时候用
    opts.addOption(&amp;quot;appname&amp;quot;, true, &amp;quot;Application Name. Default value - DistributedShell&amp;quot;);
    opts.addOption(&amp;quot;priority&amp;quot;, true, &amp;quot;Application Priority. Default 0&amp;quot;);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;2-初始化&#34;&gt;2 初始化&lt;/h3&gt;

&lt;p&gt;init会解析命令行传入的参数，例如使用的jar包、内存大小、cpu个数等。 代码里使用GnuParser解析：init时定义所有的参数opts（可以认为是一个模板），
然后将opts和实际的args传入解析后得到一个CommnadLine对象，后面查询选项直接操作该CommnadLine对象即可，如cliParser.hasOption(&amp;ldquo;help&amp;rdquo;)和cliParser.getOptionValue(&amp;ldquo;jar&amp;rdquo;)。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt; public boolean init(String[] args) throws ParseException {
    CommandLine cliParser = new GnuParser().parse(opts, args);
    amMemory = Integer.parseInt(cliParser.getOptionValue(&amp;quot;master_memory&amp;quot;, &amp;quot;10&amp;quot;));
    amVCores = Integer.parseInt(cliParser.getOptionValue(&amp;quot;master_vcores&amp;quot;, &amp;quot;1&amp;quot;));
    shellCommand = cliParser.getOptionValue(&amp;quot;shell_command&amp;quot;);
    appMasterJar = cliParser.getOptionValue(&amp;quot;jar&amp;quot;);
    ...
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;3-运行&#34;&gt;3 运行&lt;/h3&gt;

&lt;p&gt;先启动yarnClient，会建立跟RM的RPC连接，之后就跟调用本地方法一样。通过此yarnClient查询NM个数、NM详细信息（ID/地址/Container个数等）、Queue info（其实没用到，示例里只是打印了下调试用）。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;
public class Client {
  public boolean run() throws IOException, YarnException {
    yarnClient.start();
    YarnClusterMetrics clusterMetrics = yarnClient.getYarnClusterMetrics();
    List&amp;lt;NodeReport&amp;gt; clusterNodeReports = yarnClient.getNodeReports(
收集提交AM所需的信息。
    YarnClientApplication app = yarnClient.createApplication();	//创建app
    GetNewApplicationResponse appResponse = app.getNewApplicationResponse();
...
    ApplicationSubmissionContext appContext = app.getApplicationSubmissionContext();
    //AM需要的本地资源，如jar包、log文件
    Map&amp;lt;String, LocalResource&amp;gt; localResources = new HashMap&amp;lt;String, LocalResource&amp;gt;();

    FileSystem fs = FileSystem.get(conf);
    addToLocalResources(fs, appMasterJar, appMasterJarPath, appId.toString(),
        localResources, null);
    ...	//添加localResource

    vargs.add(Environment.JAVA_HOME.$$() + &amp;quot;/bin/java&amp;quot;);
    vargs.add(&amp;quot;-Xmx&amp;quot; + amMemory + &amp;quot;m&amp;quot;);
    vargs.add(appMasterMainClass);
...
    for (CharSequence str : vargs) {
      command.append(str).append(&amp;quot; &amp;quot;);	//重新组织命令行
    }
	//创建Container加载上下文，包含本地资源，环境变量，实际命令。
    ContainerLaunchContext amContainer = ContainerLaunchContext.newInstance(
      localResources, env, commands, null, null, null);

    Resource capability = Resource.newInstance(amMemory, amVCores);
    appContext.setResource(capability);		//请求使用的内存、cpu

    appContext.setAMContainerSpec(amContainer);
    appContext.setQueue(amQueue);

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;重新组织出来的commands如下：&lt;/p&gt;

&lt;p&gt;$JAVA_HOME/bin/java -Xmx10m org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster &amp;ndash;container_memory 10
提交AM（即appContext），并启动监控。 Client只关心自己提交到RM的AM是否正常运行，而AM内部的多个task，由AM管理。如果Client要查询应用程序的任务信息，需要自己设计与AM的交互。
    yarnClient.submitApplication(appContext);   //客户端提交AM到RM
    return monitorApplication(appId);
总的来说，Client做的事情比较简单，即建立与RM的连接，提交AM，监控AM运行状态。&lt;/p&gt;

&lt;p&gt;有个疑问，走读代码没有看到jar包是怎么送到NM上去的。&lt;/p&gt;

&lt;h2 id=&#34;application-master解析&#34;&gt;Application Master解析&lt;/h2&gt;

&lt;p&gt;AM简化框架如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;

      boolean doRun = appMaster.init(args);
      if (!doRun) {
        System.exit(0);
      }
      appMaster.run();
      result = appMaster.finish();
// yarn抽象了两个编程库，AMRMClient和NMClient(AM和RM都可以用)，简化AM编程。

// 1 设置RM、NM消息的异步处理方法
    AMRMClientAsync.CallbackHandler allocListener = new RMCallbackHandler();
    amRMClient = AMRMClientAsync.createAMRMClientAsync(1000, allocListener);
    amRMClient.init(conf);
    amRMClient.start();

    containerListener = createNMCallbackHandler();
    nmClientAsync = new NMClientAsyncImpl(containerListener);
    nmClientAsync.init(conf);
    nmClientAsync.start();
// 2 向RM注册
    RegisterApplicationMasterResponse response = amRMClient.registerApplicationMaster(appMasterHostname,
        appMasterRpcPort, appMasterTrackingUrl);
// 3 计算需要的Container，向RM发起请求
    // Setup ask for containers from RM
    // Send request for containers to RM
    // Until we get our fully allocated quota, we keep on polling RM for
    // containers
    // Keep looping until all the containers are launched and shell script
    // executed on them ( regardless of success/failure).
    for (int i = 0; i &amp;lt; numTotalContainersToRequest; ++i) {
      ContainerRequest containerAsk = setupContainerAskForRM();
      amRMClient.addContainerRequest(containerAsk);		//请求指定个数的Container
    }

  private ContainerRequest setupContainerAskForRM() {
    Resource capability = Resource.newInstance(containerMemory,
      containerVirtualCores);		//指定需要的memory/cpu能力
    ContainerRequest request = new ContainerRequest(capability, null, null,
        pri);


4 // RM分配Container给AM，AM启动任务RMCallbackHandler RM消息的响应，由RMCallbackHandler处理。示例中主要对前两种消息进行了处理。

  private class RMCallbackHandler implements AMRMClientAsync.CallbackHandler {
    //处理消息：Container执行完毕。在RM返回的心跳应答中携带。如果心跳应答中有已完成和新分配两种Container，先处理已完成
    public void onContainersCompleted(List&amp;lt;ContainerStatus&amp;gt; completedContainers) {
...
    //处理消息：RM新分配Container。在RM返回的心跳应答中携带
    public void onContainersAllocated(List&amp;lt;Container&amp;gt; allocatedContainers) {

    public void onShutdownRequest() {done = true;}

    //节点状态变化
    public void onNodesUpdated(List&amp;lt;NodeReport&amp;gt; updatedNodes) {}

    public float getProgress() {
onContainersAllocated收到分配的Container之后，会提交任务到NM。

    public void onContainersAllocated(List&amp;lt;Container&amp;gt; allocatedContainers) {
        LaunchContainerRunnable runnableLaunchContainer =   //创建runnable容器
            new LaunchContainerRunnable(allocatedContainer, containerListener);
        Thread launchThread = new Thread(runnableLaunchContainer);	//新建线程

        // launch and start the container on a separate thread to keep
        // the main thread unblocked
        // as all containers may not be allocated at one go.
        launchThreads.add(launchThread);
        launchThread.start();	//线程中提交Container到NM，不影响主流程

//简单分析下LaunchContainerRunnable。该类实现自Runnable，其run方法准备任务命令（本例即为date）。

  private class LaunchContainerRunnable implements Runnable {
    public LaunchContainerRunnable(
        Container lcontainer, NMCallbackHandler containerListener) {
      this.container = lcontainer;		//创建时记录待使用的Container
      this.containerListener = containerListener;
    }
    public void run() {
      vargs.add(shellCommand);		//待执行的shell命令
      vargs.add(shellArgs);			//shell命令参数
      List&amp;lt;String&amp;gt; commands = new ArrayList&amp;lt;String&amp;gt;();
      commands.add(command.toString());	//转为commands

      //根据命令、环境变量、本地资源等创建Container加载上下文
      ContainerLaunchContext ctx = ContainerLaunchContext.newInstance(
              localResources, shellEnv, commands, null, allTokens.duplicate(), null);
      containerListener.addContainer(container.getId(), container);
      //异步启动Container
      nmClientAsync.startContainerAsync(container, ctx);
// onContainersCompleted的功能比较简单，收到Container执行完毕的消息，检查其执行结果，如果执行失败，则重新发起请求，直到全部完成。

// NMCallbackHandler NM消息的响应，由NMCallbackHandler处理。

//在distShell示例里，回调句柄对NM通知过来的各种事件的处理比较简单，只是修改AM维护的Container执行完成、失败的个数。这样等到有Container执行完毕后，可以重启发起请求。失败处理和上面Container执行完毕消息的处理类似，达到了上面问题里所说的loopback效果。

  static class NMCallbackHandler
    implements NMClientAsync.CallbackHandler {

    @Override
    public void onContainerStopped(ContainerId containerId) {

    @Override
    public void onContainerStatusReceived(ContainerId containerId,

    @Override
    public void onContainerStarted(ContainerId containerId,
...
总的来说，AM做的事就是向RM/NM注册回调函数，然后请求Container；得到Container后提交任务，并跟踪这些任务的执行情况，如果失败了则重新提交，直到全部任务完成。
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;unmanagedam&#34;&gt;UnmanagedAM&lt;/h1&gt;

&lt;p&gt;distShell的Client提交AM到RM后，由RM将AM分配到某一个NM上的Container，这样给AM调试带来了困难。yarn提供了一个参数，Client可以设置为Unmanaged，提交AM后，会在客户端本地起一个单独的进程来运行AM。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;
public class UnmanagedAMLauncher {
  public void launchAM(ApplicationAttemptId attemptId)
    //创建新进程
    Process amProc = Runtime.getRuntime().exec(amCmd, envAMList.toArray(envAM));
    try {
      int exitCode = amProc.waitFor();  //等待AM进程结束
    } finally {
      amCompleted = true;
    }

  public boolean run() throws IOException, YarnException {
      appContext.setUnmanagedAM(true);		//设置为Unmanaged
      rmClient.submitApplication(appContext);	//提交AM

      ApplicationReport appReport =		//监控AM状态，如果状态变为ACCEPTED，则跳出循环，launchAM。
          monitorApplication(appId, EnumSet.of(YarnApplicationState.ACCEPTED,
            YarnApplicationState.KILLED, YarnApplicationState.FAILED,
            YarnApplicationState.FINISHED));

      if (appReport.getYarnApplicationState() == YarnApplicationState.ACCEPTED) {
        launchAM(attemptId);
&lt;/code&gt;&lt;/pre&gt;
</description>
      
    </item>
    
    <item>
      <title>hdfs-client</title>
      <link>https://dengziming.github.io/post/hadoop/hdfs-client/</link>
      <pubDate>Tue, 22 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/hadoop/hdfs-client/</guid>
      
        <description>

&lt;p&gt;参考资料：&lt;/p&gt;

&lt;p&gt;hadoop 技术内幕丛书&lt;/p&gt;

&lt;h1 id=&#34;写&#34;&gt;写&lt;/h1&gt;

&lt;h1 id=&#34;创建流&#34;&gt;创建流&lt;/h1&gt;

&lt;p&gt;简单写一个 demo 进行测试，通过打断点方法，另外发现一个问题， 在 dfsClient.namenode.create 打断点调试会报错，可能是因为动态代理卡主了 ：&lt;/p&gt;

&lt;p&gt;注意我们上传的文件 nio-data.txt 内容可以进行控制，例如我们写 600 个 a(97)，转换为 DataOutputStream 后的 byte[] 就是 600个97 ，这样调试就知道是哪个数据，600 a 是因为每个chunk的默认大小是 512&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public static void main(String[] args) throws IOException {

    FileSystem fs = FileSystem.get(new Configuration());
    
    Path src = new Path(&amp;quot;ideaspace/learn-jvm/src/main/resources/data/nio-data.txt&amp;quot;); //文件里面是一个 java 代码
    Path desc = new Path(&amp;quot;/tmp/&amp;quot;);
    if (fs.exists(desc)){
            fs.delete(desc,true);
        }

    fs.copyFromLocalFile(src,desc);

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;经过一系列的调用后进入的第一个关键方法是 ：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;@Override
  public FSDataOutputStream create(final Path f, final FsPermission permission,
    final EnumSet&amp;lt;CreateFlag&amp;gt; cflags, final int bufferSize,
    final short replication, final long blockSize, final Progressable progress,
    final ChecksumOpt checksumOpt) throws IOException {
    statistics.incrementWriteOps(1);
    Path absF = fixRelativePart(f);
    return new FileSystemLinkResolver&amp;lt;FSDataOutputStream&amp;gt;() {
      @Override
      public FSDataOutputStream doCall(final Path p)
          throws IOException, UnresolvedLinkException {
        final DFSOutputStream dfsos = dfs.create(getPathName(p), permission,
                cflags, replication, blockSize, progress, bufferSize,
                checksumOpt);
        return dfs.createWrappedOutputStream(dfsos, statistics);
      }
      @Override
      public FSDataOutputStream next(final FileSystem fs, final Path p)
          throws IOException {
        return fs.create(p, permission, cflags, bufferSize,
            replication, blockSize, progress, checksumOpt);
      }
    }.resolve(this, absF);
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;1-create&#34;&gt;1.create&lt;/h1&gt;

&lt;p&gt;首先是 create，然后是 dfs.createWrappedOutputStream(out, statistics);&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public DFSOutputStream create(String src, 
                           FsPermission permission,
                           EnumSet&amp;lt;CreateFlag&amp;gt; flag, 
                           boolean createParent,
                           short replication,
                           long blockSize,
                           Progressable progress,
                           int buffersize,
                           ChecksumOpt checksumOpt,
                           InetSocketAddress[] favoredNodes) throws IOException {
  checkOpen();
  if (permission == null) {
    permission = FsPermission.getFileDefault();
  }
  FsPermission masked = permission.applyUMask(dfsClientConf.uMask);
  if(LOG.isDebugEnabled()) {
    LOG.debug(src + &amp;quot;: masked=&amp;quot; + masked);
  }
  String[] favoredNodeStrs = null;
  if (favoredNodes != null) {
    favoredNodeStrs = new String[favoredNodes.length];
    for (int i = 0; i &amp;lt; favoredNodes.length; i++) {
      favoredNodeStrs[i] = 
          favoredNodes[i].getHostName() + &amp;quot;:&amp;quot; 
                       + favoredNodes[i].getPort();
    }
  }
  final DFSOutputStream result = DFSOutputStream.newStreamForCreate(this,
      src, masked, flag, createParent, replication, blockSize, progress,
      buffersize, dfsClientConf.createChecksum(checksumOpt),
      favoredNodeStrs);
  beginFileLease(result.getFileId(), result);
  return result;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;两个方法比较关键：&lt;/p&gt;

&lt;p&gt;DFSOutputStream.newStreamForCreate 和 beginFileLease(result.getFileId(), result)&lt;/p&gt;

&lt;h2 id=&#34;1-newstreamforcreate-方法是第一次创建真正的-流-类是-dfsoutputstream&#34;&gt;1. newStreamForCreate 方法是第一次创建真正的 流，类是 DFSOutputStream&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;static DFSOutputStream newStreamForCreate(DFSClient dfsClient, String src,
      FsPermission masked, EnumSet&amp;lt;CreateFlag&amp;gt; flag, boolean createParent,
      short replication, long blockSize, Progressable progress, int buffersize,
      DataChecksum checksum, String[] favoredNodes) throws IOException {
    ...
    while (shouldRetry) {
      shouldRetry = false;
      try {
        stat = dfsClient.namenode.create(src, masked, dfsClient.clientName,
            new EnumSetWritable&amp;lt;CreateFlag&amp;gt;(flag), createParent, replication,
            blockSize, SUPPORTED_CRYPTO_VERSIONS);
        break;
      } catch (RemoteException re) {...}
    Preconditions.checkNotNull(stat, &amp;quot;HdfsFileStatus should not be null!&amp;quot;);
    final DFSOutputStream out = new DFSOutputStream(dfsClient, src, stat,
        flag, progress, checksum, favoredNodes);
    out.start();
    return out;
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;rpc
这部分没法调试，因为在远程。只能自己观看
通过 RPC 调用 NameNodeRpcServer.create -&amp;gt; namesystem.startFile -&amp;gt; startFileInt -&amp;gt; startFileInternal ，先跳过，&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;new DFSOutputStream(dfsClient, src, stat,flag, progress, checksum, favoredNodes);&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;/** Construct a new output stream for creating a file. */
  private DFSOutputStream(DFSClient dfsClient, String src, HdfsFileStatus stat,
      EnumSet&amp;lt;CreateFlag&amp;gt; flag, Progressable progress,
      DataChecksum checksum, String[] favoredNodes) throws IOException {
    this(dfsClient, src, progress, stat, checksum);
    this.shouldSyncBlock = flag.contains(CreateFlag.SYNC_BLOCK);

    computePacketChunkSize(dfsClient.getConf().writePacketSize, bytesPerChecksum);

    Span traceSpan = null;
    if (Trace.isTracing()) {
      traceSpan = Trace.startSpan(this.getClass().getSimpleName()).detach();
    }
    streamer = new DataStreamer(stat, traceSpan);
    if (favoredNodes != null &amp;amp;&amp;amp; favoredNodes.length != 0) {
      streamer.setFavoredNodes(favoredNodes);
    }
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;新建 DFSOutputStream 中有个重要的线程 DataStreamer，功能后续研究。
DFSOutputStream 中的成员变量我们可以好好看看，什么是 checksum，chunk，packet。另外它的父类 FSOutputSummer 也很重要。&lt;/p&gt;

&lt;h2 id=&#34;2-beginfilelease-这个可以暂时忽略-后面专门研究-lease&#34;&gt;2. beginFileLease 这个可以暂时忽略，后面专门研究 lease&lt;/h2&gt;

&lt;p&gt;dfs.createWrappedOutputStream(dfsos, statistics) 对上面创建的流就行一个 包装&lt;/p&gt;

&lt;p&gt;返回 return new HdfsDataOutputStream(dfsos, statistics, startPos);&lt;/p&gt;

&lt;p&gt;至此创建流的过程就完成了。我们大概回顾一下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;DistributedFileSystem.create(final Path f, final FsPermission permission,
final EnumSet&amp;lt;CreateFlag&amp;gt; cflags, final int bufferSize,
    final short replication, final long blockSize, final Progressable progress,
    final ChecksumOpt checksumOpt)
{
    // 1. 
    DFSOutputStream dfsos = DfsClient.create(getPathName(p), permission,
                cflags, replication, blockSize, progress, bufferSize,
                checksumOpt)
    {
        // 1. 
        DFSOutputStream.newStreamForCreate(this,
        	src, masked, flag, createParent, replication, blockSize, progress,
        	buffersize, dfsClientConf.createChecksum(checksumOpt),
        	favoredNodeStrs);
        {
            // 1.
            stat = dfsClient.namenode.create(src, masked, dfsClient.clientName,
                new EnumSetWritable&amp;lt;CreateFlag&amp;gt;(flag), createParent, replication,
                blockSize, SUPPORTED_CRYPTO_VERSIONS);
            {
                // RPC
            }
            // 2.
            final DFSOutputStream out = new DFSOutputStream(dfsClient, src, stat,
            flag, progress, checksum, favoredNodes);
            {
                // 
                class DataStreamer
                class Patket
                streamer = new DataStreamer(stat, traceSpan);
            }
            
        }
        // 2.
        beginFileLease(result.getFileId(), result);
    }
                
    // 2. 
    return dfs.createWrappedOutputStream(dfsos, statistics);
    {
        return new HdfsDataOutputStream(dfsos, statistics, startPos);
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;首先是 DistributedFileSystem 创建，然后调用 DfsClient 的 create ，DfsClient需要创建流和lease，创建流 由 DFSOutputStream 完成，
DFSOutputStream 需要分别和namenode、datanode通信。DFSOutputStream 内部有 Packet 和 DataStreamer，继承自 FSOutputSummer ，FSOutputSummer 完成了write的真正逻辑&lt;/p&gt;

&lt;h1 id=&#34;2-out-write-buf-0-bytesread&#34;&gt;2. out.write(buf, 0, bytesRead)&lt;/h1&gt;

&lt;p&gt;创建完成后就是写数据，HdfsDataOutputStream 写数据比较复杂，先写到缓存，然后发送。需要做 checksum 检验，然后做成一个 chuck，然后将多个 chuck 合成一个 Packet，然后发送 Packet。&lt;/p&gt;

&lt;p&gt;FSDataOutputStream.out.write(byte[])
调用过程：
out.write(bytes) -&amp;gt; FilterOutputStream.write -&amp;gt; DataOutputStream.write -&amp;gt; out.write(byte[], off, len) -&amp;gt; FSOutputSummer.write(byte b[], int off, int len)&lt;/p&gt;

&lt;p&gt;FSOutputSummer.write&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public synchronized void write(byte b[], int off, int len)
      throws IOException {
    
    checkClosed();
    
    if (off &amp;lt; 0 || len &amp;lt; 0 || off &amp;gt; b.length - len) {
      throw new ArrayIndexOutOfBoundsException();
    }
    // 循环调用 write ，每次写入 #write1() 长度
    for (int n=0;n&amp;lt;len;n+=write1(b, off+n, len-n)) {
    }
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个 byte b[] 就是我们的数据流，通过断点我们可以看到是600个97，也就是600个a。&lt;/p&gt;

&lt;p&gt;write1，这里有几个比较核心的内容，如果写入长度比较大，直接写入流，如果写入比较少，先写到 Buffer，到达一定长度再统一进行写到流。这么做是为了减少拷贝&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;private int write1(byte b[], int off, int len) throws IOException {
  
  // 写入长度大于本地buf的长度时，直接写入本地buf的长度。
  if(count==0 &amp;amp;&amp;amp; len&amp;gt;=buf.length) {
    // local buffer is empty and user buffer size &amp;gt;= local buffer size, so
    // simply checksum the user buffer and send it directly to the underlying
    // stream
    final int length = buf.length;
    writeChecksumChunks(b, off, length);
    return length;
  }
  // 当len小于本地buf的长度时，先写入buf，当buf写满之后，flushBuffer
  // copy user data to local buffer
  
  int bytesToCopy = buf.length-count; // 这个 count 代表以及复制的数据长度，第一次是 0
  bytesToCopy = (len&amp;lt;bytesToCopy) ? len : bytesToCopy;  // 这时候就是要复制的数据长度，600
  System.arraycopy(b, off, buf, count, bytesToCopy);
  count += bytesToCopy;
  if (count == buf.length) {
    // local buffer is full
    flushBuffer();
  } 
  return bytesToCopy;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，写入数据大的话，直接调用 writeChecksumChunks 将buf长度大小的数据生成 chunksum ，
（chunksum 是检查数据完整性的，相关知识可以查看计算机网络。）并写入 packet中。如果写入数据比较少，直接放进 buffer，等待buffer比较大，再统一flush&lt;/p&gt;

&lt;p&gt;数据写完了关闭流的时候会再调用一次 fulshBuffer ,会调用 writeChecksumChunks&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;private void writeChecksumChunks(byte b[], int off, int len)
throws IOException {
  // 计算checksum
  sum.calculateChunkedSums(b, off, len, checksum, 0);
  for (int i = 0; i &amp;lt; len; i += sum.getBytesPerChecksum()) {
    int chunkLen = Math.min(sum.getBytesPerChecksum(), len - i);
    int ckOffset = i / sum.getBytesPerChecksum() * getChecksumSize();
    // 一个chunk一个chunk的写入packet
    writeChunk(b, off + i, chunkLen, checksum, ckOffset, getChecksumSize());
  }

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;sum.calculateChunkedSums 计算校验值，计算完以后b还是 600个97，off和len分别是 0和600，checksum是一个36位的数组，
但是只有前八位有值：0 = 111,1 = 50,2 = -90,3 = 31,4 = -99,5 = 97,6 = -69,7 = 102。因为每512位生成4个校验码。现在是600位，需要8个。
然后写出 chunk，chunk的长度为： 512，所以这里会调用两次 writeChunk。&lt;/p&gt;

&lt;p&gt;writeChunk 先将 chunk 写入 currentPacket 中，当currentPacket写满之后调用 waitAndQueueCurrentPacket，
将packet放入dataQueue队列，等待DataStreamer线程将packet写入pipeline中，整个block发送完毕之后将发送一个空的packet。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;writeChunk{}
currentPacket.writeChecksum(checksum, ckoff, cklen);
currentPacket.writeData(b, offset, len);
currentPacket.numChunks++;
bytesCurBlock += len;
waitAndQueueCurrentPacket()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们把这段代码跑两遍以后，去看看生成的Packet，里面有4个属性：checksumStart = 33,checksumPos = 41,dataStart = 541,dataPos = 1141&lt;/p&gt;

&lt;p&gt;可以看出这几个的意思分别是 check 的开始位置和结束为止，data的开始位置和结束为止，之所以留了 33个位置，是 Packet 的 header。然后还有一个 buffer数组，里面就是按照检查数据和数据。&lt;/p&gt;

&lt;p&gt;这时候 Packet 的结构我们已经一清二楚了。&lt;/p&gt;

&lt;p&gt;相关详细内容可以继续深入查看源码。waitAndQueueCurrentPacket() 可以看具体代码，这时候已经报连接超时异常了。接下来我们调试发送数据到 DataNode。&lt;/p&gt;

&lt;h1 id=&#34;三-dfsoutputstream-datastreamer-发送-packet&#34;&gt;三.DFSOutputStream.DataStreamer 发送 packet&lt;/h1&gt;

&lt;p&gt;上面已经调试到：waitAndQueueCurrentPacket() ,会将 Packet 发送给 DFSOutputStream.DataStreamer&lt;/p&gt;

&lt;p&gt;DFSOutputStream.DataStreamer 是一个线程，里面有个 stage 标识到了哪一步。还有一个 response 用来回复消息。&lt;/p&gt;

&lt;p&gt;DataStreamer 会从 dataQueue 中拿出 packet 发送到 pipeline , 相关代码很长。取出部分：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;synchronized (dataQueue) {

	 // 发送packet，dataQueue为null，则发送一个心跳
	 if (dataQueue.isEmpty()) {
	   one = createHeartbeatPacket();
	 } else {
	   one = dataQueue.getFirst(); // regular data packet
	 }
	}
	
        // 建立pipeline
        setPipeline(nextBlockOutputStream());
        // 启动ResponseProcessor线程，更新DataStreamer的状态为DATA_STREAMING

// 当前packet是block的最后一个packet，等待接收之前所有packet的ack
      if (one.lastPacketInBlock) {
        // wait for all data packets have been successfully acked
        synchronized (dataQueue) {
          while (!streamerClosed &amp;amp;&amp;amp; !hasError &amp;amp;&amp;amp; 
              ackQueue.size() != 0 &amp;amp;&amp;amp; dfsClient.clientRunning) {
            try {
              // wait for acks to arrive from datanodes
              dataQueue.wait(1000);
            } catch (InterruptedException  e) {
              DFSClient.LOG.warn(&amp;quot;Caught exception &amp;quot;, e);
            }
          }
        }
        if (streamerClosed || hasError || !dfsClient.clientRunning) {
          continue;
        }
        stage = BlockConstructionStage.PIPELINE_CLOSE;
      }
       
     
     // 将 packet 从 dataQueue 移到 ackQueue，准备发送packet
      synchronized (dataQueue) {
        // move packet from dataQueue to ackQueue
        if (!one.isHeartbeatPacket()) {
          dataQueue.removeFirst();
          ackQueue.addLast(one);
          dataQueue.notifyAll();
        }
      }
     // 将packet写入pipeline
        one.writeTo(blockStream);
        blockStream.flush();   
    
      // 如果当前packet是最后一个，则继续等待此packet的ack，
      // 然后endBlock
    if (one.lastPacketInBlock) {
        // wait for the close packet has been acked
        synchronized (dataQueue) {
          while (!streamerClosed &amp;amp;&amp;amp; !hasError &amp;amp;&amp;amp; 
              ackQueue.size() != 0 &amp;amp;&amp;amp; dfsClient.clientRunning) {
            dataQueue.wait(1000);// wait for acks to arrive from datanodes
          }
        }
        if (streamerClosed || hasError || !dfsClient.clientRunning) {
          continue;
        }
        endBlock();
      }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;代码很长也比较杂乱，我们主要看一下 setPipeline(nextBlockOutputStream()); 和 one.writeTo(blockStream);blockStream.flush();  等&lt;/p&gt;

&lt;p&gt;第一步是：one = dataQueue.getFirst();&lt;br /&gt;
我们看看取到的 Packet 是什么样子：和我们上面发送的一样，buf=[0,0,0(33个0),11,50..(6个检测码),0,0,(很多0)，97,97,97(600个97)，],lastPacketInBlock=false &amp;hellip;&lt;/p&gt;

&lt;p&gt;nextBlockOutputStream 是通过 namenode 得到一个 LocatedBlock ，而 setPipeline 是和 Datanode 的通道。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;nextBlockOutputStream(){
lb = locateFollowingBlock(startTime,excluded.length &amp;gt; 0 ? excluded : null); 
-- dfsClient.namenode.addBlock(src, dfsClient.clientName,block, excludedNodes, fileId, favoredNodes);

success = createBlockOutputStream(nodes, storageTypes, 0L, false);

blockStream = out; //给写出流赋值。
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;createBlockOutputStream 对于我们第一次调试来说，关注三个就够了。
创建连接： /到 datanode 的 socket连接 Socket[addr=/127.0.0.1,port=50010,localport=58006]，
new Sender(out).writeBlock 建立 block&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;createBlockOutputStream{}
// 到 datanode 的 socket连接 Socket[addr=/127.0.0.1,port=50010,localport=58006]
s = createSocketForPipeline(nodes[0], nodes.length, dfsClient);
long writeTimeout = dfsClient.getDatanodeWriteTimeout(nodes.length);

OutputStream unbufOut = NetUtils.getOutputStream(s, writeTimeout);
InputStream unbufIn = NetUtils.getInputStream(s);
IOStreamPair saslStreams = dfsClient.saslClient.socketSend(s,
  unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);
unbufOut = saslStreams.out;
unbufIn = saslStreams.in;
out = new DataOutputStream(new BufferedOutputStream(unbufOut,
    HdfsConstants.SMALL_BUFFER_SIZE));
    
-- 发送写block请求，
new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,
dfsClient.clientName, nodes, nodeStorageTypes, null, bcs, 
nodes.length, block.getNumBytes(), bytesSent, newGS,
checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile);

blockStream = out; //给写出流赋值。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;回到 run 方法，通过 initDataStreaming ，然后调用 one.writeTo(blockStream); 这个 blockStream 上面已经赋值了，就是通过 socket 建立 的连接。这里只是将packet写入pipeline中的第一个dn。&lt;/p&gt;

&lt;p&gt;看看 Packet 的 writeTo 方法，Packet 的成员变量上面写了，是检查点位置，检查长度，数据位置，数据长度，等。&lt;/p&gt;

&lt;p&gt;主要步骤：
- 计算 pktLen 头长度+检查长度+数据长度
- 新建Header，包括 packet 是否是最后一个packet的信息等。
- 判断 checksumPos != dataStart 不等于需要删掉中间的空缺数据，&lt;code&gt;System.arraycopy(buf, checksumStart, buf, dataStart - checksumLen , checksumLen)&lt;/code&gt;
这个是因为 packet 容量是好几万，我们假设 10000，能容纳的 chunk 是 20个左右，所以应该有 20 * 4 = 80 的检查数据。所以前80个位置留给了检查位置，数据从81开始写。
但是如果我们没写满一个 Packet，检查数据就不需要80个，数据也是从 81开始写，这就空缺了一些数据。
- 将 header.getBytes() [0 = 0,1 = 0,2 = 2,3 = 100,4 = 0,5 = 25 &amp;hellip;.(一共33个值)] 的值放到 Packet 的头部。
- stm.write(buf, headerStart, header.getSerializedSize() + checksumLen + dataLen);&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;void writeTo(DataOutputStream stm) throws IOException {
  
  final int dataLen = dataPos - dataStart;  1411 - 541
  final int checksumLen = checksumPos - checksumStart; 
  final int pktLen = HdfsConstants.BYTES_IN_INTEGER + dataLen + checksumLen;
  PacketHeader header = new PacketHeader(
    pktLen, offsetInBlock, seqno, lastPacketInBlock, dataLen, syncBlock);
    
  // checksumPos不等于dataStart时，将checksum移动到data前面，
  // 紧挨着data，为header空出足够的空间
  if (checksumPos != dataStart) {
    // Move the checksum to cover the gap. This can happen for the last
    // packet or during an hflush/hsync call.
    System.arraycopy(buf, checksumStart, buf, 
                     dataStart - checksumLen , checksumLen); 
    checksumPos = dataStart;
    checksumStart = checksumPos - checksumLen;
  }
  
  final int headerStart = checksumStart - header.getSerializedSize();
  assert checksumStart + 1 &amp;gt;= header.getSerializedSize();
  assert checksumPos == dataStart;
  assert headerStart &amp;gt;= 0;
  assert headerStart + header.getSerializedSize() == checksumStart;
  
  // Copy the header data into the buffer immediately preceding the checksum
  // data.
  
  // 将header复制到packet的buf中，组成一个完整的packet
  System.arraycopy(header.getBytes(), 0, buf, headerStart,
      header.getSerializedSize());
  
  // corrupt the data for testing.
  if (DFSClientFaultInjector.get().corruptPacket()) {
    buf[headerStart+header.getSerializedSize() + checksumLen + dataLen-1] ^= 0xff;
  }
  // Write the now contiguous full packet to the output stream.
  // 将buf写入输出流中
  stm.write(buf, headerStart, header.getSerializedSize() + checksumLen + dataLen);
  
  // undo corruption.
  if (DFSClientFaultInjector.get().uncorruptPacket()) {
    buf[headerStart+header.getSerializedSize() + checksumLen + dataLen-1] ^= 0xff;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以看一下 最后的 stm.write， 通过跟踪我们发现最终调用： channel.write(buf);  是通过 NIO实现的。&lt;/p&gt;

&lt;p&gt;最后我们可以看到打印了错误信息： Exception in thread &amp;ldquo;main&amp;rdquo; java.io.IOException: All datanodes 127.0.0.1:50010 are bad. Aborting&amp;hellip;
这是因为我们调试时间长，连接已经断了。这个错误是哪一步打印的我们后续可以研究研究。&lt;/p&gt;

&lt;p&gt;其实这里有个疑问，这个 Packet 是最后一个 Packet，但是 它的 lastPacketInBlock=false？，其实这个确实不是最后一个，后面还要发送一个空的 Packet，只有 37个字节的头信息。&lt;/p&gt;

&lt;h1 id=&#34;四-dataxceiver-线程写入-datanode&#34;&gt;四、DataXceiver 线程写入 DataNode&lt;/h1&gt;

&lt;p&gt;注意：&lt;/p&gt;

&lt;p&gt;服务端的调试和客户端调试是反过来的，
需要先 stop-dfs.sh, 然后 &lt;code&gt;hadoop-daemons.sh start namenode&lt;/code&gt; &lt;code&gt;hadoop-daemons.sh start secondarynamenode&lt;/code&gt;  启动 namenode，然后我们在 IDEA 中运行 datanode
然后我们通过客户端命令: hadoop fs -copyFromLocal data.txt /tmp/data.txt
另外多线程调试不太方便，每次只能在一个线程打赏断点，否则会跳来跳去很麻烦，我们就在 DataXCerver 的 writeBlock 方法打上断点&lt;/p&gt;

&lt;p&gt;以上的流程可以看做是client端，client端将数据发送到dn上，由dn负责将packet写入本地磁盘，并向下一个dn发送。
DataXceiverServer 是在 DataNode 启动的线程，run 方法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public void run() {
  Peer peer = null;
  while (datanode.shouldRun &amp;amp;&amp;amp; !datanode.shutdownForUpgrade) {
    try {
      // 接收client的socket请求
      peer = peerServer.accept();
     
     // 一个 Daemon线程
      new Daemon(datanode.threadGroup,
          DataXceiver.create(peer, datanode, this))
          .start();
    } catch 
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里先循环接受请求，每次接受到一个就新建一个线程，加入线程组中。每个 DataXceiver 线程的 run 方法就是处理请求的：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;dataXceiverServer.addPeer(peer, Thread.currentThread(), this);
peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);
InputStream input = socketIn;
try {
  IOStreamPair saslStreams = datanode.saslServer.receive(peer, socketOut,
    socketIn, datanode.getXferAddress().getPort(),
    datanode.getDatanodeId());
  input = new BufferedInputStream(saslStreams.in,
    HdfsConstants.SMALL_BUFFER_SIZE);
  socketOut = saslStreams.out;
} catch 

super.initialize(new DataInputStream(input));

op = readOp();
processOp(op);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;processOp():&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;switch(op) {
    case READ_BLOCK:
      opReadBlock();
      break;
    case WRITE_BLOCK:
      opWriteBlock(in);
      break;

    default:
      throw new IOException(&amp;quot;Unknown op &amp;quot; + op + &amp;quot; in data stream&amp;quot;);
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;private void opWriteBlock(DataInputStream in) throws IOException {
    final OpWriteBlockProto proto = OpWriteBlockProto.parseFrom(vintPrefixed(in));
    final DatanodeInfo[] targets = PBHelper.convert(proto.getTargetsList());
    TraceScope traceScope = continueTraceSpan(proto.getHeader(),
        proto.getClass().getSimpleName());
    try {
      writeBlock(PBHelper.convert(proto.getHeader().getBaseHeader().getBlock()),
          PBHelper.convertStorageType(proto.getStorageType()),
          PBHelper.convert(proto.getHeader().getBaseHeader().getToken()),
          proto.getHeader().getClientName(),
          targets,
          PBHelper.convertStorageTypes(proto.getTargetStorageTypesList(), targets.length),
          PBHelper.convert(proto.getSource()),
          fromProto(proto.getStage()),
          proto.getPipelineSize(),
          proto.getMinBytesRcvd(), proto.getMaxBytesRcvd(),
          proto.getLatestGenerationStamp(),
          fromProto(proto.getRequestedChecksum()),
          (proto.hasCachingStrategy() ?
              getCachingStrategy(proto.getCachingStrategy()) :
            CachingStrategy.newDefaultStrategy()),
            (proto.hasAllowLazyPersist() ? proto.getAllowLazyPersist() : false));
     } finally {
      if (traceScope != null) traceScope.close();
     }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个方法有我们的断点，我们会在里面进行调试 writeBlock ，大致步骤为：&lt;/p&gt;

&lt;p&gt;新建 给客户端发确认消息的 replyOut 流，
新建 给其他DataNode 发消息的 mirror 流
新建 BlockReceiver  接收数据，接收到 Packet 后进行判断，如果不是最后一个，写到输出流，如果是最后一个或者长度为0，不做处理。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;
public void writeBlock(final ExtendedBlock block,
    final StorageType storageType, 
    final Token&amp;lt;BlockTokenIdentifier&amp;gt; blockToken,
    final String clientname,
    final DatanodeInfo[] targets,
    final StorageType[] targetStorageTypes, 
    final DatanodeInfo srcDataNode,
    final BlockConstructionStage stage,
    final int pipelineSize,
    final long minBytesRcvd,
    final long maxBytesRcvd,
    final long latestGenerationStamp,
    DataChecksum requestedChecksum,
    CachingStrategy cachingStrategy,
    final boolean allowLazyPersist) throws IOException {
    
    // 我们简单取一部分代码
    
    // 输出流，现在在 DataXCerver 中，输入流就是客户端写，输出流自然即时发送到客户端的
    final DataOutputStream replyOut = new DataOutputStream(
        new BufferedOutputStream(
            getOutputStream(),
            HdfsConstants.SMALL_BUFFER_SIZE));
    
    }
    
    // 这里一大堆的 mirrorOut 开头的流都是 数据写到 DataNode 后复制副本用的，由于在本地只有一个副本，所以都是 null 
    DataOutputStream mirrorOut = null;  // stream to next target
    DataInputStream mirrorIn = null;    // reply from next target
    Socket mirrorSock = null;           // socket to next target
    String mirrorNode = null;           // the name:port of next target
    String firstBadLink = &amp;quot;&amp;quot;;           // first datanode that failed in connection setup
    Status mirrorInStatus = SUCCESS;
    final String storageUuid;
    
    // 新建 BlockReceiver， BlockReceiver 的作用可以好好看注释，另外它的构造方法也有很大的信息量，其中有个 in 输入流，
    // 是在 DataXCerver 的 run 方法中：super.initialize(new DataInputStream(input)); 也就是对输入的 socket 流的二层包装
    blockReceiver = new BlockReceiver(block, storageType, in,
    peer.getRemoteAddressString(),
    peer.getLocalAddressString(),
    stage, latestGenerationStamp, minBytesRcvd, maxBytesRcvd,
    clientname, srcDataNode, datanode, requestedChecksum,
    cachingStrategy, allowLazyPersist)
    
    // 然后是一大堆的副本拷贝，我们本地调试线跳过。
    if (targets.length &amp;gt; 0) {// 跳过}
    
      // 给客户端发一个应答消息
      if (isClient &amp;amp;&amp;amp; !isTransfer) {
        if (LOG.isDebugEnabled() || mirrorInStatus != SUCCESS) {
          LOG.info(&amp;quot;Datanode &amp;quot; + targets.length +
                   &amp;quot; forwarding connect ack to upstream firstbadlink is &amp;quot; +
                   firstBadLink);
        }
        BlockOpResponseProto.newBuilder()
          .setStatus(mirrorInStatus)
          .setFirstBadLink(firstBadLink)
          .build()
          .writeDelimitedTo(replyOut);
        replyOut.flush();
      }
      
      // 这里开始写数据，
    blockReceiver.receiveBlock(mirrorOut, mirrorIn, replyOut,mirrorAddr, null, targets, false);
    
    // 后续处理
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们看一下 blockReceiver.receiveBlock&lt;/p&gt;

&lt;p&gt;new PacketResponder
receivePacket()&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;// 如果是来自客户端而且传输，新建回复的线程
if (isClient &amp;amp;&amp;amp; !isTransfer) {
        responder = new Daemon(datanode.threadGroup, 
            new PacketResponder(replyOut, mirrIn, downstreams));
        responder.start(); // start thread to processes responses
      }

// 循环写
while (receivePacket() &amp;gt;= 0) { /* R} 这里就是写所在逻辑了

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;receivePacket 方法：&lt;/p&gt;

&lt;p&gt;packetReceiver.receiveNextPacket(in); 接受一个 Packet&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;receivePacket{
packetReceiver.receiveNextPacket(in);

PacketHeader header = packetReceiver.getHeader();

// 这里把数据写到 副本上面
//First write the packet to the mirror:
    if (mirrorOut != null &amp;amp;&amp;amp; !mirrorError) {
      try {
        long begin = Time.monotonicNow();
        packetReceiver.mirrorPacketTo(mirrorOut);
        mirrorOut.flush();
        long duration = Time.monotonicNow() - begin;
        if (duration &amp;gt; datanodeSlowLogThresholdMs) {
          LOG.warn(&amp;quot;Slow BlockReceiver write packet to mirror took &amp;quot; + duration
              + &amp;quot;ms (threshold=&amp;quot; + datanodeSlowLogThresholdMs + &amp;quot;ms)&amp;quot;);
        }
      } catch (IOException e) {
        handleMirrorOutError(e);
      }
    }
    
    // 得到 数据和 检查数据
    ByteBuffer dataBuf = packetReceiver.getDataSlice();
    ByteBuffer checksumBuf = packetReceiver.getChecksumSlice();
    
    // 如果是最后一个 packet
    if (lastPacketInBlock || len == 0) {
      if(LOG.isDebugEnabled()) {
        LOG.debug(&amp;quot;Receiving an empty packet or the end of the block &amp;quot; + block);
      }
      // sync block if requested
      if (syncBlock) {
        flushOrSync(true);
      }
    } else {
        。。。。。
        out.write(dataBuf.array(), startByteToDisk, numBytesToDisk);
    }
    // 这里就行 check 很复杂，部分数据的 check 需要重新计算。
    checksumOut.write(buf);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里的代码太复杂，而且调试的时候稍微时间长一点就出现 连接异常。可以通过配置参数解决。
然后得到了 dataBuf 和 checksumBuf，如果不是最后一个，我们需要写到 DataNode 的具体数据中。
到这里整个写的操作就完成。中间的 和客户端心跳应答、失败处理。都没有涉及。简单梳理一下：&lt;/p&gt;

&lt;h1 id=&#34;五-pipelineack-和-packetresponder-信息处理&#34;&gt;五、PipeLineAck 和 PacketResponder 信息处理&lt;/h1&gt;

&lt;p&gt;上面的 one.writeTo(blockStream); 代码是将 Packet 写入到输出流，写出之前有一段代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt; // send the packet
 synchronized (dataQueue) {
   // move packet from dataQueue to ackQueue
   if (!one.isHeartbeatPacket()) {
     dataQueue.removeFirst();
     ackQueue.addLast(one);
     dataQueue.notifyAll();
   }
 }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就是将 Packet 放进 ackQueue 中，很明显这又是一个 消费者生产者。&lt;/p&gt;

&lt;p&gt;新建 DataStreamer 的run中，得到输出流，
建立 pipeLine 之后，会有 initDataStreaming 操作，还要启一个新的线程 ResponseProcessor 接收packet的ack，这个线程在initDataStreaming中启动，并更新DataStreamer线程的状态为DATA_STREAMING
ResponseProcessor，在 ResponseProcessor 的run方法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;synchronized (dataQueue) {
  one = ackQueue.getFirst();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意每次锁住的都是 dataQueue 对象。
与他对应的是 BlockReceiver 中的 PacketResponder 。 PacketResponder 的 run 方法比较长，主要是按照 packet 的写入顺序发送 ack。&lt;/p&gt;

&lt;p&gt;由于调试器的问题，这里一直跳不进去，能看到大概逻辑： 当数据节点顺利处理完数据，而且当前节点处在数据节点中间（收到下游DataNode的消息）
如果 ackQueue 中有数据，获取一个记录，接下来如果当前节点位于数据管道的中间，就在mirror流读取下游的确认，我们在本地调试是没有这一步的。&lt;/p&gt;

&lt;p&gt;如果当前节点位于数据管道的最后， 调用 pkt = waitForAckHead(seqno) 从 ackQueue 取出对应的 pkt ，然后调用 lastPacketInBlock = pkt.lastPacketInBlock;
然后是  if (lastPacketInBlock) {finalizeBlock(startTime);} ，然后是 sendAckUpstream ，这个就是给客户端发送 ack&lt;/p&gt;

&lt;p&gt;然后 ResponseProcessor 中收到了 ack，进行处理。逻辑比较简单，ack.readFields(blockReplyStream) 读取 ask，从输入流中读取的ack的seqno与ackQueue中取得的seqno不一样则抛出异常&lt;/p&gt;

&lt;p&gt;// 接收到ack后，从ackQueue中移除packet
      synchronized (dataQueue) {
        lastAckedSeqno = seqno;
        ackQueue.removeFirst();
        dataQueue.notifyAll();
        one.releaseBuffer(byteArrayManager);
      }
。&lt;/p&gt;

&lt;h1 id=&#34;6-namenode-处理&#34;&gt;6.NameNode 处理&lt;/h1&gt;

&lt;p&gt;上面我们说了 NameNode 的处理我们没法调试所以我们跳过了NameNode，原因是客户端通过远程的RPC调用执行，这次我们在IDEA启动NameNode，然后查看NameNode是如何处理写文件的。&lt;/p&gt;

&lt;p&gt;通过 RPC 调用 NameNodeRpcServer.create -&amp;gt; namesystem.startFile -&amp;gt; startFileInt -&amp;gt; startFileInternal&lt;/p&gt;

&lt;p&gt;中间有上锁的步骤，还有 BlockManager 的验证，namesystem 中有一个 dir:FSDirectory 的属性，a pure in-memory data structure ，
里面又有一个 rootDir：INodeDirectory 代表根节点，rootDir 中有一个 List&lt;INode&gt; children 代表所以的文件 ，还有一个 INodeMap 对象，保存。
注意还有一个 INodesInPath 类，从他的方法我们看出，它代表一个文件递归得到所有父目录的 InpPath 文件。&lt;/p&gt;

&lt;p&gt;startFileInternal 方法主要步骤如下：
- 根据 src 得到 INodesInPath，再得到 INodeFile 进行检验。
- newNode = dir.addFile(src, permissions, replication, blockSize,holder, clientMachine);
- getEditLog().logOpenFile(src, newNode, overwrite, logRetryEntry); 记录日志
- leaseManager.addLease(newNode.getFileUnderConstructionFeature()&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt; private BlocksMapUpdateInfo startFileInternal(FSPermissionChecker pc, 
      String src, PermissionStatus permissions, String holder, 
      String clientMachine, boolean create, boolean overwrite, 
      boolean createParent, short replication, long blockSize, 
      boolean isLazyPersist, CipherSuite suite, CryptoProtocolVersion version,
      EncryptedKeyVersion edek, boolean logRetryEntry)

//得到文件
final INodeFile myFile = INodeFile.valueOf(inode, src, true);

try {
      BlocksMapUpdateInfo toRemoveBlocks = null;
      if (myFile == null) {
      // 判断是否创建
        if (!create) {
          throw new FileNotFoundException(&amp;quot;Can&#39;t overwrite non-existent &amp;quot; +
              src + &amp;quot; for client &amp;quot; + clientMachine);
        }
      } else {
      // 是否覆盖
        if (overwrite) {
          toRemoveBlocks = new BlocksMapUpdateInfo();
          List&amp;lt;INode&amp;gt; toRemoveINodes = new ChunkedArrayList&amp;lt;INode&amp;gt;();
          long ret = dir.delete(src, toRemoveBlocks, toRemoveINodes, now());
          if (ret &amp;gt;= 0) {
            incrDeletedFileCount(ret);
            removePathAndBlocks(src, null, toRemoveINodes, true);
          }
        } else {
        // 存在而且不覆盖的情况？操作 lease
          // If lease soft limit time is expired, recover the lease
          recoverLeaseInternal(myFile, src, holder, clientMachine, false);
          throw new FileAlreadyExistsException(src + &amp;quot; for client &amp;quot; +
              clientMachine + &amp;quot; already exists&amp;quot;);
        }
      }

// Always do an implicit mkdirs for parent directory tree.
// 父目录
      Path parent = new Path(src).getParent();
      if (parent != null &amp;amp;&amp;amp; mkdirsRecursively(parent.toString(),
              permissions, true, now())) {
              // 添加到 namespace
        newNode = dir.addFile(src, permissions, replication, blockSize,
                              holder, clientMachine);
      }
      // 操作 lease
      leaseManager.addLease(newNode.getFileUnderConstructionFeature()
          .getClientName(), src);

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;INodesInPath：&lt;/p&gt;

&lt;p&gt;newNode = dir.addFile(src, permissions, replication, blockSize,holder, clientMachine);&lt;/p&gt;

&lt;p&gt;这个 dir 是 FSDirectory 类型的变量，最终调用了 INodeDirectory.addChild(INode) 方法，时间上就是给他的 children add 一个 INode&lt;/p&gt;

&lt;p&gt;和 addlease :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;synchronized Lease addLease(String holder, String src) {
    Lease lease = getLease(holder);
    if (lease == null) {
      lease = new Lease(holder);
      leases.put(holder, lease);
      sortedLeases.add(lease);
    } else {
      renewLease(lease);
    }
    sortedLeasesByPath.put(src, lease);
    lease.paths.add(src);
    return lease;
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;  /** Get a lease and start automatic renewal */
  private void beginFileLease(final long inodeId, final DFSOutputStream out)
      throws IOException {
    getLeaseRenewer().put(inodeId, out, this);
  }
  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;调用完 startFileInternal 后，会调用
stat = dir.getFileInfo(src, false, FSDirectory.isReservedRawName(srcArg), true);
实际上就是得到一个 HdfsFileStatus 的对象，返回给客户端。NameNode 穿件文件元数据信息的过程大致如此。&lt;/p&gt;

&lt;h1 id=&#34;七-datanode-创建文件&#34;&gt;七、DataNode 创建文件&lt;/h1&gt;

&lt;p&gt;上面讲了在 DataNode 的 DataXCeiver 的写数据过程，但是我们忽略了一些和流式接口无关的部分，包括创建数据块等。&lt;/p&gt;

&lt;p&gt;实际上客户端 DFSClient 发送创建元数据给 NameNode 以后，就要根据 HdfsFileStatus 创建到 DataNode 的输出流。
在 DataStreamer 的 createBlockOutputStream 方法中创建了 blockStream = out，实际上创建之前有个步骤：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;// send the request
new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,
    dfsClient.clientName, nodes, nodeStorageTypes, null, bcs, 
    nodes.length, block.getNumBytes(), bytesSent, newGS,
    checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个 Sender 和 DataXCerver 一样，继承自 DataTransferProtocol ，我们可以理解为 DataTransferProtocol 是客户端和 DataNode 通信协议，通信实现包含了创建 数据块和发送数据，
而 Sender 和 DataXCerver 就分别是创建数据块和发送数据的实现，分别是 TCP 和 RPC 通信方式实现。&lt;/p&gt;

&lt;p&gt;new Sender(out).writeBlock 只是简单的将信息以 ProtoBuf 的格式发送出去。实际上就是发送了一个 op ，这个 op 服务端会收到并解析，然后根据 op 的值调用不同方法，最后调用了：&lt;/p&gt;

&lt;p&gt;前面分析过： BlockReceiver.receiveBlock 。里面就有构建输出流写出，但是我们没有相关这个输出流是怎么来的，实际上就是一个文件流。我们看一下 BlockReceiver 的构造方法。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;this.block = block;

switch (stage) {
        case PIPELINE_SETUP_CREATE:
          replicaInfo = datanode.data.createRbw(storageType, block, allowLazyPersist);
          datanode.notifyNamenodeReceivingBlock(
              block, replicaInfo.getStorageUuid());
          break;


streams = replicaInfo.createStreams(isCreate, requestedChecksum);
this.out = streams.getDataOut();
this.checksumOut = new DataOutputStream(new BufferedOutputStream(
          streams.getChecksumOut(), HdfsConstants.SMALL_BUFFER_SIZE));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里有个 switch (stage)，实际上写文件都是一个 PIPELINE ，包括输入某个 client - DataNode - DataNode - DataNode ，由于我是本地模式调试，所以感觉不到这种 PIPELINE 的模式而已。&lt;/p&gt;

&lt;p&gt;datanode.data 是一个 FSDataSetImpl 类型的变量，控制着整个 DataNode 的文件信息。类型结构大致如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FSDataSetImpl 
    DataStorage
        bpStorageMap&amp;lt;bp,BlockPoolSliceStorage&amp;gt;
    FSVolumnList
        List&amp;lt;FSVolumnImpl&amp;gt;
             bpSlices&amp;lt;bp, BlockPoolSlice&amp;gt;
    volumeMap:ReplicaMap&amp;lt;bp, Map&amp;lt;blockid, ReplicaInfo&amp;gt;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;FSDataSetImpl 代表整个 DataNode 的文件存储，FSVolumnList 是因为我们配置的 data 存储路径，可以用逗号隔开，一般情况下配置1个路径，FSVolumnList 里面就一个 FsVolumeImpl。
FsVolumeImpl 里面有分为 多个 blockpool 存储，一个 blockpool 对应一个文件夹而已，这在一开始版本是没有的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Storage

  List&amp;lt;StorageDirectory&amp;gt; storageDirs
  public int   layoutVersion;   // layout version of the storage data
  public int   namespaceID;     // id of the file system
  public String clusterID;      // id of the cluster
  public long  cTime;   
  static class StorageDirectory
      File root

DataStorage
   private String datanodeUuid = null;
   Map&amp;lt;String, BlockPoolSliceStorage&amp;gt; bpStorageMap
   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;FsVolumeImpl&lt;/p&gt;

&lt;p&gt;replicaInfo 保存了文件的位置信息，所以可以用来创建输出流。&lt;/p&gt;

&lt;h1 id=&#34;八-租约处理&#34;&gt;八、租约处理&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;synchronized void put(final long inodeId, final DFSOutputStream out,
      final DFSClient dfsc) {
    if (dfsc.isClientRunning()) {
      if (!isRunning() || isRenewerExpired()) {
        //start a new deamon with a new id.
        final int id = ++currentId;
        daemon = new Daemon(new Runnable() {
          @Override
          public void run() {
            try {
              if (LOG.isDebugEnabled()) {
                LOG.debug(&amp;quot;Lease renewer daemon for &amp;quot; + clientsString()
                    + &amp;quot; with renew id &amp;quot; + id + &amp;quot; started&amp;quot;);
              }
              LeaseRenewer.this.run(id);
            } catch(InterruptedException e) {
              if (LOG.isDebugEnabled()) {
                LOG.debug(LeaseRenewer.this.getClass().getSimpleName()
                    + &amp;quot; is interrupted.&amp;quot;, e);
              }
            } finally {
              synchronized(LeaseRenewer.this) {
                Factory.INSTANCE.remove(LeaseRenewer.this);
              }
              if (LOG.isDebugEnabled()) {
                LOG.debug(&amp;quot;Lease renewer daemon for &amp;quot; + clientsString()
                    + &amp;quot; with renew id &amp;quot; + id + &amp;quot; exited&amp;quot;);
              }
            }
          }
          
          @Override
          public String toString() {
            return String.valueOf(LeaseRenewer.this);
          }
        });
        daemon.start();
      }
      dfsc.putFileBeingWritten(inodeId, out);
      emptyTime = Long.MAX_VALUE;
    }
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最重要的就是  LeaseRenewer.this.run(id)， 在run中调用renew对租约续约。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;for(long lastRenewed = Time.now(); !Thread.interrupted();
        Thread.sleep(getSleepPeriod())) {
      final long elapsed = Time.now() - lastRenewed;
      if (elapsed &amp;gt;= getRenewalTime()) {
        try {
          renew();
          if (LOG.isDebugEnabled()) {
            LOG.debug(&amp;quot;Lease renewer daemon for &amp;quot; + clientsString()
                + &amp;quot; with renew id &amp;quot; + id + &amp;quot; executed&amp;quot;);
          }
          lastRenewed = Time.now();
        } catch (SocketTimeoutException ie) {
          LOG.warn(&amp;quot;Failed to renew lease for &amp;quot; + clientsString() + &amp;quot; for &amp;quot;
              + (elapsed/1000) + &amp;quot; seconds.  Aborting ...&amp;quot;, ie);
          synchronized (this) {
            while (!dfsclients.isEmpty()) {
              dfsclients.get(0).abort();
            }
          }
          break;
        } catch (IOException ie) {
          LOG.warn(&amp;quot;Failed to renew lease for &amp;quot; + clientsString() + &amp;quot; for &amp;quot;
              + (elapsed/1000) + &amp;quot; seconds.  Will retry shortly ...&amp;quot;, ie);
        }
      }

&lt;/code&gt;&lt;/pre&gt;
</description>
      
    </item>
    
    <item>
      <title>resourcemanager</title>
      <link>https://dengziming.github.io/post/hadoop/first/</link>
      <pubDate>Tue, 22 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/hadoop/first/</guid>
      
        <description>&lt;p&gt;参考资料：&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://hortonworks.com/blog/apache-hadoop-yarn-resourcemanager/&#34;&gt;https://hortonworks.com/blog/apache-hadoop-yarn-resourcemanager/&lt;/a&gt;&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>yarn-nodemanager-剖析</title>
      <link>https://dengziming.github.io/post/hadoop/yarn-nodemanager-1/</link>
      <pubDate>Tue, 22 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/hadoop/yarn-nodemanager-1/</guid>
      
        <description>

&lt;h1 id=&#34;架构&#34;&gt;架构&lt;/h1&gt;

&lt;p&gt;ContainerManagementImpl&lt;/p&gt;

&lt;h1 id=&#34;container-生命周期&#34;&gt;Container 生命周期&lt;/h1&gt;

&lt;p&gt;第一步是 RM 的 applicationMasterLauncher ，创建 ApplicationMasterLauncher 后，遇到 launch 时间 ，
case LAUNCH: launch(application); =&amp;gt; new AMLauncher(context, application, event, getConfig());&lt;/p&gt;

&lt;p&gt;这个任务放进 队列里面等待执行，一旦执行会调用 launch() 方法，然后调用 containerMgrProxy.startContainers(allRequests); 这是 RPC 调用&lt;/p&gt;

&lt;p&gt;实际上就是 ContainerManagementImpl ，然后会调用 startContainerInternal ，然后就是 new ContainerImpl 。&lt;/p&gt;

&lt;p&gt;这是 APPMaster 启动需要的 container ，实际上还有 APPMaster 调度任务需要更多的 Container ，继续向 ContainerManagementImpl 请求&lt;/p&gt;

&lt;h2 id=&#34;1-资源本地化&#34;&gt;1. 资源本地化&lt;/h2&gt;

&lt;p&gt;实际上就是 ContainerManagementImpl ，然后会调用 startContainerInternal ，然后就是 new ContainerImpl 。
然后通过 if (null == context.getApplications().putIfAbsent(applicationID,application)) 判断是否是该 NodeManager 第一个 Container ，如果是的话，new ApplicationImpl
向 ApplicationImpl 发送 ApplicationInitEvent 事件，同时发送 ApplicationContainerInitEvent 事件。&lt;/p&gt;

&lt;p&gt;这些事件会触发 ACL、log等相关的事件， 收到 ApplicationContainerInitEvent 后将 Container 加入 ApplicationImpl 的维护列表。&lt;/p&gt;

&lt;p&gt;logHandle 处理完成之后会发送一个 log 事件，applicationImpl 收到后向 ResourceLocalizeService 发送 事件，
为 private 和 application 级别的资源创建 LocalResourceTrackerImp ，为下载资源作准备。&lt;/p&gt;

&lt;p&gt;private 的资源用户可见，如果该用户已经提交过了，无需创建。同理，如果 application 已经启动过 container 了，则同一个 application 的新 container 不必在创建。&lt;/p&gt;

&lt;p&gt;经过上面操作后，ResourceLocalizeService 向 ApplicationImpl 发送 Application_Init&lt;/p&gt;

&lt;p&gt;ApplicationImpl 收到 INIT 后，向所有的 ContainerImpl 发送 InitContainer ，ApplicationImpl 也从 ApplicationState.INITING 变为 ApplicationState.RUNNING,&lt;/p&gt;

&lt;p&gt;InitContainer 命令后，和 AuxService 交互，然后从 ContainerLaunchContext 得到各类可见性资源并保存到相应数据结构，然后发送给 ResourceLocalizeService 。&lt;/p&gt;

&lt;p&gt;ResourceLocalizeService 调用 handleInitContainerResources((ContainerLocalizationRequestEvent) event); 实际是 是发送给 LocalResourcesTrackerImpl 。&lt;/p&gt;

&lt;p&gt;LocalResourcesTrackerImpl 会 判断是否需要下载等，为对应的资源创建 LocalizedResource 状态机，将 Request 发送给 LocalizedResource。&lt;/p&gt;

&lt;p&gt;后续还是这样的时间驱动，总之可以概括为 ： NodeManager 上同一个 App 所有的 ContainerImpl 异步并发向向资源下载服务 ResourceLocalizeService 发送待下载的资源，
ResourceLocalizeService下载完成后会通知依赖资源的所以 Container ，当一个 Container 依赖的资源全部下载完毕，Container 将会进入 运行阶段&lt;/p&gt;

&lt;h2 id=&#34;2-container-运行&#34;&gt;2. Container 运行&lt;/h2&gt;

&lt;p&gt;运行是 ContainerLauncher 服务实现的，主要过程为： 将待运行 Container 所需要的环境变量和运行命令写到 &lt;code&gt;launch_container.sh&lt;/code&gt; 中，
将启动该脚本的命令写入：&lt;code&gt;default_container_executor.sh&lt;/code&gt; 中。&lt;/p&gt;

&lt;p&gt;通过运行该脚本启动 Container 。主要有四步：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;ContainerImpl 向 ContainersLauncher 发送 Launch_container ，请求启动 container。
dispatcher.getEventHandler().handle(new ContainersLauncherEvent(this, launcherEvent));&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ContainersLauncher 收到后，&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;Application app =context.getApplications().get(containerId.getApplicationAttemptId().getApplicationId());
ContainerLaunch launch = new ContainerLaunch(context, getConfig(), dispatcher, exec, app,event.getContainer(), dirsHandler, containerManager);
containerLauncher.submit(launch);
running.put(containerId, launch);
break;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ContainerLaunch 放到线程池执行，对应的 call 方法为：&lt;/p&gt;

&lt;p&gt;为 Container 创建 token 文件 和 &lt;code&gt;launch_container.sh&lt;/code&gt; ，将他们保存到 NodeManager 私有目录 nmPrivate 下面， &lt;code&gt;launch_container.sh&lt;/code&gt;包含了运行所以的命令。
一般都是前面 export 环境变量，最后有个 exec 命令 。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;准备好了 命令，
&lt;code&gt;Container_Launcher&lt;/code&gt; 首先向 ContainerImpl 发送 &lt;code&gt;Container_LANUCHED&lt;/code&gt; 命令，然他启动监控等。然后调用 ContainerExector launchContainer 启动 Container 。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;然后是启动监控，汇报信息等。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>yarn-resourcemanager-1</title>
      <link>https://dengziming.github.io/post/hadoop/yarn-resourcemanager-1/</link>
      <pubDate>Tue, 22 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/hadoop/yarn-resourcemanager-1/</guid>
      
        <description>

&lt;p&gt;参考资料：&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://hortonworks.com/blog/apache-hadoop-yarn-resourcemanager/&#34;&gt;https://hortonworks.com/blog/apache-hadoop-yarn-resourcemanager/&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;提交应用程序的过程&#34;&gt;提交应用程序的过程&lt;/h1&gt;

&lt;h2 id=&#34;1-yarnclient-submitapplication-appcontext&#34;&gt;1. yarnClient.submitApplication(appContext);&lt;/h2&gt;

&lt;p&gt;新建请求，最终调用： rmClient.submitApplication(request);&lt;/p&gt;

&lt;p&gt;实际上会通过RPC调用 ClientRMService.submitApplication(SubmitApplicationRequest request)&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;得到APPID：ApplicationId applicationId = submissionContext.getApplicationId();&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;rmAppManager.submitApplication(submissionContext, System.currentTimeMillis(), user);&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;放到 rmAppManager 中，rmAppManager 中存放了所有的 application。
跟进去，发现调用了：&lt;/p&gt;

&lt;p&gt;this.rmContext.getDispatcher().getEventHandler().handle(new RMAppEvent(applicationId, RMAppEventType.START));&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public void handle(RMAppEvent event) {
      ApplicationId appID = event.getApplicationId();
      RMApp rmApp = this.rmContext.getRMApps().get(appID);
      if (rmApp != null) {
        try {
          rmApp.handle(event);
        } catch (Throwable t) {
          LOG.error(&amp;quot;Error in handling event type &amp;quot; + event.getType()
              + &amp;quot; for application &amp;quot; + appID, t);
        }
      }
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后导致这个 applicationId 所在的 RMAppEvent 状态机发生变化。&lt;/p&gt;

&lt;h2 id=&#34;2-registerapplicationmasterresponse-response-amrmclient-registerapplicationmaster-appmasterhostname-appmasterrpcport-appmastertrackingurl&#34;&gt;2.RegisterApplicationMasterResponse response = amRMClient.registerApplicationMaster(appMasterHostname, appMasterRpcPort,appMasterTrackingUrl);&lt;/h2&gt;

&lt;p&gt;注册 ApplicationMaster，注意这段代码是在用户编写的 ApplicationMaster 类中，所以这段代码运行在yarn给APPMaster分配的Container中。&lt;/p&gt;

&lt;p&gt;RegisterApplicationMasterResponse response = client.registerApplicationMaster(appHostName, appHostPort, appTrackingUrl);&lt;/p&gt;

&lt;p&gt;会调用：RegisterApplicationMasterResponse response = rmClient.registerApplicationMaster(request);&lt;/p&gt;

&lt;p&gt;最终会通过RPC调用：ApplicationMasterServeice.registerApplicationMaster(RegisterApplicationMasterRequest request)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;this.rmContext
        .getDispatcher()
        .getEventHandler()
        .handle(
          new RMAppAttemptRegistrationEvent(applicationAttemptId, request
            .getHost(), request.getRpcPort(), request.getTrackingUrl()));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这种 RMAppAttemptEventType 类型的会 通过handle进行处理：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public void handle(RMAppAttemptEvent event) {
      ApplicationAttemptId appAttemptID = event.getApplicationAttemptId();
      ApplicationId appAttemptId = appAttemptID.getApplicationId();
      RMApp rmApp = this.rmContext.getRMApps().get(appAttemptId);
      if (rmApp != null) {
        RMAppAttempt rmAppAttempt = rmApp.getRMAppAttempt(appAttemptID);
        if (rmAppAttempt != null) {
          try {
            rmAppAttempt.handle(event);
          } catch (Throwable t) {
            LOG.error(&amp;quot;Error in handling event type &amp;quot; + event.getType()
                + &amp;quot; for applicationAttempt &amp;quot; + appAttemptId, t);
          }
        }
      }
    }
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;和上面的 RMAppEvent 一样，会进入一个状态机进行处理。&lt;/p&gt;

&lt;h3 id=&#34;1-状态机相互转换细节&#34;&gt;1.状态机相互转换细节&lt;/h3&gt;

&lt;p&gt;上面的过程细化一下：&lt;/p&gt;

&lt;p&gt;RMAppImpl 收到 RMAppEventType.START 事件后，会调用 RMStateStore#storeApplication，以日志记录 RMAppImpl 当前信息，&lt;/p&gt;

&lt;p&gt;至此，RMAppImpl 的运行状态由 NEW 转移为 NEW_SAVING。该步骤就较为复杂了，下面详细介绍下。&lt;/p&gt;

&lt;p&gt;其中 RMAppEventType 注册到中央异步调度器的地方在 ResourceManager.java 中，new ApplicationEventDispatcher(rmContext) 进行处理，
处理方式很简单：通过appid得到得到 RMAppImpl ，最终会给  RMAppImpl自己处理，进入他的状态机处理。状态机有这么一个事件：&lt;/p&gt;

&lt;p&gt;addTransition(RMAppState.NEW, RMAppState.NEW_SAVING, RMAppEventType.START, new RMAppNewlySavingTransition())&lt;/p&gt;

&lt;p&gt;RMAppNewlySavingTransition 的 transition 就是 app.rmContext.getStateStore().storeNewApplication(app);  实际上就是保存应用的相关信息。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public synchronized void storeNewApplication(RMApp app) {  
    //app=RMAppImpl  
    LOG.info(&amp;quot;begin to storeNewApplication,app=&amp;quot;+app.toString());  
    ApplicationSubmissionContext context = app.getApplicationSubmissionContext();  
    assert context instanceof ApplicationSubmissionContextPBImpl;  
    ApplicationState appState =  
        new ApplicationState(app.getSubmitTime(), app.getStartTime(), context,app.getUser());  
    dispatcher.getEventHandler().handle(new RMStateStoreAppEvent(appState));  
  }  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意： dispatcher.getEventHandler().handle(new RMStateStoreAppEvent(appState));  这里会调用 RMStateStore 状态机的 transition，实际上就是 store + notifyDoneStoringApplication&lt;/p&gt;

&lt;p&gt;&lt;code&gt;rmDispatcher.getEventHandler().handle(new RMAppNewSavedEvent(appId, storedException));&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;这个事件又会进入 RMAppImpl 的状态机，对应代码 addTransition(RMAppState.NEW_SAVING, RMAppState.SUBMITTED, RMAppEventType.APP_NEW_SAVED, new AddApplicationToSchedulerTransition())&lt;/p&gt;

&lt;p&gt;调用：app.handler.handle(new AppAddedSchedulerEvent(app.applicationId,app.submissionContext.getQueue(), app.user));&lt;/p&gt;

&lt;p&gt;会触发： RMAppImpl 处理 AppAddedSchedulerEvent&lt;/p&gt;

&lt;p&gt;然后这个事件会分配给：CapacityScheduler ，&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;case APP_ADDED:  
    {  
      AppAddedSchedulerEvent appAddedEvent = (AppAddedSchedulerEvent) event;  
      addApplication(appAddedEvent.getApplicationId(),  
        appAddedEvent.getQueue(), appAddedEvent.getUser());  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;addApplication 会调用 rmContext.getDispatcher().getEventHandler().handle(new RMAppEvent(applicationId, RMAppEventType.APP_ACCEPTED));&lt;/p&gt;

&lt;p&gt;RMAppImpl 会触发 ：addTransition(RMAppState.SUBMITTED, RMAppState.ACCEPTED,  RMAppEventType.APP_ACCEPTED, new StartAppAttemptTransition())&lt;/p&gt;

&lt;p&gt;对应的transition： createNewAttempt(); handler.handle(new RMAppStartAttemptEvent(currentAttempt.getAppAttemptId(),  transferStateFromPreviousAttempt));&lt;br /&gt;
实际上就是触发 RMAppAttemptImpl 状态机操作。&lt;/p&gt;

&lt;p&gt;RMAppAttemptImpl 接受 RMAppAttemptEventType.START 事件后，进行一系列初始化工作。将自身状态由NEW转换为SUBMITTED，并调用 AttemptStartedTransition。&lt;/p&gt;

&lt;p&gt;AttemptStartedTransition appAttempt.eventHandler.handle(new AppAttemptAddedSchedulerEvent(  appAttempt.applicationAttemptId, transferStateFromPreviousAttempt));&lt;/p&gt;

&lt;p&gt;AppAttemptAddedSchedulerEvent 会交给 CapacityScheduler 。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;case APP_ATTEMPT_ADDED:  
    {  
      AppAttemptAddedSchedulerEvent appAttemptAddedEvent =  
          (AppAttemptAddedSchedulerEvent) event;  
      addApplicationAttempt(appAttemptAddedEvent.getApplicationAttemptId(),  
        appAttemptAddedEvent.getTransferStateFromPreviousAttempt(),  
        appAttemptAddedEvent.getShouldNotifyAttemptAdded());  
    }  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实际上就是讲这个 attempt 放进队列，等待处理。并且：rmContext.getDispatcher().getEventHandler().handle( new RMAppAttemptEvent(applicationAttemptId, RMAppAttemptEventType.ATTEMPT_ADDED));&lt;/p&gt;

&lt;p&gt;RMAppAttemptImpl 接受到事件 RMAppAttemptEventType.ATTEMPT_ADDED 后，状态由SUBMITTED转换为SCHEDULED。进入内部类ScheduleTransition的transition函数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;private static final class ScheduleTransition  
      implements  
      MultipleArcTransition&amp;lt;RMAppAttemptImpl, RMAppAttemptEvent, RMAppAttemptState&amp;gt; {  
    @Override  
    public RMAppAttemptState transition(RMAppAttemptImpl appAttempt,  
        RMAppAttemptEvent event) {  
        LOG.info(&amp;quot;class::ScheduleTransition, func::transition, begin.&amp;quot;);  
      if (!appAttempt.submissionContext.getUnmanagedAM()) {  
        // Request a container for the AM.  
        ResourceRequest request =  
            BuilderUtils.newResourceRequest(  
                AM_CONTAINER_PRIORITY, ResourceRequest.ANY, appAttempt  
                    .getSubmissionContext().getResource(), 1);  
  
        // SchedulerUtils.validateResourceRequests is not necessary because  
        // AM resource has been checked when submission  
        Allocation amContainerAllocation = appAttempt.scheduler.allocate(  
            appAttempt.applicationAttemptId,  
            Collections.singletonList(request), EMPTY_CONTAINER_RELEASE_LIST, null, null);  
        if (amContainerAllocation != null  
            &amp;amp;&amp;amp; amContainerAllocation.getContainers() != null) {  
          assert (amContainerAllocation.getContainers().size() == 0);  
        }  
        return RMAppAttemptState.SCHEDULED;  
      } else {  
        // save state and then go to LAUNCHED state  
        appAttempt.storeAttempt();  
        return RMAppAttemptState.LAUNCHED_UNMANAGED_SAVING;  
      }  
    }  
  } 
   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里面就是：新建资源 ResourceRequest ，然后 appAttempt.scheduler.allocate&lt;/p&gt;

&lt;p&gt;&amp;mdash;&amp;mdash; 这里断层了,谁触发了 AMContainerImpl 启动和分配 Container，需要后续再看。&lt;/p&gt;

&lt;p&gt;这里有个疑问需要解答一下，之前一直好奇是哪里启动了 AMContainerImpl，上面的 schedule.allocate 将需要的资源提交给 schedule ，实际上 schedule 会分配。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;application.updateResourceRequests(ask);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这一句话，&lt;/p&gt;

&lt;p&gt;以  FairScheduler 为例，启动服务会调用 initScheduler(conf); 里面有三行代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;schedulingThread = new ContinuousSchedulingThread();
schedulingThread.setName(&amp;quot;FairSchedulerContinuousScheduling&amp;quot;);
schedulingThread.setDaemon(true);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;会有守护线程调用 continuousSchedulingAttempt(); 实际上会调用：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;    for (NodeId nodeId : nodeIdList) {
      FSSchedulerNode node = getFSSchedulerNode(nodeId);
      try {
        if (node != null &amp;amp;&amp;amp; Resources.fitsIn(minimumAllocation,
            node.getAvailableResource())) {
          attemptScheduling(node);
        }
      } catch (Throwable ex) {
        LOG.error(&amp;quot;Error while attempting scheduling for node &amp;quot; + node +
            &amp;quot;: &amp;quot; + ex.toString(), ex);
      }
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个 attemptScheduling(node); 就会创建 AMContainerImpl 实例，至于怎么创建，需要了解各个 Schedule 的内部细节。&lt;/p&gt;

&lt;p&gt;ResourceManager 为应用程序的 AM 分配资源后，创建一个 RMContainerImpl，并向它发送一个 RMContainerEventType.START 事件。&lt;/p&gt;

&lt;p&gt;RMContainerImpl 收到 RMContainerEventType.START 事件后，直接向 RMAppAttemptImpl 发送一个 RMAppAttemptEventType.CONTAINER_ALLOCATED&lt;/p&gt;

&lt;p&gt;RMAppAttemptImpl 收到 RMAppAttemptEventType.CONTAINER_ALLOCATED 事件后：调用 AMContainerAllocatedTransition：&lt;/p&gt;

&lt;p&gt;transition函数中，调用 scheduler.allocate 获取分配的资源，scheduler 返回资源之前，会向 RMContainerImpl 发送 RMContainerEventType.ALLOCATED事件。&lt;/p&gt;

&lt;p&gt;RMAppAttemptImpl 收到资源后，向 RMStateStore 发送 MStateStoreEventType.STORE_APP_ATTEMPT 事件请求记录日志。&lt;/p&gt;

&lt;p&gt;至此，RMAppAttemptImpl 状态从 SCHEDULED 转换为 ALLOCATED_SAVING。&lt;/p&gt;

&lt;p&gt;日志记录完成后，RMStateStore 向 RMAppAttemptImpl 发送 RMAppAttemptEventType.ATTEMPT_NEW_SAVED 事件。&lt;/p&gt;

&lt;p&gt;RMAppAttemptImpl 收到 RMAppAttemptEventType.ATTEMPT_NEW_SAVED 事件后，
向 ApplicationMasterLauncher 发送 AMLauncherEventType.LAUNCH 事件，
至此，RMAppAttemptImpl 状态从 ALLOCATED_SAVING 转换为 ALLOCATED。&lt;/p&gt;

&lt;p&gt;后面的和这里类似，不过涉及到了 RMContainer状态机，先跳过。&lt;/p&gt;

&lt;h2 id=&#34;3-总结&#34;&gt;3.总结&lt;/h2&gt;

&lt;p&gt;通过这个实例我们大概了解了yarn中的RPC、调度器、服务、状态机配合的过程。
一般是客户端（可以使用户的client、nodeManager进程或者它启动的container进程）发送请求，中间通过RPC调用了ResourceManager中的某个服务，这个服务会触发一定的事件，并且返回。&lt;/p&gt;

&lt;p&gt;例如客户端提交一个应用程序，首先有个 appid，每个appid对应的有一个 RMApp ，放在 rmAppManager 的一个map中。这个 RMApp 是一个状态机。&lt;/p&gt;

&lt;p&gt;然后会调用 this.rmContext.getDispatcher().getEventHandler().handle(new RMAppEvent(applicationId, RMAppEventType.START));&lt;/p&gt;

&lt;p&gt;调度器会启动对应的 EventHandle 去处理这个事件，而 对应的 EventHandle 会根据appid 通过 rmAppManager 得到对应的 RMApp，调用对应的状态转化函数就实现了状态转化。&lt;/p&gt;

&lt;p&gt;再例如某个container启动 APPMaster，也是调用
this.rmContext.getDispatcher().getEventHandler().handle
(new RMAppAttemptRegistrationEvent(applicationAttemptId, request.getHost(), request.getRpcPort(), request.getTrackingUrl()));&lt;/p&gt;

&lt;p&gt;然后调度器会启动对应的 EventHandle 去处理这个事件，而 对应的 EventHandle 会根据appid 通过 rmAppManager 得到对应的 RMApp，
这时候事件类似是 RMAppAttemptEvent，处理逻辑变了，会在另一个状态机进行操作。&lt;/p&gt;

&lt;h2 id=&#34;4-rmcontainer状态机&#34;&gt;4.RMContainer状态机&lt;/h2&gt;

&lt;p&gt;上面分析了 两个状态机，实际上还有一个 RMContainer ，这个和上面两个类似吧。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>yarn-基础库</title>
      <link>https://dengziming.github.io/post/hadoop/yarn-%E5%9F%BA%E7%A1%80%E5%BA%93/</link>
      <pubDate>Tue, 22 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/hadoop/yarn-%E5%9F%BA%E7%A1%80%E5%BA%93/</guid>
      
        <description>

&lt;h1 id=&#34;yarn-事件库和服务库&#34;&gt;yarn-事件库和服务库&lt;/h1&gt;

&lt;h2 id=&#34;使用&#34;&gt;使用&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;新建Event和EventType&lt;/li&gt;
&lt;li&gt;新建 AsyncDispatcher 并给 AsyncDispatcher 注册 Event 和对应的 EventHandler&lt;Event&gt;&lt;/li&gt;
&lt;li&gt;调用 AsyncDispatcher 的 getEventHandler 得到 EventHandler 然后调用 handler 的 handle 方法处理 Event&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;基本原理&#34;&gt;基本原理：&lt;/h2&gt;

&lt;p&gt;AsyncDispatcher 注册 EventHandler&lt;Event&gt; 的过程实际上生成了一个 map，保存了每个事件对应的handler。同时有一个 队列，用于放置 Event&lt;/p&gt;

&lt;p&gt;调用 handle 的时候 将Event放进queue中，内部启动一个线程不断处理 queue的任务。&lt;/p&gt;

&lt;h1 id=&#34;yarn-状态机&#34;&gt;yarn-状态机&lt;/h1&gt;

&lt;h2 id=&#34;使用-1&#34;&gt;使用&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;初始化&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;StateMachineFactory
.addTransition(JobStateInternal.NEW, JobStateInternal.INITED, JobEventType.JOB_INIT,new InitTransition())
.addTransition(JobStateInternal.INITED, JobStateInternal.SETUP, JobEventType.JOB_START,new StartTransition())
.installTopology()
.make()
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;新建对应的 Transition&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public static class InitTransition implements SingleArcTransition&amp;lt;JobStateMachine,JobEvent&amp;gt;{

        @Override
        public void transition(JobStateMachine job, JobEvent event) {
            System.out.println(&amp;quot;Receiving event &amp;quot; + event);
        }

    }
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;调用 StateMachine 的 doTransition(event.getType(), event)&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;原理&#34;&gt;原理&lt;/h2&gt;

&lt;p&gt;installTopology的时候创建一个拓扑图，记录每个 State 能接受的 Event，以及接受该 Event 后的操作，以及操作后的 State。&lt;/p&gt;

&lt;p&gt;每次有Event传入，调用对应的 Transition ，并且将 此时刻 的状态变为 操作后的状态。&lt;/p&gt;
</description>
      
    </item>
    
  </channel>
</rss>