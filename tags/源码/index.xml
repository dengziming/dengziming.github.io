<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>源码 on 数据分析师之旅</title>
    <link>https://dengziming.github.io/tags/%E6%BA%90%E7%A0%81/</link>
    <description>Recent content in 源码 on 数据分析师之旅</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Thu, 18 Apr 2019 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://dengziming.github.io/tags/%E6%BA%90%E7%A0%81/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>kafka-producer</title>
      <link>https://dengziming.github.io/post/kafka/kafka-producer/</link>
      <pubDate>Thu, 18 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/kafka/kafka-producer/</guid>
      
        <description>

&lt;h2 id=&#34;构造函数&#34;&gt;构造函数&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;private KafkaProducer(ProducerConfig config, Serializer&amp;lt;K&amp;gt; keySerializer, Serializer&amp;lt;V&amp;gt; valueSerializer) {
    try {
        //省略一段
        
        this.metrics = new Metrics(metricConfig, reporters, time);
        // 构造各个组件
        this.partitioner = config.getConfiguredInstance(ProducerConfig.PARTITIONER_CLASS_CONFIG, Partitioner.class);
        long retryBackoffMs = config.getLong(ProducerConfig.RETRY_BACKOFF_MS_CONFIG);
        
        this.metadata = new Metadata(retryBackoffMs, config.getLong(ProducerConfig.METADATA_MAX_AGE_CONFIG));
        this.maxRequestSize = config.getInt(ProducerConfig.MAX_REQUEST_SIZE_CONFIG);
        this.totalMemorySize = config.getLong(ProducerConfig.BUFFER_MEMORY_CONFIG);
        
        this.compressionType = CompressionType.forName(config.getString(ProducerConfig.COMPRESSION_TYPE_CONFIG));
        
        
        this.accumulator = new RecordAccumulator(config.getInt(ProducerConfig.BATCH_SIZE_CONFIG),
                this.totalMemorySize,
                this.compressionType,
                config.getLong(ProducerConfig.LINGER_MS_CONFIG),
                retryBackoffMs,
                metrics,
                time);
        List&amp;lt;InetSocketAddress&amp;gt; addresses = ClientUtils.parseAndValidateAddresses(config.getList(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG));
        this.metadata.update(Cluster.bootstrap(addresses), time.milliseconds());
        ChannelBuilder channelBuilder = ClientUtils.createChannelBuilder(config.values());
        
        NetworkClient client = new NetworkClient(
                new Selector(config.getLong(ProducerConfig.CONNECTIONS_MAX_IDLE_MS_CONFIG), this.metrics, time, &amp;quot;producer&amp;quot;, channelBuilder),
                this.metadata,
                clientId,
                config.getInt(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION),
                config.getLong(ProducerConfig.RECONNECT_BACKOFF_MS_CONFIG),
                config.getInt(ProducerConfig.SEND_BUFFER_CONFIG),
                config.getInt(ProducerConfig.RECEIVE_BUFFER_CONFIG),
                this.requestTimeoutMs, time);
        this.sender = new Sender(client,
                this.metadata,
                this.accumulator,
                config.getInt(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION) == 1,
                config.getInt(ProducerConfig.MAX_REQUEST_SIZE_CONFIG),
                (short) parseAcks(config.getString(ProducerConfig.ACKS_CONFIG)),
                config.getInt(ProducerConfig.RETRIES_CONFIG),
                this.metrics,
                new SystemTime(),
                clientId,
                this.requestTimeoutMs);
        String ioThreadName = &amp;quot;kafka-producer-network-thread&amp;quot; + (clientId.length() &amp;gt; 0 ? &amp;quot; | &amp;quot; + clientId : &amp;quot;&amp;quot;);
        
        // 创建启动现场
        this.ioThread = new KafkaThread(ioThreadName, this.sender, true);
        this.ioThread.start();

        this.errors = this.metrics.sensor(&amp;quot;errors&amp;quot;);
        // ....
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;send-方法&#34;&gt;send 方法&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;@Override
public Future&amp;lt;RecordMetadata&amp;gt; send(ProducerRecord&amp;lt;K, V&amp;gt; record, Callback callback) {
    // intercept the record, which can be potentially modified; this method does not throw exceptions
    ProducerRecord&amp;lt;K, V&amp;gt; interceptedRecord = this.interceptors == null ? record : this.interceptors.onSend(record);
    return doSend(interceptedRecord, callback);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;doSend 方法&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;private Future&amp;lt;RecordMetadata&amp;gt; doSend(ProducerRecord&amp;lt;K, V&amp;gt; record, Callback callback) {
    TopicPartition tp = null;
    try {
        // first make sure the metadata for the topic is available
        long waitedOnMetadataMs = waitOnMetadata(record.topic(), this.maxBlockTimeMs);
        long remainingWaitMs = Math.max(0, this.maxBlockTimeMs - waitedOnMetadataMs);
        byte[] serializedKey;
        try {
            serializedKey = keySerializer.serialize(record.topic(), record.key());
        } catch (ClassCastException cce) {
            throw new SerializationException(&amp;quot;Can&#39;t convert key of class &amp;quot; + record.key().getClass().getName() +
                    &amp;quot; to class &amp;quot; + producerConfig.getClass(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).getName() +
                    &amp;quot; specified in key.serializer&amp;quot;);
        }
        byte[] serializedValue;
        try {
            serializedValue = valueSerializer.serialize(record.topic(), record.value());
        } catch (ClassCastException cce) {
            throw new SerializationException(&amp;quot;Can&#39;t convert value of class &amp;quot; + record.value().getClass().getName() +
                    &amp;quot; to class &amp;quot; + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() +
                    &amp;quot; specified in value.serializer&amp;quot;);
        }
        int partition = partition(record, serializedKey, serializedValue, metadata.fetch());
        int serializedSize = Records.LOG_OVERHEAD + Record.recordSize(serializedKey, serializedValue);
        ensureValidRecordSize(serializedSize);
        tp = new TopicPartition(record.topic(), partition);
        long timestamp = record.timestamp() == null ? time.milliseconds() : record.timestamp();
        log.trace(&amp;quot;Sending record {} with callback {} to topic {} partition {}&amp;quot;, record, callback, record.topic(), partition);
        // producer callback will make sure to call both &#39;callback&#39; and interceptor callback
        Callback interceptCallback = this.interceptors == null ? callback : new InterceptorCallback&amp;lt;&amp;gt;(callback, this.interceptors, tp);
        RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey, serializedValue, interceptCallback, remainingWaitMs);
        if (result.batchIsFull || result.newBatchCreated) {
            log.trace(&amp;quot;Waking up the sender since topic {} partition {} is either full or getting a new batch&amp;quot;, record.topic(), partition);
            this.sender.wakeup();
        }
        return result.future;
        // handling exceptions and record the errors;
        // for API exceptions return them in the future,
        // for other exceptions throw directly
    } catch (Exception e) {
        // ....
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;关键过程如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;调用ProducerInterceptors.onSend()方法，通过ProducerInterceptor对消息进行拦截或修改。&lt;/li&gt;
&lt;li&gt;调用waitOnMetadata()方法获取Kafka集群的信息，底层会唤醒Send线程更新Metadata中保存的Kafka集群元数据。&lt;/li&gt;
&lt;li&gt;调用Serializer.serialize()方法序列化消息的key和value。&lt;/li&gt;
&lt;li&gt;调用partition()为消息选择合适的分区。&lt;/li&gt;
&lt;li&gt;调用RecordAccumulator.append()方法，将消息追加到RecordAccumulator中。&lt;/li&gt;
&lt;li&gt;唤醒Sender线程，由Sender线程将RecordAccumulator中缓存的消息发送出去。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;producerinterceptors-producerinterceptor&#34;&gt;ProducerInterceptors＆ProducerInterceptor&lt;/h3&gt;

&lt;p&gt;面向切面编程的方法，重写 onSend onAcknowledgement onSendError 方法，添加逻辑。&lt;/p&gt;

&lt;p&gt;ProducerInterceptors 则是类似组合模式。&lt;/p&gt;

&lt;h3 id=&#34;metadata&#34;&gt;Metadata&lt;/h3&gt;
</description>
      
    </item>
    
    <item>
      <title>janus官方实例调试解析</title>
      <link>https://dengziming.github.io/post/titan/janus%E6%B5%8B%E8%AF%95%E8%B0%83%E8%AF%95/</link>
      <pubDate>Fri, 26 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/titan/janus%E6%B5%8B%E8%AF%95%E8%B0%83%E8%AF%95/</guid>
      
        <description>

&lt;h2 id=&#34;开始&#34;&gt;开始&lt;/h2&gt;

&lt;p&gt;打好断点。主要类：&lt;/p&gt;

&lt;p&gt;JanusGraphFactory.build() 建造者模式。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;
// new GraphDatabaseConfiguration

// 创建两个 conf 对象
BasicConfiguration localBasicConfiguration = new BasicConfiguration(ROOT_NS,localConfig, BasicConfiguration.Restriction.NONE);
ModifiableConfiguration overwrite = new ModifiableConfiguration(ROOT_NS,new CommonsConfiguration(), BasicConfiguration.Restriction.NONE);

// get storeManager，根据配置反射生成。
final KeyColumnValueStoreManager storeManager = Backend.getStorageManager(localBasicConfiguration);

// conf ，需要连接数据库获得配置。
KCVSConfiguration keyColumnValueStoreConfiguration=Backend.getStandaloneGlobalConfiguration(storeManager,localBasicConfiguration);

// 后面是连接数据库进行读写和默认设置
preLoadConfiguration()

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Builder.open() 新建 StandardJanusGraph,首先是静态代码和成员变量&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;
// TraversalStrategies 优化策略， 需要结合 tinkerpop 的代码才能理解
static {
        TraversalStrategies graphStrategies = TraversalStrategies.GlobalCache.getStrategies(Graph.class).clone()
                .addStrategies(AdjacentVertexFilterOptimizerStrategy.instance(), JanusGraphLocalQueryOptimizerStrategy.instance(), JanusGraphStepStrategy.instance());

        //Register with cache
        TraversalStrategies.GlobalCache.registerStrategies(StandardJanusGraph.class, graphStrategies);
        TraversalStrategies.GlobalCache.registerStrategies(StandardJanusGraphTx.class, graphStrategies);
    }
    
&amp;lt;clinit&amp;gt;:641, StandardJanusGraph (org.janusgraph.graphdb.database)

// Predicate 用来判断新增的 Relation 是 schema 还是 data，可以看出 BaseRelationType 是属于 schema ，而且第一个顶点是 JanusGraphSchemaVertex
private static final Predicate&amp;lt;InternalRelation&amp;gt; SCHEMA_FILTER =
        internalRelation -&amp;gt; internalRelation.getType() instanceof BaseRelationType &amp;amp;&amp;amp; internalRelation.getVertex(0) instanceof JanusGraphSchemaVertex;
    
// NO_SCHEMA_FILTER NO_FILTER 

private final SchemaCache.StoreRetrieval typeCacheRetrieval = new SchemaCache.StoreRetrieval() {。。。}

this.backend = configuration.getBackend();

// 序列化
this.serializer = config.getSerializer();
StoreFeatures storeFeatures = backend.getStoreFeatures();
this.indexSerializer = new IndexSerializer(configuration.getConfiguration(), this.serializer,
        this.backend.getIndexInformation(), storeFeatures.isDistributed() &amp;amp;&amp;amp; storeFeatures.isKeyOrdered());
this.edgeSerializer = new EdgeSerializer(this.serializer);
this.vertexExistenceQuery = edgeSerializer.getQuery(BaseKey.VertexExists, Direction.OUT, new EdgeSerializer.TypedInterval[0]).setLimit(1);
this.queryCache = new RelationQueryCache(this.edgeSerializer);
this.schemaCache = configuration.getTypeCache(typeCacheRetrieval);
 
// management 日志管理
Log managementLog = backend.getSystemMgmtLog();
// registerReader 后，就会不断的读取日志。
managementLogger = new ManagementLogger(this, managementLog, schemaCache, this.times);
managementLog.registerReader(ReadMarker.fromNow(), managementLogger);


&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Backend, 协调和配置所有后端系统&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;
private final ConcurrentHashMap&amp;lt;String, Locker&amp;gt; lockers = new ConcurrentHashMap&amp;lt;&amp;gt;();

// 并发锁创建
CONSISTENT_KEY_LOCKER_CREATOR = .....// lockerStore = storeManager.openDatabase(lockerName);// 

ASTYANAX_RECIPE_LOCKER_CREATOR = 。。

indexes = getIndexes(configuration);

managementLogManager = getKCVSLogManager(MANAGEMENT_LOG);
txLogManager = getKCVSLogManager(TRANSACTION_LOG);
userLogManager = getLogManager(USER_LOG);

KeyColumnValueStore idStore = storeManager.openDatabase(config.get(IDS_STORE_NAME));
idAuthority = new ConsistentKeyIDAuthority(idStore, storeManager, config);
KeyColumnValueStore edgeStoreRaw = storeManagerLocking.openDatabase(EDGESTORE_NAME);
KeyColumnValueStore indexStoreRaw = storeManagerLocking.openDatabase(INDEXSTORE_NAME);

txLogManager.openLog(SYSTEM_TX_LOG_NAME);

txLogStore = new NoKCVSCache(storeManager.openDatabase(SYSTEM_TX_LOG_NAME));

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;JanusGraphManagement management = graph.openManagement();&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;skip
&lt;/code&gt;&lt;/pre&gt;
</description>
      
    </item>
    
    <item>
      <title>tinkerpop 的 step</title>
      <link>https://dengziming.github.io/post/tinkerpop/tinkerpop%E7%9A%84step/</link>
      <pubDate>Fri, 27 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/tinkerpop/tinkerpop%E7%9A%84step/</guid>
      
        <description>

&lt;h2 id=&#34;一-简单调试&#34;&gt;一、简单调试&lt;/h2&gt;

&lt;p&gt;api地址： &lt;a href=&#34;http://tinkerpop.apache.org/javadocs/current/full/&#34;&gt;http://tinkerpop.apache.org/javadocs/current/full/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;第一步：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;JanusGraph graph = JanusGraphFactory.open(&amp;quot;janusgraph-dist/src/assembly/cfilter/conf/janusgraph-berkeleyje-es.properties&amp;quot;);

GraphTraversalSource g = graph.traversal();

g.V().has(&amp;quot;name&amp;quot;, &amp;quot;saturn&amp;quot;).path();

List&amp;lt;Path&amp;gt; paths = path.toList();

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一步一步看整个调用过程：&lt;/p&gt;

&lt;p&gt;进入: fill:179, Traversal (org.apache.tinkerpop.gremlin.process.traversal)&lt;/p&gt;

&lt;p&gt;fill 方法的代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;final Step&amp;lt;?, E&amp;gt; endStep = this.asAdmin().getEndStep();
while (true) {
    final Traverser&amp;lt;E&amp;gt; traverser = endStep.next();
    TraversalHelper.addToCollection(collection, traverser.get(), traverser.bulk());
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;asAdmin 得到 endStep，有点类似 spark 的 stage 拆分后得到 shuffleMapTask。然后调用 endStep.next() 得到 traverser。&lt;/p&gt;

&lt;p&gt;这里的代码我们前面已经熟悉过了，再看一下。进入： next:128, AbstractStep (org.apache.tinkerpop.gremlin.process.traversal.step.util)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;final Traverser.Admin&amp;lt;E&amp;gt; traverser = this.processNextStart();
if (null != traverser.get() &amp;amp;&amp;amp; 0 != traverser.bulk())
    return this.prepareTraversalForNextStep(traverser);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;进入 processNextStart:118, PathStep (org.apache.tinkerpop.gremlin.process.traversal.step.map)
&lt;code&gt;return PathProcessor.processTraverserPathLabels(super.processNextStart(), this.keepLabels);&lt;/code&gt;
可以看出调用了父类的 processNextStart 方法，&lt;/p&gt;

&lt;p&gt;进入 processNextStart:36, MapStep (org.apache.tinkerpop.gremlin.process.traversal.step.map)&lt;/p&gt;

&lt;p&gt;由于是 mapStep，所以类似 spark 的 mapPartitionsRdd ，逻辑就是得到前面的 rdd，然后执行 map 方法的逻辑。
所以这里 mapStep 也是一样，得到  starts 的 next，然后调用map。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;final Traverser.Admin&amp;lt;S&amp;gt; traverser = this.starts.next();
return traverser.split(this.map(traverser), this);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;进入 next:50, ExpandableStepIterator (org.apache.tinkerpop.gremlin.process.traversal.step.util)，我们说过这就是对 hostStep 的一个封装。主要就是&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;if (this.hostStep.getPreviousStep().hasNext())
   return this.hostStep.getPreviousStep().next();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个 hostStep 就是上面的 mapStep。这里有 getPreviousStep 然后 next。&lt;/p&gt;

&lt;p&gt;然后又进入到了 processNextStart:142, GraphStep (org.apache.tinkerpop.gremlin.process.traversal.step.map)，
这里的 iteratorSupplier 变量其实是在 GraphStep 或者他的子类中赋值的，所以 get 方法得到的就是：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public JanusGraphStep(final GraphStep&amp;lt;S, E&amp;gt; originalStep) {
    super(originalStep.getTraversal(), originalStep.getReturnClass(), originalStep.isStartStep(), originalStep.getIds());
    originalStep.getLabels().forEach(this::addLabel);
    this.setIteratorSupplier(() -&amp;gt; {
        if (this.ids == null) {
            return Collections.emptyIterator();
        }
        else if (this.ids.length &amp;gt; 0) {
            final Graph graph = (Graph)traversal.asAdmin().getGraph().get();
            return iteratorList((Iterator)graph.vertices(this.ids));
        }
        if (hasLocalContainers.isEmpty()) {
            hasLocalContainers.put(new ArrayList&amp;lt;&amp;gt;(), new QueryInfo(new ArrayList&amp;lt;&amp;gt;(), 0, BaseQuery.NO_LIMIT));
        }
        final JanusGraphTransaction tx = JanusGraphTraversalUtil.getTx(traversal);
        final GraphCentricQuery globalQuery = buildGlobalGraphCentricQuery(tx);

        final Multimap&amp;lt;Integer, GraphCentricQuery&amp;gt; queries = ArrayListMultimap.create();
        if (globalQuery != null &amp;amp;&amp;amp; !globalQuery.getSubQuery(0).getBackendQuery().isEmpty()) {
            queries.put(0, globalQuery);
        } else {
            hasLocalContainers.entrySet().forEach(c -&amp;gt; queries.put(c.getValue().getLowLimit(), buildGraphCentricQuery(tx, c)));
        }

        final GraphCentricQueryBuilder builder = (GraphCentricQueryBuilder) tx.query();
        final List&amp;lt;Iterator&amp;lt;E&amp;gt;&amp;gt; responses = new ArrayList&amp;lt;&amp;gt;();
        queries.entries().forEach(q -&amp;gt;  executeGraphCentryQuery(builder, responses, q));

        return new MultiDistinctOrderedIterator&amp;lt;E&amp;gt;(lowLimit, highLimit, responses, orders);
    });
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从这段代码，结合前面我们分析过的 GraphStep ，我们看出和图相关的 GraphStep 主要就是有一个 iteratorSupplier。因为这个step 就是为了从图拿数据。&lt;/p&gt;

&lt;p&gt;我们再看看别的 Step。&lt;/p&gt;

&lt;h2 id=&#34;简单-step-查看&#34;&gt;简单 Step 查看&lt;/h2&gt;

&lt;p&gt;其实我们查看 Step 主要就是了解 processNextStart 的行为，接下来先看几个简单的。&lt;/p&gt;

&lt;p&gt;简单的 step 一般只处理一个逻辑，类似 spark 中的 map flatMap filter 等方法。&lt;/p&gt;

&lt;h3 id=&#34;mapstep&#34;&gt;MapStep&lt;/h3&gt;

&lt;p&gt;MapStep 是抽象类，表示这个Step有很多实现，需要自己继承。processNextStart 方法就是调用 starts 的next 返回一个Traverser，然后调用 map(返回一个Traverser);&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;protected Traverser.Admin&amp;lt;E&amp;gt; processNextStart() {
    final Traverser.Admin&amp;lt;S&amp;gt; traverser = this.starts.next();
    return traverser.split(this.map(traverser), this);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;MapStep 有很多的实现类，例如：PropertyKeyStep LabelStep PropertyValueStep PathStep MathStep EdgeOtherVertexStep 等，他们的 map 方法实现很简单。&lt;/p&gt;

&lt;h3 id=&#34;filterstep&#34;&gt;FilterStep&lt;/h3&gt;

&lt;p&gt;和 MapStep 类似，它的子类有 WhereStep HasStep NotStep CoinStep IsStep 等。&lt;/p&gt;

&lt;h3 id=&#34;flatmapstep&#34;&gt;FlatMapStep&lt;/h3&gt;

&lt;p&gt;和 MapStep 类似，它的子类有 EdgeVertexStep VertexStep PropertiesStep 等。&lt;/p&gt;

&lt;h3 id=&#34;aggregatestep&#34;&gt;AggregateStep&lt;/h3&gt;

&lt;p&gt;听名字是聚合的意思，应该是多个结果合并。内部有个 TraverserSet&lt;S&gt; barrier 代表所有待合并的 Traverser。&lt;/p&gt;

&lt;h3 id=&#34;groupstep&#34;&gt;GroupStep&lt;/h3&gt;

&lt;p&gt;我们可以写一段代码测试一下：g.V().group().by(T.label).next()&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;this.asAdmin().addStep(new GroupStep&amp;lt;&amp;gt;(this.asAdmin()));&lt;/li&gt;
&lt;li&gt;this.asAdmin().getEndStep()).modulateBy(token);&lt;/li&gt;
&lt;li&gt;1. new TokenTraversal(token)&lt;/li&gt;
&lt;li&gt;2. GroupStep.modulateBy(final Traversal.Admin&amp;lt;?, ?&amp;gt; kvTraversal)&lt;/li&gt;
&lt;li&gt;1. this.seed = this.reducingBiOperator.apply(this.seed, this.projectTraverser(this.starts.next()));&lt;/li&gt;
&lt;li&gt;2. GroupStep.doFinalReduction((Map&lt;K, Object&gt;) object, this.valueTraversal);&lt;/li&gt;
&lt;/ol&gt;
</description>
      
    </item>
    
    <item>
      <title>janusgraph源码分析7-关系存储</title>
      <link>https://dengziming.github.io/post/titan/janusgraph%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%907-%E5%85%B3%E7%B3%BB%E5%AD%98%E5%82%A8/</link>
      <pubDate>Mon, 09 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/titan/janusgraph%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%907-%E5%85%B3%E7%B3%BB%E5%AD%98%E5%82%A8/</guid>
      
        <description>

&lt;h1 id=&#34;基础类&#34;&gt;基础类&lt;/h1&gt;

&lt;h2 id=&#34;internalrelation-和-internalrelationtype&#34;&gt;InternalRelation 和 InternalRelationType&lt;/h2&gt;

&lt;p&gt;有关类型体系很复杂，可以使用 IDEA 的显示继承体系功能，查看类图。类图比较大，不太好看。大概描述一下：
主要是 JanusGraphElement 作为顶级类，接下来还有一个 InternalElement 作为顶级的 Internal 类。
JanusGraphElement 继承的类主要分为 JanusGraphRelation,JanusGraphVertex 两个分支，前者又分为 JanusGraphVertexProperty, JanusGraphEdge 。
InternalElement 的继承类主要分为 InternalRelation ,InternalVertex 两个分支，前者又分为 JanusGraphVertexProperty, JanusGraphEdge 。
其中 Internal 开头类总是有一个 JanusGraph 开头的类作为父类。例如 InternalRelation 继承自 JanusGraphRelation。&lt;/p&gt;

&lt;p&gt;JanusGraphVertex 比较特殊，他除了有 InternalVertex 子类以外，还有 VertexLabel 和 RelationType 两个子类。
同理 InternalVertex 的继承体系下，除了真正的实体以外，还有一个 JanusGraphSchemaVertex ，他有 VertexLabelVertex, RelationTypeVertex 两个子类，
RelationTypeVertex 又有 EdgeLabelVertex 和 PropertyKeyVertex 两个子类。还有 BaseLabel BaseKey BaseVerteLabel 等子类。&lt;/p&gt;

&lt;p&gt;这里就需要提到我们之前说的，janus 的 schema 也是以顶点的形式保存的，顶级类就是 JanusGraphSchemaVertex ，有 VertexLabelVertex, EdgeLabelVertex 和 PropertyKeyVertex 三个实现。
他们分别代表了 VertexLabel EdgeLabel PropertyKey 的 Vertex，同时我们想想，这些 Vertex 也是 janus 的元素 也是有属性的，我们岂不是还要新建三个类，保存他们的 Property Label 等？
然后他们的 Label 也是有属性的，这样下去就子子孙孙无穷尽也。所以才有了上面的 BaseLabel BaseKey BaseVerteLabel 作为终极的 Vertex。&lt;/p&gt;

&lt;p&gt;然后我们看一下 InternalRelation 和 InternalRelationType 的关系，InternalRelation 代表的就是一种关系，有 JanusGraphEdge 和 JanusGraphVertexProperty 两种，&lt;/p&gt;

&lt;p&gt;例如一个用户的性别是女，也就是给一个顶点添加一个性别 &lt;code&gt;女&lt;/code&gt; 的属性：
首先有两个顶点, a: InternalVertex (JanusGraphVertex)， 性别则是一个 b: PropertyKey (InternalRelationType) 也是一个 Vertex，
而 &lt;code&gt;女&lt;/code&gt; 则是 property 的值，实际上就是在这两个不同类型的 Vertex 之间建立一条连接。再加上一个 value 这三个组合在一起就是一个 JanusGraphVertexProperty 。&lt;/p&gt;

&lt;p&gt;再例如我们要给一个顶点的 VertexLabel 是 User：
首先有一个用户顶点，a: InternalVertex (JanusGraphVertex)，然后 User 也是一个建好的 schema，也就是顶点： VertexLabelVertex 。然后给他们之间建立一条关系，这个关系也是一个顶点 BaseLabel.VertexLabelEdge。&lt;/p&gt;

&lt;p&gt;在比如给两个用户之间添加一个 Friend 的关系。
首先有两个顶点就是用户，然后新建一个 StandardEdge，然后 这两个顶点分别和这个 StandardEdge 建立一个 EdgeLabel 为 Friend 的关系。&lt;/p&gt;

&lt;p&gt;到这里我们大概明白，其实添加 Property 就是和 和一个 PropertyKey 建立一条边，添加 Edge 就是和一个 vertex 建立一条边，添加 VertexLabel 就是和一个 VertexLabel 建立一条边。&lt;/p&gt;

&lt;p&gt;InternalRelation 就是添加的边，可以序列化存储起来，也可以读出来反序列化成 InternalRelation。 InternalRelationType 就是类型，类型也是一个顶点， 而 PropertyKey 这种类型对应的属性都是 Base开头的。&lt;/p&gt;

&lt;p&gt;##&lt;/p&gt;

&lt;h2 id=&#34;relationcache&#34;&gt;RelationCache&lt;/h2&gt;

&lt;h2 id=&#34;staticarrayentry&#34;&gt;StaticArrayEntry&lt;/h2&gt;

&lt;p&gt;类似 java.nio 的 ByteBuffer。&lt;/p&gt;

&lt;h1 id=&#34;edgeserializer&#34;&gt;EdgeSerializer&lt;/h1&gt;

&lt;h2 id=&#34;writerelation&#34;&gt;writeRelation&lt;/h2&gt;

&lt;p&gt;EdgeSerializer 类主要用来写 edgestore 库，这个库序列化方式相对比较简单，但代码还是比较多。&lt;/p&gt;

&lt;p&gt;从代码调用开始看：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;for (Long vertexId : mutations.keySet()) {
       Preconditions.checkArgument(vertexId &amp;gt; 0, &amp;quot;Vertex has no id: %s&amp;quot;, vertexId);
       final List&amp;lt;InternalRelation&amp;gt; edges = mutations.get(vertexId);
       final List&amp;lt;Entry&amp;gt; additions = new ArrayList&amp;lt;&amp;gt;(edges.size());
       final List&amp;lt;Entry&amp;gt; deletions = new ArrayList&amp;lt;&amp;gt;(Math.max(10, edges.size() / 10));
       for (final InternalRelation edge : edges) {
           final InternalRelationType baseType = (InternalRelationType) edge.getType();
           assert baseType.getBaseType()==null;

           for (InternalRelationType type : baseType.getRelationIndexes()) {
               if (type.getStatus()== SchemaStatus.DISABLED) continue;
               for (int pos = 0; pos &amp;lt; edge.getArity(); pos++) {
                   if (!type.isUnidirected(Direction.BOTH) &amp;amp;&amp;amp; !type.isUnidirected(EdgeDirection.fromPosition(pos)))
                       continue; //Directionality is not covered
                   if (edge.getVertex(pos).longId()==vertexId) {
                       StaticArrayEntry entry = edgeSerializer.writeRelation(edge, type, pos, tx);
                       if (edge.isRemoved()) {
                           deletions.add(entry);
                       } else {
                           Preconditions.checkArgument(edge.isNew());
                           int ttl = getTTL(edge);
                           if (ttl &amp;gt; 0) {
                               entry.setMetaData(EntryMetaData.TTL, ttl);
                           }
                           additions.add(entry);
                       }
                   }
               }
           }
       }

       StaticBuffer vertexKey = idManager.getKey(vertexId);
       mutator.mutateEdges(vertexKey, additions, deletions);
   }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这是java类 StandardJanusGraph 写数据 的代码。可以看出写数据之前是需要调用 StaticArrayEntry entry = edgeSerializer.writeRelation(edge, type, pos, tx);
所以接下来我么的任务就是看看这个方法，我们先看看这几个参数的意义：&lt;/p&gt;

&lt;p&gt;InternalRelation relation, 代表一条关系，可以是 edge，也可以是 Property。
如果是edge，edge的两个顶点都会保存这条边，如果是 Property，只会有节点保存，PropertyKey 不会保存。&lt;/p&gt;

&lt;p&gt;InternalRelationType type,  可以是 Property 和 Edge&lt;/p&gt;

&lt;p&gt;int position, 通过调用部分代码，可以看出表示顶点在这个关系中的位置。例如 v1 -[e1]-&amp;gt; v2, 对于e1来讲，v1的pos是0，v2的pos是1。
TypeInspector tx 用来检测类型.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;
public StaticArrayEntry writeRelation(InternalRelation relation, 
										InternalRelationType type, 
										int position,
										TypeInspector tx) 
										
{
    // 判断类型
    assert type==relation.getType() || (type.getBaseType() != null
            &amp;amp;&amp;amp; type.getBaseType().equals(relation.getType()));
    // 得到方向，可以是 只有 OUT 和 IN 两种结果
    Direction dir = EdgeDirection.fromPosition(position);
    
    // isUnidirected 方法是判断是不是这个方向的。
    Preconditions.checkArgument(type.isUnidirected(Direction.BOTH) || type.isUnidirected(dir));
    
    // 得到 type 的id，注意 JanusGraph 中的schema 也是以顶点的形式存储，也有 id。
    long typeId = type.longId();
    
    // 得到 PROPERTY_DIR 或者 EDGE_OUT_DIR 或者 EDGE_IN_DIR
    DirectionID dirID = getDirID(dir, relation.isProperty() ? RelationCategory.PROPERTY : RelationCategory.EDGE);

    // 得到一个输出，实际就是 byte 数组
    DataOutput out = serializer.getDataOutput(DEFAULT_CAPACITY);
    
    // 保存 key 和 value 的临界点
    int valuePosition;
    
    // 这里调用方法写入 typeId dirID isInvisibleType ，详细内容我们后面看 TODO
    IDHandler.writeRelationType(out, typeId, dirID, type.isInvisibleType());
    
    // multiplicity 代表多元性
    Multiplicity multiplicity = type.multiplicity();

    long[] sortKey = type.getSortKey();
    // 多对多关系不允许有排序的key
    assert !multiplicity.isConstrained() || sortKey.length==0: type.name();
    
    int keyStartPos = out.getPosition();
    if (!multiplicity.isConstrained()) { // isConstrained 代表是否有限制。SINGLE 和 SET 有限制，LIST 无限制。
        // 写排序key ，这个方法后面讨论 TODO
        writeInlineTypes(sortKey, relation, out, tx, InlineType.KEY);
    }
    int keyEndPos = out.getPosition();

    long relationId = relation.longId();

    //How multiplicity is handled for edges and properties is slightly different
    if (relation.isEdge()) {  // 如果是边关系
        // 得到另一个顶点的id
        long otherVertexId = relation.getVertex((position + 1) % 2).longId();
        if (multiplicity.isConstrained()) { // 非多对多
            if (multiplicity.isUnique(dir)) { // 只有一个这种类型的边。例如每个 Person只有一个父亲节点。
                valuePosition = out.getPosition(); // 得到 position
                // 写出另一个顶点的id
                VariableLong.writePositive(out, otherVertexId);
            } else { // 可能有多个关系，例如一个Person可能有多个儿子节点，再或者 SIMPLE。
                
                // 这时候从后往前写，这个方法后面讨论  TODO 
                VariableLong.writePositiveBackward(out, otherVertexId);
                valuePosition = out.getPosition();
            }
            // 然后写出 关系的 relationId
            VariableLong.writePositive(out, relationId);
        } else {// 多对多，我们的数据绝大部分都是这种情况
            //从后往前写 vertex 和 relationId。得到position
            VariableLong.writePositiveBackward(out, otherVertexId);
            VariableLong.writePositiveBackward(out, relationId);
            valuePosition = out.getPosition();
        }

/**
总结上面
SortKey是一种特殊的属性，JanusGraph允许在定义Edge Label时指定其中的一个或多个属性为Sort Key。
对于边的Sort Key属性，JanusGraph在存储时会将其存储在Relation Type ID的后面,其他所有字段的前面。
通过这种方式，可以保证一个节点的多条同一个类型的边，会按Sort Key属性排序存储。这对于一个节点有大量边时，对查询性能提升有帮助。

MULTIPLICITY为MULTI时的存储结构： 从后往前写 otherVertexId 和 relationId。其余放在 value 里面

MULTIPLICITY非MULTI且此方向存在多条边时的存储结构：从后往前写 otherVertexId，relationId 和其余放在 value里面

MULTIPLICITY非MULTI且此方向仅有一条边时的存储结构： 不记录relationId，otherVertexId 放在value 里面

我一直在思考这么设计的原因，现在想想明白了，MULTIPLICITY 为 非MULTI 时候，也就是有限制，无论是什么限制，总之两个顶点之间只能有一条该类型的边。
所以 MULTIPLICITY 为 非MULTI 的时候，将 relationId 放在 value 中。这样哪怕你重复新建这条边，只是value 变了，key并没有变，而key是存在 bigtable 的 column 中的， bigtable 有个特性就是可以直接覆盖。
MULTIPLICITY 为 MULTI 的时候，relationId 放在 key 中，每次添加新的边，column 的值不一样，这样就不会覆盖原有的边。

*/

    } else { // 如果是属性关系，得到属性的 key 和 value
        assert relation.isProperty();
        Preconditions.checkArgument(relation.isProperty());
        Object value = ((JanusGraphVertexProperty) relation).value();
        Preconditions.checkNotNull(value);
        PropertyKey key = (PropertyKey) type;
        assert key.dataType().isInstance(value);

        // 没有限制，不是 LIST 类型
        if (multiplicity.isConstrained()) {
            if (multiplicity.isUnique(dir)) { //Cardinality=SINGLE
                // property 放在 value 中
                valuePosition = out.getPosition();
                writePropertyValue(out,key,value);
            } else { //Cardinality=SET
                // property 放在 key 中
                writePropertyValue(out,key,value);
                valuePosition = out.getPosition();
            }
            // 写出 relationId
            VariableLong.writePositive(out, relationId);
        } else {
            assert multiplicity.getCardinality()== Cardinality.LIST;
            // 在key中反向写出 relationId, property 放在 value 中
            VariableLong.writePositiveBackward(out, relationId);
            valuePosition = out.getPosition();
            writePropertyValue(out,key,value);
        }
    }

/** 总结上面

Cardinality为SINGLE时的存储结构

列名只存储Property Key的ID及方向。具体的Property Value值以及Property ID(relationId)，都存放在Cell的Value中。
另外，如果该Property还有额外的 Remaining properties，也会放在Value中。Remaining properties一般不使用，仅在一些特殊场景下，用于为该Property记录更多的附加信息(比如存储元数据Edge Labe的定义等)。

PropertyKeyID 及方向整个结构的详细结构在后文中描述；占用一个或多个字段，具体格式在后文描述;及采用相同的格式，具体格式在后文描述。

Candinality为LIST时的存储结构

各个部分与Cardinality为SINGLE时的结构相似，区别在于属性的ID被放在了列名中，而不是放在Value中。

Candinality为SET存储结构

各个部分与Cardinality为SINGLE时的结构相似，区别在于属性的值被放在了列名中，而不是放在Value中。

*** /


    //Write signature 
    // 得到 relationType 所有的 signature 的 PropertyKeyid，写到 value 中
    long[] signature = type.getSignature();
    writeInlineTypes(signature, relation, out, tx, InlineType.SIGNATURE);

    //Write remaining properties
    // sortKey 和 signature 是已经写过，所以排除掉
    LongSet writtenTypes = new LongHashSet(sortKey.length + signature.length);
    if (sortKey.length &amp;gt; 0 || signature.length &amp;gt; 0) {
        for (long id : sortKey) writtenTypes.add(id);
        for (long id : signature) writtenTypes.add(id);
    }
    LongArrayList remainingTypes = new LongArrayList(8);
    for (PropertyKey t : relation.getPropertyKeysDirect()) {
        if (!(t instanceof ImplicitKey) &amp;amp;&amp;amp; !writtenTypes.contains(t.longId())) {
            remainingTypes.add(t.longId());
        }
    }
    //Sort types before writing to ensure that value is always written the same way
    long[] remaining = remainingTypes.toArray();
    Arrays.sort(remaining);
    for (long tid : remaining) {
        // 剩下的 value 写到值部分。
        PropertyKey t = tx.getExistingPropertyKey(tid);
        writeInline(out, t, relation.getValueDirect(t), InlineType.NORMAL);
    }
    assert valuePosition&amp;gt;0;

    // 返回，返回的时候需要注意根据 type.getSortOrder() 的结果进行判断，如果 DESC 需要将 key 部分反过来
    return new StaticArrayEntry(type.getSortOrder() == Order.DESC ?
                                out.getStaticBufferFlipBytes(keyStartPos, keyEndPos) :
                                out.getStaticBuffer(), valuePosition);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到这里我们基本了解了数据的存储结构，但是细节还是没了解。比如key具体多少位，每一位是啥。接下来我们需要稍微了解一下每次写的时候对应方法的细节。
我们只需要找有变量 out 的代码部分。&lt;/p&gt;

&lt;p&gt;第一次是 &lt;code&gt;DataOutput out = serializer.getDataOutput(DEFAULT_CAPACITY)&lt;/code&gt;, 这个就是创建新的Buffer，然后是 IDHandler 写部分。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/**
 * The edge type is written as follows: [ Invisible &amp;amp;amp; System (2 bit) | Relation-Type-ID (1 bit) | Relation-Type-Count (variable) | Direction-ID (1 bit)]
 * Would only need 1 bit to store relation-type-id, but using two so we can upper bound.
 * 
 * 注释说明，edge格式： Invisible &amp;amp; System  2bit, Relation-Type-ID 1 bit, Relation-Type-Count 变化的,Direction-ID 一位。
 * 这里有个小疑问，Relation-Type-ID 也是一个 long 类型，1bit 应该没法表示。我们在代码中看
 *
 * @param out
 * @param relationTypeId
 * @param dirID
 */
public static void writeRelationType(WriteBuffer out, long relationTypeId, DirectionID dirID, boolean invisible) {
    
    // 断言判断
    assert relationTypeId &amp;gt; 0 &amp;amp;&amp;amp; (relationTypeId &amp;lt;&amp;lt; 1) &amp;gt; 0; //Check positive and no-overflow

    // 去掉 relationTypeId 的 padding，在后面补一位 dirID.getDirectionInt。
    long strippedId = (IDManager.stripEntireRelationTypePadding(relationTypeId) &amp;lt;&amp;lt; 1) + dirID.getDirectionInt();
    {
    // 这个方法就是将 id 的 Padding 部分 去掉。
    public static long stripEntireRelationTypePadding(long id) {
        Preconditions.checkArgument(isProperRelationType(id));
        return VertexIDType.UserEdgeLabel.removePadding(id);
        {
            VertexIDType.UserEdgeLabel.removePadding(id){
                id &amp;gt;&amp;gt;&amp;gt; offset();// 这个 offset() 代表 id 的padding 长度，NormalVertex 是 3，EdgeLabel 是5，UserEdgeLabel 是6
            }
        }
    }
    }
    
    //
    VariableLong.writePositiveWithPrefix(out, strippedId, dirID.getPrefix(invisible, IDManager.isSystemRelationTypeId(relationTypeId)), PREFIX_BIT_LEN);
    {
    // IDManager.isSystemRelationTypeId(relationTypeId)) 判断是否是系统关系
    // getPrefix 方法如下，其实就是得到了 上面所说的数据，
    	private int getPrefix(boolean invisible, boolean systemType) {
    	    assert !systemType || invisible; // systemType implies invisible
    	    return ((systemType?0:invisible?2:1)&amp;lt;&amp;lt;1) + getRelationType();
    	}
    
    // 整个方法就是写下 prefix strippedId 。
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;整个方法大概就清楚了，然后是 writeInlineTypes 和 writeInline ，writePropertyValue ，和上面的方法类似。
然后是 VariableLong.writePositiveBackward(out, otherVertexId); 和 VariableLong.writePositive(out, otherVertexId);&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public static void writePositive(WriteBuffer out, final long value) {
    assert value &amp;gt;= 0;
    writeUnsigned(out, value);
    
    	/** writeUnsigned 方法 */
    	{
    	private static void writeUnsigned(WriteBuffer out, final long value) {
    	    writeUnsigned(out, unsignedBlockBitLength(value), value);
    	    
    	    /** unsignedBlockBitLength 最终是 block 的数量 * 7 */
    	    {
    	    return unsignedNumBlocks(value)*7;
    	    	/** unsignedNumBlocks 求 block 数量 */
    	    	{
    	    	     return numVariableBlocks(unsignedBitLength(value));
    	    	     {
    	    	     // 得到数据去掉所有0 的位数，如果是0有1位。也就是无符号位数
    	    	     unsignedBitLength(value){
    	    	         return (value == 0) ? 1 : Long.SIZE - Long.numberOfLeadingZeros(value);
    	    	     }
    	    	     /** 这个方法返回 位数－1 除以 7 再加一，
    	    	     简单理解 ,就是第一个bit一个 block，剩下每7bit 一个 block
    	    	     实际上是除以七进一。
    	    	      */
    	    	     numVariableBlocks{
    	    	         return (numBits - 1) / 7 + 1;
    	    	     }
    	    	     }
    	    	}
    	    }
    	    
    	    /** writeUnsigned 方法 */
    	    {
    		private static void writeUnsigned(WriteBuffer out, int offset, final long value) {
        		assert offset % 7 == 0;
        		while (offset &amp;gt; 0) { // offset 就是上面求的 block 数量 * 7
        		    offset -= 7; // 一次写 7 位。
        		    
        		    byte b = (byte) ((value &amp;gt;&amp;gt;&amp;gt; offset) &amp;amp; BIT_MASK); // 左移 offset 与 01111111 进行 `逻辑与` 操作。
        		    if (offset == 0) {
        		        b = (byte) (b | STOP_MASK);  // 如果是最后一位，再与 -128(111111...1110000000) 进行或操作,这个操作的结果会得到一个类似补码的数据
        		    }
        		    out.putByte(b);
    		    }
    		}
    	    }
    	}
    }
}
/**
综上所述, 整个写 long 的方法，首先是计算数据的 block 数，每 7 位一个block。
写出的时候，每次写一个 byte(8bit)，其中一个block 7bit，再加一个占位符(0)。最后再与 -128(111111...1110000000) 进行或操作，
例如 72 会变成 -56， 满足 72 - (-56) = 128，这应该是补码还是反码记不清了。
*/

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后是  VariableLong.writePositiveBackward(out, otherVertexId)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;
/**
 * The format used is this:
 * - The first bit indicates whether this is the first block (reading backwards, this would be the stop criterion)
 * - In the first byte, the 3 bits after the first bit indicate the number of bytes written minus 3 (since 3 is
 * the minimum number of bytes written. So, if the 3 bits are 010 = 2 =&amp;gt; 5 bytes written. The value is aligned to
 * the left to ensure that this encoding is byte order preserving.
 *
 *  根据注释，第一 bit 表示是否是第一个 block （往后读需要一个停止标识），紧接着代表数据的位数。
 * 
 * @param out
 * @param value
 */
private static void writeUnsignedBackward(WriteBuffer out, final long value) {
    
    int numBytes = unsignedBackwardLength(value);
    /** unsignedBackwardLength 这个类似上面，得到最少的 bytes 数量。可以看出至少有3 bytes。*/
    {
        int bitLength = unsignedBitLength(value); // 这个上面已经看过。
        assert bitLength &amp;gt; 0 &amp;amp;&amp;amp; bitLength &amp;lt;= 64;
        return Math.max(3, 1 + (bitLength &amp;lt;= 4 ? 0 : (1 + (bitLength - 5) / 7)));
    }
    int prefixLen = numBytes - 3;
    assert prefixLen &amp;gt;= 0 &amp;amp;&amp;amp; prefixLen &amp;lt; 8; //Consumes 3 bits
    //Prepare first byte
    byte b = (byte)((prefixLen &amp;lt;&amp;lt; 4) | 0x80); //stop marker (first bit) and length
    for (int i = numBytes - 1; i &amp;gt;= 0; i--) {
        b = (byte)(b | (0x7F &amp;amp; (value &amp;gt;&amp;gt;&amp;gt; (i * 7)))); // 左移 i*7 位，和 0x7F 进行逻辑与，实际上就是取七位。
        out.putByte(b);
        b = 0;
    }
}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面的对比我们看出 writePositiveBackward 和 writePositive 的差别在于 writePositiveBackward 把停止标识放在了开头，writePositive 放在结尾。&lt;/p&gt;

&lt;p&gt;看完序列化的代码我们可以大概知道存储的格式，我们整理一下。序列化的步骤在 writeRelation 中，首先写出Relation 的方向、可见性、schemaId，然后如果有sortKey写出sortKey的值，
然后判断是Edge 还是Property，根据他们的 multiplicity 处理有所不同。详情上面已经有了。最后还要写出 signature 和剩下的属性。&lt;/p&gt;

&lt;h2 id=&#34;readrelation&#34;&gt;readRelation&lt;/h2&gt;

&lt;p&gt;和 writeRelation 对应的是 readRelation，相关调用如下，主要是 readRelation 方法。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;/**
 * Returns the list of adjacent vertex ids for this query. By reading those ids
 * from the entries directly (without creating objects) we get much better performance.
 *
 * @return
 */
public VertexList vertexIds() {
    LongArrayList list = new LongArrayList();
    long previousId = 0;
    for (Long id : Iterables.transform(this,new Function&amp;lt;Entry, Long&amp;gt;() {
        @Nullable
        @Override
        public Long apply(@Nullable Entry entry) {
            return edgeSerializer.readRelation(entry,true,tx).getOtherVertexId();
        }
    })) {
        list.add(id);
        if (id&amp;gt;=previousId &amp;amp;&amp;amp; previousId&amp;gt;=0) previousId=id;
        else previousId=-1;
    }
    return new VertexLongList(tx,list,previousId&amp;gt;=0);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;进入方法发现核心就是一个 parseRelation 方法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;
@Override
public RelationCache parseRelation(Entry data, boolean excludeProperties, TypeInspector tx) {
    ReadBuffer in = data.asReadBuffer();

    LongObjectHashMap properties = excludeProperties ? null : new LongObjectHashMap(4);
    
    // 第一步，读取关系类型。就是上面的写进去的 三位prefix+typeId ，包括方向，类型，可见性
    RelationTypeParse typeAndDir = IDHandler.readRelationType(in);

    long typeId = typeAndDir.typeId;
    Direction dir = typeAndDir.dirID.getDirection();

   // 根据id 查询对应的类型
    RelationType relationType = tx.getExistingRelationType(typeId);
    InternalRelationType def = (InternalRelationType) relationType;
    Multiplicity multiplicity = def.multiplicity();
    long[] keySignature = def.getSortKey();

    long relationId;
    Object other;
    int startKeyPos = in.getPosition();
    int endKeyPos = 0;
    
    // 这里和前面写的对应， 分别读取
    if (relationType.isEdgeLabel()) {
        long otherVertexId;
        if (multiplicity.isConstrained()) {
            if (multiplicity.isUnique(dir)) { 
                otherVertexId = VariableLong.readPositive(in);
            } else {
                in.movePositionTo(data.getValuePosition());
                otherVertexId = VariableLong.readPositiveBackward(in);
                in.movePositionTo(data.getValuePosition());
            }
            relationId = VariableLong.readPositive(in);
        } else {
            in.movePositionTo(data.getValuePosition());

            relationId = VariableLong.readPositiveBackward(in);
            otherVertexId = VariableLong.readPositiveBackward(in);
            endKeyPos = in.getPosition();
            in.movePositionTo(data.getValuePosition());
        }
        other = otherVertexId;
    } else {
        assert relationType.isPropertyKey();
        PropertyKey key = (PropertyKey) relationType;

        if (multiplicity.isConstrained()) {
            other = readPropertyValue(in,key);
            relationId = VariableLong.readPositive(in);
        } else {
            in.movePositionTo(data.getValuePosition());
            relationId = VariableLong.readPositiveBackward(in);
            endKeyPos = in.getPosition();
            in.movePositionTo(data.getValuePosition());
            other = readPropertyValue(in,key);
        }
        Preconditions.checkState(other!=null,
            &amp;quot;Encountered error in deserializer [null value returned]. Check serializer compatibility.&amp;quot;);
    }
    assert other!=null;

    // 
    if (!excludeProperties &amp;amp;&amp;amp; !multiplicity.isConstrained() &amp;amp;&amp;amp; keySignature.length&amp;gt;0) {
        int currentPos = in.getPosition();
        //Read sort key which only exists if type is not unique in this direction
        assert endKeyPos&amp;gt;startKeyPos;
        int keyLength = endKeyPos-startKeyPos; //after reading the ids, we are on the last byte of the key
        in.movePositionTo(startKeyPos);
        ReadBuffer inKey = in;
        if (def.getSortOrder()== Order.DESC) inKey = in.subrange(keyLength,true);
        readInlineTypes(keySignature, properties, inKey, tx, InlineType.KEY);
        in.movePositionTo(currentPos);
    }

    if (!excludeProperties) {
        //read value signature
        readInlineTypes(def.getSignature(), properties, in, tx, InlineType.SIGNATURE);

        //Third: read rest
        while (in.hasRemaining()) {
            PropertyKey type = tx.getExistingPropertyKey(IDHandler.readInlineRelationType(in));
            Object propertyValue = readInline(in, type, InlineType.NORMAL);
            assert propertyValue != null;
            properties.put(type.longId(), propertyValue);
        }

        if (data.hasMetaData()) {
            for (Map.Entry&amp;lt;EntryMetaData,Object&amp;gt; metas : data.getMetaData().entrySet()) {
                ImplicitKey key = ImplicitKey.MetaData2ImplicitKey.get(metas.getKey());
                if (key!=null) {
                    assert metas.getValue()!=null;
                    properties.put(key.longId(),metas.getValue());
                }
            }
        }
    }

    return new RelationCache(dir, typeId, relationId, other, properties);
}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以看出，如果你熟悉上面的readRelation，就是 反过来读一遍。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>janusgraph源码分析8-底层交互</title>
      <link>https://dengziming.github.io/post/titan/janusgraph%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%909-%E8%B4%A1%E7%8C%AE%E5%AF%BC%E6%95%B0%E6%8D%AE%E6%BA%90%E7%A0%81/</link>
      <pubDate>Mon, 09 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/titan/janusgraph%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%909-%E8%B4%A1%E7%8C%AE%E5%AF%BC%E6%95%B0%E6%8D%AE%E6%BA%90%E7%A0%81/</guid>
      
        <description>

&lt;h1 id=&#34;反向分析&#34;&gt;反向分析&lt;/h1&gt;

&lt;h2 id=&#34;cassandra-写数据-api&#34;&gt;cassandra 写数据 API&lt;/h2&gt;

&lt;p&gt;cassandra 的结构类似 bigtable ，数据实际上是多层嵌套的 map，第一个 key 是 rowkey，第二层key 是 columnFamily，第三层key 是 column，第四层(也可以忽略) 是 timestamp，然后是 value。&lt;/p&gt;

&lt;p&gt;写数据的 API 如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt; CTConnection conn = null;
 try {
     conn = pool.borrowObject(keySpaceName);
     Cassandra.Client client = conn.getClient();
     if (atomicBatch) {
         client.atomic_batch_mutate(batch, consistency);
     } else {
         client.batch_mutate(batch, consistency);
     }
 } catch (Exception ex) {
     throw CassandraThriftKeyColumnValueStore.convertException(ex);
 } finally {
     pool.returnObjectUnsafe(keySpaceName, conn);
 }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里的 batch 就是一个多层嵌套的map。&lt;code&gt;final Map&amp;lt;ByteBuffer, Map&amp;lt;String, List&amp;lt;org.apache.cassandra.thrift.Mutation&amp;gt;&amp;gt;&amp;gt; batch = new HashMap&amp;lt;&amp;gt;(size);&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;这里看起来只有两层，第一层的 ByteBuffer 当然是 rowKey，第二层是 String 是 columnFamily。而 &lt;code&gt;List&amp;lt;org.apache.cassandra.thrift.Mutation&amp;gt;&lt;/code&gt; 很明显就是添加或者删除的 key:value。&lt;/p&gt;

&lt;h2 id=&#34;写入-cassandra-的数据格式&#34;&gt;写入 cassandra 的数据格式&lt;/h2&gt;

&lt;p&gt;上面是写 cassandra 的 API，而最终调用这段代码的位置在 &lt;code&gt;CassandraThriftStoreManager.mutateMany(Map&amp;lt;String, Map&amp;lt;StaticBuffer, KCVMutation&amp;gt;&amp;gt; mutations, StoreTransaction txh)&lt;/code&gt; 方法。&lt;/p&gt;

&lt;p&gt;我们需要了解的就是  &lt;code&gt;Map&amp;lt;String, Map&amp;lt;StaticBuffer, KCVMutation&amp;gt;&amp;gt; mutations&lt;/code&gt; 和 &lt;code&gt;Map&amp;lt;ByteBuffer, Map&amp;lt;String, List&amp;lt;org.apache.cassandra.thrift.Mutation&amp;gt;&amp;gt;&amp;gt; batch&lt;/code&gt; 的对应关系。&lt;/p&gt;

&lt;p&gt;从代码可以看出：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;final Map&amp;lt;ByteBuffer, Map&amp;lt;String, List&amp;lt;org.apache.cassandra.thrift.Mutation&amp;gt;&amp;gt;&amp;gt; batch = new HashMap&amp;lt;&amp;gt;(size);

for (final Map.Entry&amp;lt;String, Map&amp;lt;StaticBuffer, KCVMutation&amp;gt;&amp;gt; keyMutation : mutations.entrySet()) {
    
    // mutations 的 key 是 columnFamily
    final String columnFamily = keyMutation.getKey(); 
    
    for (final Map.Entry&amp;lt;StaticBuffer, KCVMutation&amp;gt; mutEntry : keyMutation.getValue().entrySet()) {
        
        // mutations 的第二层 key 是 rowKey
        ByteBuffer keyBB = mutEntry.getKey().asByteBuffer();

        // Get or create the single Cassandra Mutation object responsible for this key
        // Most mutations only modify the edgeStore and indexStore
        
        final Map&amp;lt;String, List&amp;lt;org.apache.cassandra.thrift.Mutation&amp;gt;&amp;gt; cfmutation
            = batch.computeIfAbsent(keyBB, k -&amp;gt; new HashMap&amp;lt;&amp;gt;(3));

        final KCVMutation mutation = mutEntry.getValue();
        final List&amp;lt;org.apache.cassandra.thrift.Mutation&amp;gt; thriftMutation = new ArrayList&amp;lt;&amp;gt;(mutations.size());
        
        // 省略删除的代码。
        
        if (mutation.hasAdditions()) {
            
            for (final Entry ent : mutation.getAdditions()) {
                final ColumnOrSuperColumn columnOrSuperColumn = new ColumnOrSuperColumn();
                
                // mutations 的第三层 key 是 column
                final Column column = new Column(ent.getColumnAs(StaticBuffer.BB_FACTORY));
                // mutations 的 value 是 value
                column.setValue(ent.getValueAs(StaticBuffer.BB_FACTORY));

                column.setTimestamp(commitTime.getAdditionTime(times));

                final Integer ttl = (Integer) ent.getMetaData().get(EntryMetaData.TTL);
                if (null != ttl &amp;amp;&amp;amp; ttl &amp;gt; 0) {
                    column.setTtl(ttl);
                }

                columnOrSuperColumn.setColumn(column);
                org.apache.cassandra.thrift.Mutation m = new org.apache.cassandra.thrift.Mutation();
                m.setColumn_or_supercolumn(columnOrSuperColumn);
                thriftMutation.add(m);
            }
        }

        cfmutation.put(columnFamily, thriftMutation);
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以看出 mutateMany 方法的参数和写到 cassandra 的结果不是完全一致，主要是 rowkey 和 columnFamily 的位置是反的。&lt;/p&gt;

&lt;h2 id=&#34;传入-mutatemany-的数据&#34;&gt;传入 mutateMany 的数据&lt;/h2&gt;

&lt;p&gt;通过调试可以看出，调用 mutateMany 的地方主要是 &lt;code&gt;CacheTransation.persist&lt;/code&gt; ,而调用 persist 的就是 flushInternal 方法。相应代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;// 成员变量： Map&amp;lt;KCVSCache, Map&amp;lt;StaticBuffer, KCVEntryMutation&amp;gt;&amp;gt; mutations

// 新建Map，这个 map 就是上面 mutateMany 的参数，key 分别是 columnFamily 和 rowKey ，
final Map&amp;lt;String, Map&amp;lt;StaticBuffer, KCVMutation&amp;gt;&amp;gt; subMutations = new HashMap&amp;lt;&amp;gt;(mutations.size());

int numSubMutations = 0;
// 遍历 mutations
for (Map.Entry&amp;lt;KCVSCache,Map&amp;lt;StaticBuffer, KCVEntryMutation&amp;gt;&amp;gt; storeMutations : mutations.entrySet()) {
    final Map&amp;lt;StaticBuffer, KCVMutation&amp;gt; sub = new HashMap&amp;lt;&amp;gt;();
    
    // KCVSCache 的 getKey().getName() 就是 columnFamily
    subMutations.put(storeMutations.getKey().getName(),sub);
   
    // mutations 的 value
    for (Map.Entry&amp;lt;StaticBuffer,KCVEntryMutation&amp;gt; mutationsForKey : storeMutations.getValue().entrySet()) {
        if (mutationsForKey.getValue().isEmpty()) continue;
        
        // 将 mutationsForKey 放进去，这个 convert 做了啥没有具体研究，可能只是一个适配。
        sub.put(mutationsForKey.getKey(), convert(mutationsForKey.getValue()));
        numSubMutations+=mutationsForKey.getValue().getTotalMutations();
        if (numSubMutations&amp;gt;= persistChunkSize) {
            numSubMutations = persist(subMutations);
            sub.clear();
            subMutations.put(storeMutations.getKey().getName(),sub);
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;mutations-的构造&#34;&gt;mutations 的构造&lt;/h2&gt;

&lt;p&gt;上面我们看出了，其实基本上没复杂处理，接下来我们看看 mutations 数据哪里来的。&lt;/p&gt;

&lt;p&gt;对于 mutations 的修改操作，来自于 mutate 方法，代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;// 传入的是 store（包含了columnFamily） key（rowKey） additions 和 deletions
void mutate(KCVSCache store, StaticBuffer key, List&amp;lt;Entry&amp;gt; additions, List&amp;lt;Entry&amp;gt; deletions) throws BackendException {
    Preconditions.checkNotNull(store);
    if (additions.isEmpty() &amp;amp;&amp;amp; deletions.isEmpty()) return;
    
    // 构造 KCVEntryMutation
    KCVEntryMutation m = new KCVEntryMutation(additions, deletions);
    
    // 这几步就是简单的合并所以的 additions 和 deletions
    final Map&amp;lt;StaticBuffer, KCVEntryMutation&amp;gt; storeMutation = mutations.computeIfAbsent(store, k -&amp;gt; new HashMap&amp;lt;&amp;gt;());
    KCVEntryMutation existingM = storeMutation.get(key);
    
    if (existingM != null) {
        existingM.merge(m);
    } else {
        storeMutation.put(key, m);
    }

    numMutations += m.getTotalMutations();

    if (batchLoading &amp;amp;&amp;amp; numMutations &amp;gt;= persistChunkSize) {
        flushInternal();
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;mutate-方法参数来源&#34;&gt;mutate 方法参数来源&lt;/h2&gt;

&lt;p&gt;mutate 方法传入的是 store（包含了columnFamily） key（rowKey） additions 和 deletions，这几个参数哪里来的呢？ KCVSCache 的 mutateEntries，
mutateEdges 调用时机呢？ edgeStore.mutateEntries(key, additions, deletions, storeTx); indexStore.mutateEntries(key, additions, deletions, storeTx);
我们先以 edgeStore 为例，在 StandardJanusGraph 的 prepareCommit 方法中，调用了 mutator.mutateEdges(vertexKey, additions, deletions);
代码如下，我们删掉了部分代码，包括 索引和数据删除。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;ListMultimap&amp;lt;Long, InternalRelation&amp;gt; mutations = ArrayListMultimap.create();
ListMultimap&amp;lt;InternalVertex, InternalRelation&amp;gt; mutatedProperties = ArrayListMultimap.create();
List&amp;lt;IndexSerializer.IndexUpdate&amp;gt; indexUpdates = Lists.newArrayList();


//2) Collect added edges and their index updates and acquire edge locks
// add 是 InternalRelation ，包括 VertexProperty 和 Edge，前面分析过，VertexProperty 实际上就是顶点和一个 schema 的订单建一条边，Edge 就是两个顶点建一条边。
for (InternalRelation add : Iterables.filter(addedRelations,filter)) {
    Preconditions.checkArgument(add.isNew());
    
    // getLen 返回这个 Relation 的长度，如果是 VertexProperty 是1，Edge 是需要根据方向进行判断
    for (int pos = 0; pos &amp;lt; add.getLen(); pos++) {
        // 得到对应的 vertex 
        InternalVertex vertex = add.getVertex(pos);
        if (pos == 0 || !add.isLoop()) {
        
            // mutatedProperties 的 key: InternalVertex,value:InternalRelation,mutatedProperties 是用于更新索引的，在我们这里实际上没什么用。
            if (add.isProperty()) mutatedProperties.put(vertex,add);
            // mutations 的 key ： vertexId, value ： InternalRelation
            mutations.put(vertex.longId(), add);
        }
        if (!vertex.isNew() &amp;amp;&amp;amp; acquireLock(add,pos,acquireLocks)) {
            Entry entry = edgeSerializer.writeRelation(add, pos, tx);
            mutator.acquireEdgeLock(idManager.getKey(vertex.longId()), entry.getColumn());
        }
    }
}


//5) Add relation mutations
for (Long vertexId : mutations.keySet()) {
    Preconditions.checkArgument(vertexId &amp;gt; 0, &amp;quot;Vertex has no id: %s&amp;quot;, vertexId);
    final List&amp;lt;InternalRelation&amp;gt; edges = mutations.get(vertexId);
    final List&amp;lt;Entry&amp;gt; additions = new ArrayList&amp;lt;&amp;gt;(edges.size());
    final List&amp;lt;Entry&amp;gt; deletions = new ArrayList&amp;lt;&amp;gt;(Math.max(10, edges.size() / 10));
    for (final InternalRelation edge : edges) {
        // 得到 InternalRelationType ，分为 PropertyKey 和 EdgeLabel 两类
        final InternalRelationType baseType = (InternalRelationType) edge.getType();
        assert baseType.getBaseType()==null;

        for (InternalRelationType type : baseType.getRelationIndexes()) { 
            if (type.getStatus()== SchemaStatus.DISABLED) continue;
            // getArity 和 getLen 不一样，
            for (int pos = 0; pos &amp;lt; edge.getArity(); pos++) {
                if (!type.isUnidirected(Direction.BOTH) &amp;amp;&amp;amp; !type.isUnidirected(EdgeDirection.fromPosition(pos)))
                    continue; //Directionality is not covered
                
                // 如果是起始顶点
                if (edge.getVertex(pos).longId()==vertexId) {
                
                    // 根据 edge type pos tx 得到应该序列化的 StaticArrayEntry
                    StaticArrayEntry entry = edgeSerializer.writeRelation(edge, type, pos, tx);
                    if (edge.isRemoved()) {
                        deletions.add(entry);
                    } else {
                        Preconditions.checkArgument(edge.isNew());
                        int ttl = getTTL(edge);
                        if (ttl &amp;gt; 0) {
                            entry.setMetaData(EntryMetaData.TTL, ttl);
                        }
                        additions.add(entry);
                    }
                }
            }
        }
    }

    StaticBuffer vertexKey = idManager.getKey(vertexId);
    mutator.mutateEdges(vertexKey, additions, deletions);
}

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;edgeserializer-writerelation-到底做了什么&#34;&gt;edgeSerializer.writeRelation 到底做了什么&lt;/h2&gt;

&lt;p&gt;我们现在就想知道，数据是怎么被序列化话 entry 的，代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public StaticArrayEntry writeRelation(InternalRelation relation, InternalRelationType type, int position,
                                      TypeInspector tx) {
    assert type==relation.getType() || (type.getBaseType() != null
            &amp;amp;&amp;amp; type.getBaseType().equals(relation.getType()));
    // 得到方向，pos 是 0 就是 out，是 1 就是 in
    Direction dir = EdgeDirection.fromPosition(position);
    
    // 方向验证
    Preconditions.checkArgument(type.isUnidirected(Direction.BOTH) || type.isUnidirected(dir));
    
    // 得到 typeId， 这个 type 是 VertexLabel 或者 PropertyKey
    long typeId = type.longId();
    // 得到 dirID 
    DirectionID dirID = getDirID(dir, relation.isProperty() ? RelationCategory.PROPERTY : RelationCategory.EDGE);
    
    // 得到 一个 out
    DataOutput out = serializer.getDataOutput(DEFAULT_CAPACITY);
    // key 和 value 的分割地址。
    int valuePosition;
    
    // 写 typeId 和 dirID 
    IDHandler.writeRelationType(out, typeId, dirID, type.isInvisibleType());
    
    // 得到 multiplicity 和 sortKey
    Multiplicity multiplicity = type.multiplicity();
    long[] sortKey = type.getSortKey();
    
    assert !multiplicity.isConstrained() || sortKey.length==0: type.name();
    int keyStartPos = out.getPosition();
    if (!multiplicity.isConstrained()) {
        // 如果 multiplicity 是 没有限制，也就是为 MULTI，必须要有 sortKey，写出 sortKey。
        writeInlineTypes(sortKey, relation, out, tx, InlineType.KEY);
    }
    
    // 到这里 key 就写完了，得到 key 的 pos
    int keyEndPos = out.getPosition();

    long relationId = relation.longId();

    //How multiplicity is handled for edges and properties is slightly different
    if (relation.isEdge()) {
        // 得到另一个 vertex 的 id
        long otherVertexId = relation.getVertex((position + 1) % 2).longId();
        // 如果 multiplicity 有限制
        if (multiplicity.isConstrained()) {
            // isUnique
            if (multiplicity.isUnique(dir)) {
                // 得到 valuePosition ，写出 otherVertexId 
                valuePosition = out.getPosition();
                VariableLong.writePositive(out, otherVertexId);
            } else {
                // 反方向写 otherVertexId ,记下 valuePosition
                VariableLong.writePositiveBackward(out, otherVertexId);
                valuePosition = out.getPosition();
            }
            // 写下 relationId
            VariableLong.writePositive(out, relationId);
        } else {
            // 没有限制，反方向写出 otherVertexId 和 relationId ，记下 valuePosition
            VariableLong.writePositiveBackward(out, otherVertexId);
            VariableLong.writePositiveBackward(out, relationId);
            valuePosition = out.getPosition();
        }
    } else { // PropertyKey
        assert relation.isProperty();
        Preconditions.checkArgument(relation.isProperty());
        // 得到 property 的值。
        Object value = ((JanusGraphVertexProperty) relation).value();
        Preconditions.checkNotNull(value);
        PropertyKey key = (PropertyKey) type;
        assert key.dataType().isInstance(value);
        
        // 写出 value 得到 valuePosition
        if (multiplicity.isConstrained()) { // 没有限制的 property
            if (multiplicity.isUnique(dir)) { //Cardinality=SINGLE
                valuePosition = out.getPosition();
                writePropertyValue(out,key,value);
            } else { //Cardinality=SET
                writePropertyValue(out,key,value);
                valuePosition = out.getPosition();
            }
            VariableLong.writePositive(out, relationId);
        } else {
            assert multiplicity.getCardinality()== Cardinality.LIST;
            VariableLong.writePositiveBackward(out, relationId);
            valuePosition = out.getPosition();
            writePropertyValue(out,key,value);
        }
    }

    //Write signature
    long[] signature = type.getSignature();
    writeInlineTypes(signature, relation, out, tx, InlineType.SIGNATURE);

    //Write remaining properties
    LongSet writtenTypes = new LongHashSet(sortKey.length + signature.length);
    if (sortKey.length &amp;gt; 0 || signature.length &amp;gt; 0) {
        for (long id : sortKey) writtenTypes.add(id);
        for (long id : signature) writtenTypes.add(id);
    }
    LongArrayList remainingTypes = new LongArrayList(8);
    for (PropertyKey t : relation.getPropertyKeysDirect()) {
        if (!(t instanceof ImplicitKey) &amp;amp;&amp;amp; !writtenTypes.contains(t.longId())) {
            remainingTypes.add(t.longId());
        }
    }
    //Sort types before writing to ensure that value is always written the same way
    long[] remaining = remainingTypes.toArray();
    Arrays.sort(remaining);
    for (long tid : remaining) {
        PropertyKey t = tx.getExistingPropertyKey(tid);
        writeInline(out, t, relation.getValueDirect(t), InlineType.NORMAL);
    }
    assert valuePosition&amp;gt;0;

    return new StaticArrayEntry(type.getSortOrder() == Order.DESC ?
                                out.getStaticBufferFlipBytes(keyStartPos, keyEndPos) :
                                out.getStaticBuffer(), valuePosition);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其实就是不断吧值写进去并且记录一下值的位置。&lt;/p&gt;

&lt;p&gt;我们需要了解一下 writeInline 方法以及  StaticArrayEntry VariableLong 类。&lt;/p&gt;

&lt;h3 id=&#34;staticarrayentry&#34;&gt;StaticArrayEntry&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;Entry (org.janusgraph.diskstorage)
BaseStaticArrayEntry (org.janusgraph.diskstorage.util)
StaticEntry in StaticArrayEntryList (org.janusgraph.diskstorage.util)
StaticArrayEntry (org.janusgraph.diskstorage.util)
SwappingEntry in StaticArrayEntryList (org.janusgraph.diskstorage.util)
StaticArrayEntry (org.janusgraph.diskstorage.util)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Entry 代表存储在 cassandra 基本结构，有 getColumn getValuePosition getValue 等方法，
BaseStaticArrayEntry 则是利用一个 array,offset,limit,valuePosition 进行封装。&lt;/p&gt;

&lt;h3 id=&#34;variablelong&#34;&gt;VariableLong&lt;/h3&gt;

&lt;p&gt;这个提供了一个读写Long类型的数据的方法，具体后续研究。&lt;/p&gt;

&lt;h3 id=&#34;writeinline&#34;&gt;writeInline&lt;/h3&gt;

&lt;p&gt;writeInline 方法实现：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;private void writeInlineTypes(long[] keyIds, InternalRelation relation, DataOutput out, TypeInspector tx,
                              InlineType inlineType) {
    for (long keyId : keyIds) {
        PropertyKey t = tx.getExistingPropertyKey(keyId);
        writeInline(out, t, relation.getValueDirect(t), inlineType);
    }
}

private void writeInline(DataOutput out, PropertyKey inlineKey, Object value, InlineType inlineType) {
    assert inlineType.writeInlineKey() || !AttributeUtil.hasGenericDataType(inlineKey);

    if (inlineType.writeInlineKey()) {
        IDHandler.writeInlineRelationType(out, inlineKey.longId());
    }

    writePropertyValue(out,inlineKey,value, inlineType);
}

private void writePropertyValue(DataOutput out, PropertyKey key, Object value, InlineType inlineType) {
    if (AttributeUtil.hasGenericDataType(key)) {
        assert !inlineType.writeByteOrdered();
        out.writeClassAndObject(value);
    } else {
        assert value==null || value.getClass().equals(key.dataType());
        if (inlineType.writeByteOrdered()) out.writeObjectByteOrder(value, key.dataType());
        else out.writeObject(value, key.dataType());
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从代码我们可以看出，实际上都是对id进行的操作，所以如果知道了顶点的 id，给顶点添加边和属性，实际上不需要查询这个顶点，直接操作即可，所以这给我们导数据提供了一种思路，可以直接操作 id。&lt;/p&gt;

&lt;h2 id=&#34;writerelation-方法的参数怎么构造的&#34;&gt;writeRelation 方法的参数怎么构造的&lt;/h2&gt;

&lt;p&gt;从上面我们可以看出来 edgeSerializer.writeRelation 方法的参数是 (edge, type, pos, tx)，而 edge 来自于对 mutations 的处理，mutations 来自 add ,add 来自 addedRelations。&lt;/p&gt;

&lt;p&gt;addedRelations 进行 add 操作的步骤在 StandardJanusGraph 的 connectRelation(InternalRelation r) 方法中。connectRelation 方法有两处调用 addEdge 和 addProperty。&lt;/p&gt;

&lt;p&gt;addEdge 和 addProperty 的调用栈就比较多了。&lt;/p&gt;

&lt;h1 id=&#34;正向理清思路&#34;&gt;正向理清思路&lt;/h1&gt;

&lt;h2 id=&#34;1-janus-官网介绍&#34;&gt;1. janus 官网介绍&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://docs.janusgraph.org/latest/schema.html&#34;&gt;https://docs.janusgraph.org/latest/schema.html&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;edge-label&#34;&gt;Edge Label&lt;/h3&gt;

&lt;p&gt;Multiplicity&lt;/p&gt;

&lt;p&gt;MULTI SIMPLE MANY2ONE ONE2MANY ONE2ONE 五种，每种的意义可以参考官网。默认的是 MULTI&lt;/p&gt;

&lt;h3 id=&#34;property-keys&#34;&gt;Property Keys&lt;/h3&gt;

&lt;p&gt;dataType(Class) 确定数据类型，Object.class 能够传入任何参数，但是不鼓励。&lt;/p&gt;

&lt;p&gt;Property Key Cardinality&lt;/p&gt;

&lt;p&gt;SINGLE: Allows at most one value per element for such key. In other words, the key→value mapping is unique for all elements in the graph. The property key birthDate is an example with SINGLE cardinality since each person has exactly one birth date.
LIST: Allows an arbitrary number of values per element for such key. In other words, the key is associated with a list of values allowing duplicate values. Assuming we model sensors as vertices in a graph, the property key sensorReading is an example with LIST cardinality to allow lots of (potentially duplicate) sensor readings to be recorded.
SET: Allows multiple values but no duplicate values per element for such key. In other words, the key is associated with a set of values. The property key name has SET cardinality if we want to capture all names of an individual (including nick name, maiden name, etc).&lt;/p&gt;

&lt;h3 id=&#34;relation-types&#34;&gt;Relation Types&lt;/h3&gt;

&lt;p&gt;Edge labels and property keys are jointly referred to as relation types ,must unique&lt;/p&gt;

&lt;h3 id=&#34;vertex-labels&#34;&gt;Vertex Labels&lt;/h3&gt;

&lt;p&gt;call makeVertexLabel(String).make()&lt;/p&gt;

&lt;h3 id=&#34;unidirected-edges&#34;&gt;Unidirected Edges&lt;/h3&gt;

&lt;p&gt;单向的边是只能在向外方向上遍历的边。单指向边具有较低的存储占用，但在它们支持的遍历类型中受到限制。单向的边在概念上类似于万维网中的超链接，在这个意义上，外顶点可以遍历边缘，但是顶点不知道它的存在。&lt;/p&gt;

&lt;h2 id=&#34;2-addproperty-和-addedge&#34;&gt;2. addProperty 和 addEdge&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public JanusGraphVertexProperty addProperty(VertexProperty.Cardinality cardinality, JanusGraphVertex vertex, PropertyKey key, Object value) {
    if (key.cardinality().convert()!=cardinality &amp;amp;&amp;amp; cardinality!=VertexProperty.Cardinality.single)
        throw new SchemaViolationException(&amp;quot;Key is defined for %s cardinality which conflicts with specified: %s&amp;quot;,key.cardinality(),cardinality);
    verifyWriteAccess(vertex);
    Preconditions.checkArgument(!(key instanceof ImplicitKey),&amp;quot;Cannot create a property of implicit type: %s&amp;quot;,key.name());
    vertex = ((InternalVertex) vertex).it();
    Preconditions.checkNotNull(key);
    checkPropertyConstraintForVertexOrCreatePropertyConstraint(vertex, key);
    final Object normalizedValue = verifyAttribute(key, value);
    
    // 得到 Cardinality SINGLE LIST SET ，一般是 SINGLE
    Cardinality keyCardinality = key.cardinality();
    
    // 省略部分代码
    try {
          // 省略检查
          
        StandardVertexProperty prop = new StandardVertexProperty(IDManager.getTemporaryRelationID(temporaryIds.nextID()), key, (InternalVertex) vertex, normalizedValue, ElementLifeCycle.New);
        if (config.hasAssignIDsImmediately()) graph.assignID(prop);
        connectRelation(prop);
        return prop;
    } finally {
        uniqueLock.unlock();
    }

}

public JanusGraphEdge addEdge(JanusGraphVertex outVertex, JanusGraphVertex inVertex, EdgeLabel label) {
    verifyWriteAccess(outVertex, inVertex);
    outVertex = ((InternalVertex) outVertex).it();
    inVertex = ((InternalVertex) inVertex).it();
    Preconditions.checkNotNull(label);
    checkConnectionConstraintOrCreateConnectionConstraint(outVertex, inVertex, label);
    Multiplicity multiplicity = label.multiplicity();
    TransactionLock uniqueLock = getUniquenessLock(outVertex, (InternalRelationType) label,inVertex);
    uniqueLock.lock(LOCK_TIMEOUT);
    try {
     // 省略检查
        StandardEdge edge = new StandardEdge(IDManager.getTemporaryRelationID(temporaryIds.nextID()), label, (InternalVertex) outVertex, (InternalVertex) inVertex, ElementLifeCycle.New);
        if (config.hasAssignIDsImmediately()) graph.assignID(edge);
        connectRelation(edge);
        return edge;
    } finally {
        uniqueLock.unlock();
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其实这两个做的最主要的就两步： new StandardEdge new StandardVertexProperty  connectRelation(edge);&lt;/p&gt;

&lt;p&gt;connectRelation 最主要的就是 addedRelations.add&amp;reg;&lt;/p&gt;

&lt;p&gt;最后在 commit 的时候，会 处理  addedRelations，代码逻辑在上面我们已经看过了。我们在简化一下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;for (InternalRelation add : Iterables.filter(addedRelations,filter)) {
    Preconditions.checkArgument(add.isNew());

    for (int pos = 0; pos &amp;lt; add.getLen(); pos++) {
        InternalVertex vertex = add.getVertex(pos);
        if (pos == 0 || !add.isLoop()) {
            // 添加 mutations
            mutations.put(vertex.longId(), add);
        }
    }
}

// 
for (Long vertexId : mutations.keySet()) {

    final List&amp;lt;InternalRelation&amp;gt; edges = mutations.get(vertexId);
    final List&amp;lt;Entry&amp;gt; additions = new ArrayList&amp;lt;&amp;gt;(edges.size());
    final List&amp;lt;Entry&amp;gt; deletions = new ArrayList&amp;lt;&amp;gt;(Math.max(10, edges.size() / 10));
    
    for (final InternalRelation edge : edges) {
        final InternalRelationType baseType = (InternalRelationType) edge.getType();
        assert baseType.getBaseType()==null;

        for (InternalRelationType type : baseType.getRelationIndexes()) { // getRelationIndexes 这里是得到了 RelationTypeIndex 相关的 关系
            if (type.getStatus()== SchemaStatus.DISABLED) continue;
            for (int pos = 0; pos &amp;lt; edge.getArity(); pos++) {
                if (!type.isUnidirected(Direction.BOTH) &amp;amp;&amp;amp; !type.isUnidirected(EdgeDirection.fromPosition(pos)))
                    continue; //Directionality is not covered
                if (edge.getVertex(pos).longId()==vertexId) {
                    StaticArrayEntry entry = edgeSerializer.writeRelation(edge, type, pos, tx);
                    if (edge.isRemoved()) {
                        deletions.add(entry);
                    } else {
                        Preconditions.checkArgument(edge.isNew());
                        int ttl = getTTL(edge);
                        if (ttl &amp;gt; 0) {
                            entry.setMetaData(EntryMetaData.TTL, ttl);
                        }
                        additions.add(entry);
                    }
                }
            }
        }
    }

    StaticBuffer vertexKey = idManager.getKey(vertexId);
    mutator.mutateEdges(vertexKey, additions, deletions);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;mutateedges&#34;&gt;mutateEdges&lt;/h2&gt;

&lt;p&gt;会逐步调用
edgeStore.mutateEntries(key, additions, deletions, storeTx);
mutateEntries(StaticBuffer key, List&lt;Entry&gt; additions, List&lt;Entry&gt; deletions, StoreTransaction txh)
mutate&lt;/p&gt;

&lt;p&gt;mutate 会将改变都记录到 mutations 中，在 flushInternal 的时候 mutations 会变换一下记录到 subMutations ，然后调用 persist(subMutations);
紧接着调用 manager.mutateMany(subMutations, tx); 最后重构成 cfmutation，通过 cassandra 的 CTConnection 保存到 cassandra 中。&lt;/p&gt;

&lt;p&gt;这样看来，整个过程就清晰了。&lt;/p&gt;

&lt;h2 id=&#34;id-分配&#34;&gt;id 分配&lt;/h2&gt;

&lt;p&gt;StandardIDPool 进行 id 的分配，调用 graph.assignID(schemaVertex, BaseVertexLabel.DEFAULT_VERTEXLABEL) 等方法的时候，会调用。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;assignID:455, StandardJanusGraph (org.janusgraph.graphdb.database)
assignID:153, VertexIDAssigner (org.janusgraph.graphdb.database.idassigner)
assignID:182, VertexIDAssigner (org.janusgraph.graphdb.database.idassigner)
assignID:308, VertexIDAssigner (org.janusgraph.graphdb.database.idassigner)
nextID:204, StandardIDPool (org.janusgraph.graphdb.database.idassigner)
nextBlock:173, StandardIDPool (org.janusgraph.graphdb.database.idassigner)
startIDBlockGetter:247, StandardIDPool (org.janusgraph.graphdb.database.idassigner)

call:288, StandardIDPool$IDBlockGetter (org.janusgraph.graphdb.database.idassigner)
getIDBlock:213, ConsistentKeyIDAuthority (org.janusgraph.diskstorage.idmanagement)
    getBlockApplication:373, ConsistentKeyIDAuthority (org.janusgraph.diskstorage.idmanagement)
idStore.mutate(partitionKey, Arrays.asList(StaticArrayEntry.of(finalTarget)), KeyColumnValueStore.NO_DELETIONS, txh);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里涉及到了很多东西，而且是在两个线程中完成的，就不太方便处理了。&lt;/p&gt;

&lt;h3 id=&#34;vertexidassigner&#34;&gt;VertexIDAssigner&lt;/h3&gt;

&lt;p&gt;首先是 VertexIDAssigner 的创建：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public VertexIDAssigner(Configuration config, IDAuthority idAuthority, StoreFeatures idAuthFeatures) {
    Preconditions.checkNotNull(idAuthority);
    this.idAuthority = idAuthority;

    int partitionBits = NumberUtil.getPowerOf2(config.get(CLUSTER_MAX_PARTITIONS));
    idManager = new IDManager(partitionBits);
    Preconditions.checkArgument(idManager.getPartitionBound() &amp;lt;= Integer.MAX_VALUE &amp;amp;&amp;amp; idManager.getPartitionBound()&amp;gt;0);
    this.partitionIdBound = (int)idManager.getPartitionBound();
    hasLocalPartitions = idAuthFeatures.hasLocalKeyPartition();

    placementStrategy = Backend.getImplementationClass(config, config.get(PLACEMENT_STRATEGY),
            REGISTERED_PLACEMENT_STRATEGIES);
    placementStrategy.injectIDManager(idManager);
    log.debug(&amp;quot;Partition IDs? [{}], Local Partitions? [{}]&amp;quot;,true,hasLocalPartitions);

    long baseBlockSize = config.get(IDS_BLOCK_SIZE);
    idAuthority.setIDBlockSizer(new SimpleVertexIDBlockSizer(baseBlockSize));

    renewTimeoutMS = config.get(IDS_RENEW_TIMEOUT);
    renewBufferPercentage = config.get(IDS_RENEW_BUFFER_PERCENTAGE);

    idPools = new ConcurrentHashMap&amp;lt;Integer, PartitionIDPool&amp;gt;(partitionIdBound);
    schemaIdPool = new StandardIDPool(idAuthority, IDManager.SCHEMA_PARTITION, PoolType.SCHEMA.getIDNamespace(),
            IDManager.getSchemaCountBound(), renewTimeoutMS, renewBufferPercentage);
    partitionVertexIdPool = new StandardIDPool(idAuthority, IDManager.PARTITIONED_VERTEX_PARTITION, PoolType.PARTITIONED_VERTEX.getIDNamespace(),
            PoolType.PARTITIONED_VERTEX.getCountBound(idManager), renewTimeoutMS, renewBufferPercentage);
    setLocalPartitions(partitionBits);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;里面主要有 idAuthority , idManager(partitionBits=5) partitionIdBound=32  placementStrategy idPools schemaIdPool partitionVertexIdPool .&lt;/p&gt;

&lt;h4 id=&#34;assignid&#34;&gt;assignID&lt;/h4&gt;

&lt;p&gt;id 有三个部分组成 [0 count suffix partitionId],count，最高位是0，然后是后缀。后缀在 IDManager 中有配置,partitionId 默认是5位.
第一部是 得到 partitionID ，分为很多种情况，例如 schema 为0，分区的为 -1，vertex 的为 placementStrategy 随机获得。例如8， Relation 通过 incident 获得。&lt;/p&gt;

&lt;p&gt;然后才是 assignID
先得到 count ，得到过程是：
通过 partition 在 idPools 得到 PartitionIDPool，如果没有，新建 PartitionIDPool ，然后在每个 PartitionIDPool 中新建 3个 StandardIDPool ，分别对应 NORMAL_VERTEX, UNMODIFIABLE_VERTEX, RELATION;
在 PartitionIDPool 中得到现在的 element 所对应的 idPool，然后调用 count = idPool.nextID()  得到count ,nextID 会调用 currentBlock 得到 id。如果当前的 currentBlock 分配完了，重新申请一个 block&lt;/p&gt;

&lt;p&gt;调用 getId 方法的时候，会有一个 uniqueIDBitWidth ，默认是 4位，然后还有一个 unique 数值是0，最后返回的是得到的count 左右4位，如果是1，就是16.
返回了 count，然后构造的 结果就是 00000 0 000010000000&lt;/p&gt;

&lt;h3 id=&#34;standardidpool&#34;&gt;StandardIDPool&lt;/h3&gt;

&lt;p&gt;构造传入了：idAuthority partition idNamespace idUpperBound renewBufferPercentage
还有一个 exec ，用来执行线程。
还有 currentBlock currentIndex renewBlockIndex 记录当前的状态。&lt;/p&gt;

&lt;p&gt;###&lt;/p&gt;

&lt;p&gt;我们从 assignID 开始看。分为两步：&lt;/p&gt;

&lt;p&gt;partitionID = placementStrategy.getPartition(element);&lt;/p&gt;

&lt;h2 id=&#34;调试一次&#34;&gt;调试一次&lt;/h2&gt;

&lt;p&gt;接下来我们可以调试一次，通过调试过每一步，熟悉每一步的内容。&lt;/p&gt;

&lt;h1 id=&#34;bulk-loading&#34;&gt;bulk loading&lt;/h1&gt;

&lt;p&gt;接下来我们要做一个导数据的工具。我们有一堆给定好的书籍，然后我们能够将数据导入到 janus 中，我们需要结合 cassandra 和 hbase 自带的 bulk loading 工具。
首先我们需要得到所有的序列化的数据，实际上就是 edge 数据，而 index 的数据我们可以后续调用 reindex。&lt;/p&gt;

&lt;h2 id=&#34;vertex-导入&#34;&gt;vertex 导入&lt;/h2&gt;

&lt;p&gt;我们可以想象一下，导入边的流程，首先要有一个表格，并且这个表格要带有表头，然后下面的就是数据。表头包括字段名和数据类型，其中第一个是主键。例如有电话的数据，表头结构为：
phone:string,name:string,relation:integer&lt;/p&gt;

&lt;p&gt;我们程序首先是验证数据，验证数据主要是重复性检验，格式检验。然后需要创建 schema。读取所有的表头，并创建好 schema。然后导入数据。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>janusgraph源码分析8-索引存储</title>
      <link>https://dengziming.github.io/post/titan/janusgraph%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%908-%E7%B4%A2%E5%BC%95%E5%AD%98%E5%82%A8/</link>
      <pubDate>Mon, 09 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/titan/janusgraph%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%908-%E7%B4%A2%E5%BC%95%E5%AD%98%E5%82%A8/</guid>
      
        <description>

&lt;p&gt;上一节我们了解了 JanusGraph 的关系存储，主要是在 EdgeSerializer 中的序列化和反序列化，我们还要这次看看 IndexSerializer 的相关类。&lt;/p&gt;

&lt;h1 id=&#34;基础类&#34;&gt;基础类&lt;/h1&gt;

&lt;h2 id=&#34;indexserializer&#34;&gt;IndexSerializer&lt;/h2&gt;

&lt;p&gt;用来序列化，反序列化&lt;/p&gt;

&lt;h2 id=&#34;indexprovider&#34;&gt;IndexProvider&lt;/h2&gt;

&lt;p&gt;IndexProvider 继承自 IndexInformation ，IndexInformation 主要判断是否支持某个 KeyInformation，
IndexProvider 最主要的是 mutate 方法，该方法就是用来保存数据到底层存储系统。&lt;/p&gt;

&lt;h2 id=&#34;keyinformation&#34;&gt;KeyInformation&lt;/h2&gt;

&lt;p&gt;保存key的信息，有三个内部接口&lt;/p&gt;

&lt;h3 id=&#34;storeretriever&#34;&gt;StoreRetriever&lt;/h3&gt;

&lt;p&gt;能够根据key得到 KeyInformation&lt;/p&gt;

&lt;h3 id=&#34;indexretriever&#34;&gt;IndexRetriever&lt;/h3&gt;

&lt;p&gt;能够根据key 和 store得到 KeyInformation
根据store 得到 StoreRetriever&lt;/p&gt;

&lt;h3 id=&#34;retriever&#34;&gt;Retriever&lt;/h3&gt;

&lt;p&gt;根据 index 得到IndexRetriever&lt;/p&gt;

&lt;p&gt;这几个比较混乱。主要实现在 IndexInfoRetriever 中。&lt;/p&gt;

&lt;h2 id=&#34;indexserializer-indexinforetriever&#34;&gt;IndexSerializer.IndexInfoRetriever&lt;/h2&gt;

&lt;p&gt;IndexInfoRetriever 继承自 KeyInformation.Retriever，只有一个 get 方法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public static class IndexInfoRetriever implements KeyInformation.Retriever {

    private final StandardJanusGraphTx transaction;

    private IndexInfoRetriever(StandardJanusGraphTx tx) {
        Preconditions.checkNotNull(tx);
        transaction=tx;
    }

    @Override
    public KeyInformation.IndexRetriever get(final String index) {
        return new KeyInformation.IndexRetriever() {

            final Map&amp;lt;String,KeyInformation.StoreRetriever&amp;gt; indexes = new ConcurrentHashMap&amp;lt;&amp;gt;();

            @Override
            public KeyInformation get(String store, String key) {
                return get(store).get(key);
            }

            @Override
            public KeyInformation.StoreRetriever get(final String store) {
                if (indexes.get(store)==null) {
                    Preconditions.checkState(transaction!=null,&amp;quot;Retriever has not been initialized&amp;quot;);
                    final MixedIndexType extIndex = getMixedIndex(store, transaction);
                    assert extIndex.getBackingIndexName().equals(index);
                    final ImmutableMap.Builder&amp;lt;String,KeyInformation&amp;gt; b = ImmutableMap.builder();
                    for (final ParameterIndexField field : extIndex.getFieldKeys()) b.put(key2Field(field),getKeyInformation(field));
                    final ImmutableMap&amp;lt;String,KeyInformation&amp;gt; infoMap = b.build();
                    final KeyInformation.StoreRetriever storeRetriever = infoMap::get;
                    indexes.put(store,storeRetriever);
                }
                return indexes.get(store);
            }

        };
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过看代码，我们发现其实都是几个map。
1. 首先 IndexInfoRetriever 里面有个 &lt;code&gt;final Map&amp;lt;String,KeyInformation.StoreRetriever&amp;gt; indexes = new ConcurrentHashMap&amp;lt;&amp;gt;();&lt;/code&gt;
2. KeyInformation.StoreRetriever 实际上也就是一个 &lt;code&gt;final ImmutableMap&amp;lt;String,KeyInformation&amp;gt; infoMap = b.build();&lt;/code&gt;
3. 调用 IndexInfoRetriever 的 get 方法，会调用getMixedIndex(store, transaction); 也就是说这个得到的只是 MixedIndexType。&lt;/p&gt;

&lt;h2 id=&#34;recordentry&#34;&gt;RecordEntry&lt;/h2&gt;

&lt;p&gt;这个类 有三个属性，分别是 long relationId, Object value, PropertyKey key，这应该就代表了待建索引的一个记录。&lt;/p&gt;

&lt;h2 id=&#34;indexrecords&#34;&gt;IndexRecords&lt;/h2&gt;

&lt;p&gt;public static class IndexRecords extends ArrayList&lt;RecordEntry[]&gt;&lt;/p&gt;

&lt;p&gt;看上去像一个二维数组，记录索引的更新。&lt;/p&gt;

&lt;h2 id=&#34;indexupdate&#34;&gt;IndexUpdate&lt;/h2&gt;

&lt;p&gt;这个类主要是提供一些计算索引更新的工具方法。&lt;/p&gt;

&lt;h2 id=&#34;索引回顾&#34;&gt;索引回顾&lt;/h2&gt;

&lt;p&gt;我们先回顾一下相关知识，主要是我们建索引的时候发生了什么。&lt;code&gt;JanusGraphIndex nameIndex = management.buildIndex(&amp;quot;name&amp;quot;, Vertex.class).addKey(name).buildCompositeIndex();&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;private JanusGraphIndex createCompositeIndex(String indexName, ElementCategory elementCategory, boolean unique, JanusGraphSchemaType constraint, PropertyKey... keys) {
    // 
    Preconditions.checkArgument(!unique || elementCategory == ElementCategory.VERTEX, &amp;quot;Unique indexes can only be created on vertices [%s]&amp;quot;, indexName);
    boolean allSingleKeys = true;
    boolean oneNewKey = false;
    for (PropertyKey key : keys) {
        if (key.cardinality() != Cardinality.SINGLE) allSingleKeys = false;
        if (key.isNew()) oneNewKey = true;
        else updatedTypes.add((PropertyKeyVertex) key);
    }

    Cardinality indexCardinality;
    if (unique) indexCardinality = Cardinality.SINGLE;
    else indexCardinality = (allSingleKeys ? Cardinality.SET : Cardinality.LIST);

    boolean canIndexBeEnabled = oneNewKey || (constraint != null &amp;amp;&amp;amp; constraint.isNew());

    TypeDefinitionMap def = new TypeDefinitionMap();
    def.setValue(TypeDefinitionCategory.INTERNAL_INDEX, true);
    def.setValue(TypeDefinitionCategory.ELEMENT_CATEGORY, elementCategory);
    def.setValue(TypeDefinitionCategory.BACKING_INDEX, Token.INTERNAL_INDEX_NAME);
    def.setValue(TypeDefinitionCategory.INDEXSTORE_NAME, indexName);
    def.setValue(TypeDefinitionCategory.INDEX_CARDINALITY, indexCardinality);
    def.setValue(TypeDefinitionCategory.STATUS, canIndexBeEnabled ? SchemaStatus.ENABLED : SchemaStatus.INSTALLED);
    // 新建一个顶点。
    JanusGraphSchemaVertex indexVertex = transaction.makeSchemaVertex(JanusGraphSchemaCategory.GRAPHINDEX, indexName, def);
    for (int i = 0; i &amp;lt; keys.length; i++) {
        Parameter[] paras = {ParameterType.INDEX_POSITION.getParameter(i)};
        // 添加边，顶点分别是两个 index 和 propertykey
        addSchemaEdge(indexVertex, keys[i], TypeDefinitionCategory.INDEX_FIELD, paras);
    }

    Preconditions.checkArgument(constraint == null || (elementCategory.isValidConstraint(constraint) &amp;amp;&amp;amp; constraint instanceof JanusGraphSchemaVertex));
    if (constraint != null) {
        // 如果加了限制 ，在添加一条边。
        addSchemaEdge(indexVertex, (JanusGraphSchemaVertex) constraint, TypeDefinitionCategory.INDEX_SCHEMA_CONSTRAINT, null);
    }
    updateSchemaVertex(indexVertex);
    JanusGraphIndexWrapper index = new JanusGraphIndexWrapper(indexVertex.asIndexType());
    if (!oneNewKey) updateIndex(index, SchemaAction.REGISTER_INDEX);
    return index;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;addSchemaEdge 方法&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public JanusGraphEdge addSchemaEdge(JanusGraphVertex out, JanusGraphVertex in, TypeDefinitionCategory def, Object modifier) {
    assert def.isEdge();
    // 加一条边，边的 label 是 SchemaDefinitionEdge
    JanusGraphEdge edge = addEdge(out, in, BaseLabel.SchemaDefinitionEdge);
    TypeDefinitionDescription desc = new TypeDefinitionDescription(def, modifier);
    edge.property(BaseKey.SchemaDefinitionDesc.name(), desc);
    return edge;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看出其实就是新建一个 顶点，添加属性，然后添加边。&lt;/p&gt;

&lt;h2 id=&#34;indexserializer-getindexupdates-del&#34;&gt;indexSerializer.getIndexUpdates(del)&lt;/h2&gt;

&lt;p&gt;我们现在就看看 index 如何序列化。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public Collection&amp;lt;IndexUpdate&amp;gt; getIndexUpdates(InternalRelation relation) {
    assert relation.isNew() || relation.isRemoved();
    final Set&amp;lt;IndexUpdate&amp;gt; updates = Sets.newHashSet();
    final IndexUpdate.Type updateType = getUpdateType(relation);
    final int ttl = updateType==IndexUpdate.Type.ADD?StandardJanusGraph.getTTL(relation):0;
    for (final RelationType type : relation.getPropertyKeysDirect()) {
        if (!(type instanceof PropertyKey)) continue;
        final PropertyKey key = (PropertyKey)type;
        for (final IndexType index : ((InternalRelationType)key).getKeyIndexes()) {
            if (!indexAppliesTo(index,relation)) continue;
            IndexUpdate update;
            if (index instanceof CompositeIndexType) {
                final CompositeIndexType iIndex= (CompositeIndexType) index;
                final RecordEntry[] record = indexMatch(relation, iIndex);
                if (record==null) continue;
                update = new IndexUpdate&amp;lt;&amp;gt;(iIndex, updateType, getIndexKey(iIndex, record), getIndexEntry(iIndex, record, relation), relation);
            } else {
                assert relation.valueOrNull(key)!=null;
                if (((MixedIndexType)index).getField(key).getStatus()== SchemaStatus.DISABLED) continue;
                update = getMixedIndexUpdate(relation, key, relation.valueOrNull(key), (MixedIndexType) index, updateType);
            }
            if (ttl&amp;gt;0) update.setTTL(ttl);
            updates.add(update);
        }
    }
    return updates;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;compositeindextype&#34;&gt;CompositeIndexType&lt;/h3&gt;

&lt;p&gt;我们先看 CompositeIndexType 部分，我们发现主要就是 indexMatch 方法 和 new IndexUpdate，得到某个 relation 相关的index：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public static RecordEntry[] indexMatch(JanusGraphRelation relation, CompositeIndexType index) {
    // 得到所有的key。
    final IndexField[] fields = index.getFieldKeys();
    // 新建一个对应的数组
    final RecordEntry[] match = new RecordEntry[fields.length];
    for (int i = 0; i &amp;lt;fields.length; i++) {
        final IndexField f = fields[i];
        final Object value = relation.valueOrNull(f.getFieldKey());
        if (value==null) return null; //No match
        match[i] = new RecordEntry(relation.longId(),value,f.getFieldKey());
    }
    return match;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;总的来说还是很简单的，得到所有的索引字段的值即可，但是假如一个索引有两个字段，我们每次更新其中一个字段，都会更新一次索引，这岂不是会很麻烦。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;private StaticBuffer getIndexKey(CompositeIndexType index, Object[] values) {
    final DataOutput out = serializer.getDataOutput(8*DEFAULT_OBJECT_BYTELEN + 8);
    // 写入 indexType 的 ID
    VariableLong.writePositive(out, index.getID());
    final IndexField[] fields = index.getFieldKeys();
    Preconditions.checkArgument(fields.length&amp;gt;0 &amp;amp;&amp;amp; fields.length==values.length);
    for (int i = 0; i &amp;lt; fields.length; i++) {
        final IndexField f = fields[i];
        final Object value = values[i];
        Preconditions.checkNotNull(value);
        // 写入 index 的值。
        if (AttributeUtil.hasGenericDataType(f.getFieldKey())) {
            out.writeClassAndObject(value);
        } else {
            assert value.getClass().equals(f.getFieldKey().dataType()) : value.getClass() + &amp;quot; - &amp;quot; + f.getFieldKey().dataType();
            out.writeObjectNotNull(value);
        }
    }
    StaticBuffer key = out.getStaticBuffer();
    if (hashKeys) key = HashingUtil.hashPrefixKey(hashLength,key);
    return key;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看出 compositeindex 数据的key结构，indexId+value(所有的)。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;private Entry getIndexEntry(CompositeIndexType index, RecordEntry[] record, JanusGraphElement element) {
    final DataOutput out = serializer.getDataOutput(1+8+8*record.length+4*8);
    out.putByte(FIRST_INDEX_COLUMN_BYTE);
    if (index.getCardinality()!=Cardinality.SINGLE) { // 代表是 SET 或者 LIST
        // 写出 value 的 id
        VariableLong.writePositive(out,element.longId());
        if (index.getCardinality()!=Cardinality.SET) { // 如果是LIST
            // 循环写出 relationId
            for (final RecordEntry re : record) {
                VariableLong.writePositive(out,re.relationId);
            }
        }
    }
    // column 和 value 的分界点。
    final int valuePosition=out.getPosition();
    if (element instanceof JanusGraphVertex) { // 如果是顶点
        VariableLong.writePositive(out,element.longId());
    } else {
        assert element instanceof JanusGraphRelation;
        final RelationIdentifier rid = (RelationIdentifier)element.id();
        final long[] longs = rid.getLongRepresentation();
        Preconditions.checkArgument(longs.length == 3 || longs.length == 4);
        for (final long aLong : longs) VariableLong.writePositive(out, aLong);
    }
    return new StaticArrayEntry(out.getStaticBuffer(),valuePosition);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这两个都比较类似的，按照固定的格式，生成看key 和value。&lt;/p&gt;

&lt;h3 id=&#34;getmixedindexupdate&#34;&gt;getMixedIndexUpdate&lt;/h3&gt;

&lt;p&gt;上面看的是 compositeIndex 的序列化过程，还有 MixedIndex。&lt;code&gt;return new IndexUpdate&amp;lt;&amp;gt;(index, updateType, element2String(element), new IndexEntry(key2Field(index.getField(key)), value), element);&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;private static String element2String(Object elementId) {
    if (elementId instanceof Long) return longID2Name((Long)elementId);
    else return ((RelationIdentifier) elementId).toString();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;private static String key2Field(ParameterIndexField field) {
    assert field!=null;
    return ParameterType.MAPPED_NAME.findParameter(field.getParameters(),keyID2Name(field.getFieldKey()));
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这两个部分有很多其他的类，比较乱，后续可以自己整理一下，但是整体意思就是得到一个 key value 的类。。&lt;/p&gt;

&lt;h2 id=&#34;反序列化查询&#34;&gt;反序列化查询&lt;/h2&gt;

&lt;p&gt;查询过程稍微有点复杂，一般会通过读索引。后续进行分析。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>janusgraph主要类分析</title>
      <link>https://dengziming.github.io/post/titan/janusgraph%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%905-%E4%B8%BB%E8%A6%81%E7%B1%BB/</link>
      <pubDate>Tue, 03 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/titan/janusgraph%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%905-%E4%B8%BB%E8%A6%81%E7%B1%BB/</guid>
      
        <description></description>
      
    </item>
    
    <item>
      <title>resourcemanager</title>
      <link>https://dengziming.github.io/post/hadoop/hadoopha/</link>
      <pubDate>Wed, 23 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/hadoop/hadoopha/</guid>
      
        <description>

&lt;p&gt;参考资料：&lt;/p&gt;

&lt;h1 id=&#34;hadoopha&#34;&gt;HadoopHa&lt;/h1&gt;

&lt;p&gt;hadoop 有两个NameNode，Active NameNode和Standby NameNode，通过 DFSZKFailoverController extends ZKFailoverController 进行切换。
ZKFailoverController通过HealthMonitor线程能及时检测到NameNode的健康状况，在主NameNode故障时借助Zookeeper实现自动的主备选举和切换。
DataNode 会同时向主NameNode和备NameNode上报数据块的位置信息，但只接收来自active namenode的读写命令。&lt;/p&gt;

&lt;p&gt;为啥把监控分开?&lt;/p&gt;

&lt;p&gt;显然，我们不能在NN进程内进行心跳等信息同步，最简单的原因，一次FullGC就可以让NN挂起十几分钟，所以，必须要有一个独立的短小精悍的watchdog来专门负责监控。这也是一个松耦合的设计，便于扩展或更改。&lt;/p&gt;

&lt;p&gt;通过隔离和Quorum Journal Manager(QJM)共享存储空间实现HDFS HA&lt;/p&gt;

&lt;h1 id=&#34;dfszkfailovercontroller&#34;&gt;DFSZKFailoverController&lt;/h1&gt;

&lt;p&gt;启动代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;  public static void main(String args[])
      throws Exception {
    if (DFSUtil.parseHelpArgument(args, 
        ZKFailoverController.USAGE, System.out, true)) {
      System.exit(0);
    }
    
    GenericOptionsParser parser = new GenericOptionsParser(
        new HdfsConfiguration(), args);
    DFSZKFailoverController zkfc = DFSZKFailoverController.create(
        parser.getConfiguration());
    {
        NNHAServiceTarget localTarget = new NNHAServiceTarget(
        localNNConf, nsId, nnId);
        return new DFSZKFailoverController(localNNConf, localTarget);
    }
    
    System.exit(zkfc.run(parser.getRemainingArgs()));
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;run 的步骤：
initZK();
formatZK(force, interactive);
initRPC();
initHM();
startRPC();
mainLoop();&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;initZK();
{
    elector = new ActiveStandbyElector(zkQuorum,
        zkTimeout, getParentZnode(), zkAcls, zkAuths,
        new ElectorCallbacks(), maxRetryNum);
    {
    	new ElectorCallbacks()
    	  // 临时节点ActiveStandbyElectorLock，用于标识锁
    	zkLockFilePath = znodeWorkingDir + &amp;quot;/&amp;quot; + LOCK_FILENAME;
    	// 永久节点ActiveBreadCrumb，用于存放active信息
    	zkBreadCrumbPath = znodeWorkingDir + &amp;quot;/&amp;quot; + BREADCRUMB_FILENAME;
    	this.maxRetryNum = maxRetryNum;
    	// createConnection for future API calls
    	// 创建zk连接
    	createConnection();
    	{
    	      // 不幸的是，zk的构造方法连接上zk之后，可能马上触发连接事件。
  			  // 因此如果构造zk之后注册watcher，可能不会捕获到连接事件。
  			  // 取而代之的方法是，先构造Watcher，在设置了zk的引用之前，使它阻塞所有的事件
  			  
  			  watcher = new WatcherWithClientRef();
  			  ZooKeeper zk = new ZooKeeper(zkHostPort, zkSessionTimeout, watcher);
  			  // 在watcher中设置zk的引用
  			  watcher.setZooKeeperRef(zk);
  			  // Wait for the asynchronous success/failure. This may throw an exception
  			  // if we don&#39;t connect within the session timeout.
  			  watcher.waitForZKConnectionEvent(zkSessionTimeout);
  			  
  			  for (ZKAuthInfo auth : zkAuthInfo) {
  			    zk.addAuthInfo(auth.getScheme(), auth.getAuth());
  			  }
  			  return zk;
    	      }
    	  }
    	}
}

formatZK(force, interactive);
initRPC();
{
    new ZKFCRpcServer(conf, bindAddr, this, getPolicyProvider());
    {
          this.zkfc = zkfc;
  			// 使用protocol buffer序列化
  			RPC.setProtocolEngine(conf, ZKFCProtocolPB.class,
  			    ProtobufRpcEngine.class);
  			ZKFCProtocolServerSideTranslatorPB translator =
  			    new ZKFCProtocolServerSideTranslatorPB(this);
  			BlockingService service = ZKFCProtocolService
  			    .newReflectiveBlockingService(translator);
  			// 使用hadoop rpc接口得到rpc server
  			// ZKFCProtocol是rpc协议，service是rpc协议的实现类
  			// ZKFCProtocolPB是protobuf rpc接口的一个过渡类
  			this.server = new RPC.Builder(conf).setProtocol(ZKFCProtocolPB.class)
  			    .setInstance(service).setBindAddress(bindAddr.getHostName())
  			    .setPort(bindAddr.getPort()).setNumHandlers(HANDLER_COUNT)
  			    .setVerbose(false).build();
    }
}
initHM();
startRPC();
mainLoop();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;WatcherWithClientRef 在构造zk时被注册为默认watcher，主要监听连接或者断开事件。当调用initZk之后，watcher.process会对事件进行处理，连接、断开、过期的状态类型都是EventType.None。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>yarn-api使用</title>
      <link>https://dengziming.github.io/post/hadoop/yarn-api%E4%BD%BF%E7%94%A8/</link>
      <pubDate>Wed, 23 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/hadoop/yarn-api%E4%BD%BF%E7%94%A8/</guid>
      
        <description>

&lt;p&gt;参考： &lt;a href=&#34;https://ieevee.com/tech/2015/05/05/yarn-dist-shell.html&#34;&gt;https://ieevee.com/tech/2015/05/05/yarn-dist-shell.html&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;distributeshell&#34;&gt;distributeshell&lt;/h1&gt;

&lt;h2 id=&#34;client解析&#34;&gt;Client解析&lt;/h2&gt;

&lt;p&gt;distShell主要有2个类组成，Client和ApplicationMaster。两个类都带有main入口。Client的主要工作是启动AM，真正要做的任务由AM来调度。 Client的简化框架如下。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public static void main(String[] args) {
    boolean result = false;
    try {
      Client client = new Client();  //1 创建Client对象
      try {
        boolean doRun = client.init(args);  //2 初始化
        if (!doRun) {
          System.exit(0);
        }
      }
      result = client.run();   //3 运行
    }
    if (result) {
      System.exit(0);
    }
    System.exit(2);
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;1-创建client对象&#34;&gt;1 创建Client对象&lt;/h3&gt;

&lt;p&gt;创建时会指定本Client要用到的AM。 创建yarnClient。yarn将client与RM的交互抽象出了编程库YarnClient，用以应用程序提交、状态查询和控制等，简化应用程序。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;  public Client(Configuration conf) throws Exception  {
    this(		//指定AM
      &amp;quot;org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster&amp;quot;,
      conf);
  Client(String appMasterMainClass, Configuration conf) {
    this.conf = conf;
    this.appMasterMainClass = appMasterMainClass;
    yarnClient = YarnClient.createYarnClient();		//创建yarnClient
    yarnClient.init(conf);
    opts = new Options();	//创建opts，后面解析参数的时候用
    opts.addOption(&amp;quot;appname&amp;quot;, true, &amp;quot;Application Name. Default value - DistributedShell&amp;quot;);
    opts.addOption(&amp;quot;priority&amp;quot;, true, &amp;quot;Application Priority. Default 0&amp;quot;);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;2-初始化&#34;&gt;2 初始化&lt;/h3&gt;

&lt;p&gt;init会解析命令行传入的参数，例如使用的jar包、内存大小、cpu个数等。 代码里使用GnuParser解析：init时定义所有的参数opts（可以认为是一个模板），
然后将opts和实际的args传入解析后得到一个CommnadLine对象，后面查询选项直接操作该CommnadLine对象即可，如cliParser.hasOption(&amp;ldquo;help&amp;rdquo;)和cliParser.getOptionValue(&amp;ldquo;jar&amp;rdquo;)。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt; public boolean init(String[] args) throws ParseException {
    CommandLine cliParser = new GnuParser().parse(opts, args);
    amMemory = Integer.parseInt(cliParser.getOptionValue(&amp;quot;master_memory&amp;quot;, &amp;quot;10&amp;quot;));
    amVCores = Integer.parseInt(cliParser.getOptionValue(&amp;quot;master_vcores&amp;quot;, &amp;quot;1&amp;quot;));
    shellCommand = cliParser.getOptionValue(&amp;quot;shell_command&amp;quot;);
    appMasterJar = cliParser.getOptionValue(&amp;quot;jar&amp;quot;);
    ...
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;3-运行&#34;&gt;3 运行&lt;/h3&gt;

&lt;p&gt;先启动yarnClient，会建立跟RM的RPC连接，之后就跟调用本地方法一样。通过此yarnClient查询NM个数、NM详细信息（ID/地址/Container个数等）、Queue info（其实没用到，示例里只是打印了下调试用）。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;
public class Client {
  public boolean run() throws IOException, YarnException {
    yarnClient.start();
    YarnClusterMetrics clusterMetrics = yarnClient.getYarnClusterMetrics();
    List&amp;lt;NodeReport&amp;gt; clusterNodeReports = yarnClient.getNodeReports(
收集提交AM所需的信息。
    YarnClientApplication app = yarnClient.createApplication();	//创建app
    GetNewApplicationResponse appResponse = app.getNewApplicationResponse();
...
    ApplicationSubmissionContext appContext = app.getApplicationSubmissionContext();
    //AM需要的本地资源，如jar包、log文件
    Map&amp;lt;String, LocalResource&amp;gt; localResources = new HashMap&amp;lt;String, LocalResource&amp;gt;();

    FileSystem fs = FileSystem.get(conf);
    addToLocalResources(fs, appMasterJar, appMasterJarPath, appId.toString(),
        localResources, null);
    ...	//添加localResource

    vargs.add(Environment.JAVA_HOME.$$() + &amp;quot;/bin/java&amp;quot;);
    vargs.add(&amp;quot;-Xmx&amp;quot; + amMemory + &amp;quot;m&amp;quot;);
    vargs.add(appMasterMainClass);
...
    for (CharSequence str : vargs) {
      command.append(str).append(&amp;quot; &amp;quot;);	//重新组织命令行
    }
	//创建Container加载上下文，包含本地资源，环境变量，实际命令。
    ContainerLaunchContext amContainer = ContainerLaunchContext.newInstance(
      localResources, env, commands, null, null, null);

    Resource capability = Resource.newInstance(amMemory, amVCores);
    appContext.setResource(capability);		//请求使用的内存、cpu

    appContext.setAMContainerSpec(amContainer);
    appContext.setQueue(amQueue);

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;重新组织出来的commands如下：&lt;/p&gt;

&lt;p&gt;$JAVA_HOME/bin/java -Xmx10m org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster &amp;ndash;container_memory 10
提交AM（即appContext），并启动监控。 Client只关心自己提交到RM的AM是否正常运行，而AM内部的多个task，由AM管理。如果Client要查询应用程序的任务信息，需要自己设计与AM的交互。
    yarnClient.submitApplication(appContext);   //客户端提交AM到RM
    return monitorApplication(appId);
总的来说，Client做的事情比较简单，即建立与RM的连接，提交AM，监控AM运行状态。&lt;/p&gt;

&lt;p&gt;有个疑问，走读代码没有看到jar包是怎么送到NM上去的。&lt;/p&gt;

&lt;h2 id=&#34;application-master解析&#34;&gt;Application Master解析&lt;/h2&gt;

&lt;p&gt;AM简化框架如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;

      boolean doRun = appMaster.init(args);
      if (!doRun) {
        System.exit(0);
      }
      appMaster.run();
      result = appMaster.finish();
// yarn抽象了两个编程库，AMRMClient和NMClient(AM和RM都可以用)，简化AM编程。

// 1 设置RM、NM消息的异步处理方法
    AMRMClientAsync.CallbackHandler allocListener = new RMCallbackHandler();
    amRMClient = AMRMClientAsync.createAMRMClientAsync(1000, allocListener);
    amRMClient.init(conf);
    amRMClient.start();

    containerListener = createNMCallbackHandler();
    nmClientAsync = new NMClientAsyncImpl(containerListener);
    nmClientAsync.init(conf);
    nmClientAsync.start();
// 2 向RM注册
    RegisterApplicationMasterResponse response = amRMClient.registerApplicationMaster(appMasterHostname,
        appMasterRpcPort, appMasterTrackingUrl);
// 3 计算需要的Container，向RM发起请求
    // Setup ask for containers from RM
    // Send request for containers to RM
    // Until we get our fully allocated quota, we keep on polling RM for
    // containers
    // Keep looping until all the containers are launched and shell script
    // executed on them ( regardless of success/failure).
    for (int i = 0; i &amp;lt; numTotalContainersToRequest; ++i) {
      ContainerRequest containerAsk = setupContainerAskForRM();
      amRMClient.addContainerRequest(containerAsk);		//请求指定个数的Container
    }

  private ContainerRequest setupContainerAskForRM() {
    Resource capability = Resource.newInstance(containerMemory,
      containerVirtualCores);		//指定需要的memory/cpu能力
    ContainerRequest request = new ContainerRequest(capability, null, null,
        pri);


4 // RM分配Container给AM，AM启动任务RMCallbackHandler RM消息的响应，由RMCallbackHandler处理。示例中主要对前两种消息进行了处理。

  private class RMCallbackHandler implements AMRMClientAsync.CallbackHandler {
    //处理消息：Container执行完毕。在RM返回的心跳应答中携带。如果心跳应答中有已完成和新分配两种Container，先处理已完成
    public void onContainersCompleted(List&amp;lt;ContainerStatus&amp;gt; completedContainers) {
...
    //处理消息：RM新分配Container。在RM返回的心跳应答中携带
    public void onContainersAllocated(List&amp;lt;Container&amp;gt; allocatedContainers) {

    public void onShutdownRequest() {done = true;}

    //节点状态变化
    public void onNodesUpdated(List&amp;lt;NodeReport&amp;gt; updatedNodes) {}

    public float getProgress() {
onContainersAllocated收到分配的Container之后，会提交任务到NM。

    public void onContainersAllocated(List&amp;lt;Container&amp;gt; allocatedContainers) {
        LaunchContainerRunnable runnableLaunchContainer =   //创建runnable容器
            new LaunchContainerRunnable(allocatedContainer, containerListener);
        Thread launchThread = new Thread(runnableLaunchContainer);	//新建线程

        // launch and start the container on a separate thread to keep
        // the main thread unblocked
        // as all containers may not be allocated at one go.
        launchThreads.add(launchThread);
        launchThread.start();	//线程中提交Container到NM，不影响主流程

//简单分析下LaunchContainerRunnable。该类实现自Runnable，其run方法准备任务命令（本例即为date）。

  private class LaunchContainerRunnable implements Runnable {
    public LaunchContainerRunnable(
        Container lcontainer, NMCallbackHandler containerListener) {
      this.container = lcontainer;		//创建时记录待使用的Container
      this.containerListener = containerListener;
    }
    public void run() {
      vargs.add(shellCommand);		//待执行的shell命令
      vargs.add(shellArgs);			//shell命令参数
      List&amp;lt;String&amp;gt; commands = new ArrayList&amp;lt;String&amp;gt;();
      commands.add(command.toString());	//转为commands

      //根据命令、环境变量、本地资源等创建Container加载上下文
      ContainerLaunchContext ctx = ContainerLaunchContext.newInstance(
              localResources, shellEnv, commands, null, allTokens.duplicate(), null);
      containerListener.addContainer(container.getId(), container);
      //异步启动Container
      nmClientAsync.startContainerAsync(container, ctx);
// onContainersCompleted的功能比较简单，收到Container执行完毕的消息，检查其执行结果，如果执行失败，则重新发起请求，直到全部完成。

// NMCallbackHandler NM消息的响应，由NMCallbackHandler处理。

//在distShell示例里，回调句柄对NM通知过来的各种事件的处理比较简单，只是修改AM维护的Container执行完成、失败的个数。这样等到有Container执行完毕后，可以重启发起请求。失败处理和上面Container执行完毕消息的处理类似，达到了上面问题里所说的loopback效果。

  static class NMCallbackHandler
    implements NMClientAsync.CallbackHandler {

    @Override
    public void onContainerStopped(ContainerId containerId) {

    @Override
    public void onContainerStatusReceived(ContainerId containerId,

    @Override
    public void onContainerStarted(ContainerId containerId,
...
总的来说，AM做的事就是向RM/NM注册回调函数，然后请求Container；得到Container后提交任务，并跟踪这些任务的执行情况，如果失败了则重新提交，直到全部任务完成。
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;unmanagedam&#34;&gt;UnmanagedAM&lt;/h1&gt;

&lt;p&gt;distShell的Client提交AM到RM后，由RM将AM分配到某一个NM上的Container，这样给AM调试带来了困难。yarn提供了一个参数，Client可以设置为Unmanaged，提交AM后，会在客户端本地起一个单独的进程来运行AM。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;
public class UnmanagedAMLauncher {
  public void launchAM(ApplicationAttemptId attemptId)
    //创建新进程
    Process amProc = Runtime.getRuntime().exec(amCmd, envAMList.toArray(envAM));
    try {
      int exitCode = amProc.waitFor();  //等待AM进程结束
    } finally {
      amCompleted = true;
    }

  public boolean run() throws IOException, YarnException {
      appContext.setUnmanagedAM(true);		//设置为Unmanaged
      rmClient.submitApplication(appContext);	//提交AM

      ApplicationReport appReport =		//监控AM状态，如果状态变为ACCEPTED，则跳出循环，launchAM。
          monitorApplication(appId, EnumSet.of(YarnApplicationState.ACCEPTED,
            YarnApplicationState.KILLED, YarnApplicationState.FAILED,
            YarnApplicationState.FINISHED));

      if (appReport.getYarnApplicationState() == YarnApplicationState.ACCEPTED) {
        launchAM(attemptId);
&lt;/code&gt;&lt;/pre&gt;
</description>
      
    </item>
    
    <item>
      <title>hdfs-client</title>
      <link>https://dengziming.github.io/post/hadoop/hdfs-client/</link>
      <pubDate>Tue, 22 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/hadoop/hdfs-client/</guid>
      
        <description>

&lt;p&gt;参考资料：&lt;/p&gt;

&lt;p&gt;hadoop 技术内幕丛书&lt;/p&gt;

&lt;h1 id=&#34;写&#34;&gt;写&lt;/h1&gt;

&lt;h1 id=&#34;创建流&#34;&gt;创建流&lt;/h1&gt;

&lt;p&gt;简单写一个 demo 进行测试，通过打断点方法，另外发现一个问题， 在 dfsClient.namenode.create 打断点调试会报错，可能是因为动态代理卡主了 ：&lt;/p&gt;

&lt;p&gt;注意我们上传的文件 nio-data.txt 内容可以进行控制，例如我们写 600 个 a(97)，转换为 DataOutputStream 后的 byte[] 就是 600个97 ，这样调试就知道是哪个数据，600 a 是因为每个chunk的默认大小是 512&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public static void main(String[] args) throws IOException {

    FileSystem fs = FileSystem.get(new Configuration());
    
    Path src = new Path(&amp;quot;ideaspace/learn-jvm/src/main/resources/data/nio-data.txt&amp;quot;); //文件里面是一个 java 代码
    Path desc = new Path(&amp;quot;/tmp/&amp;quot;);
    if (fs.exists(desc)){
            fs.delete(desc,true);
        }

    fs.copyFromLocalFile(src,desc);

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;经过一系列的调用后进入的第一个关键方法是 ：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;@Override
  public FSDataOutputStream create(final Path f, final FsPermission permission,
    final EnumSet&amp;lt;CreateFlag&amp;gt; cflags, final int bufferSize,
    final short replication, final long blockSize, final Progressable progress,
    final ChecksumOpt checksumOpt) throws IOException {
    statistics.incrementWriteOps(1);
    Path absF = fixRelativePart(f);
    return new FileSystemLinkResolver&amp;lt;FSDataOutputStream&amp;gt;() {
      @Override
      public FSDataOutputStream doCall(final Path p)
          throws IOException, UnresolvedLinkException {
        final DFSOutputStream dfsos = dfs.create(getPathName(p), permission,
                cflags, replication, blockSize, progress, bufferSize,
                checksumOpt);
        return dfs.createWrappedOutputStream(dfsos, statistics);
      }
      @Override
      public FSDataOutputStream next(final FileSystem fs, final Path p)
          throws IOException {
        return fs.create(p, permission, cflags, bufferSize,
            replication, blockSize, progress, checksumOpt);
      }
    }.resolve(this, absF);
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;1-create&#34;&gt;1.create&lt;/h1&gt;

&lt;p&gt;首先是 create，然后是 dfs.createWrappedOutputStream(out, statistics);&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public DFSOutputStream create(String src, 
                           FsPermission permission,
                           EnumSet&amp;lt;CreateFlag&amp;gt; flag, 
                           boolean createParent,
                           short replication,
                           long blockSize,
                           Progressable progress,
                           int buffersize,
                           ChecksumOpt checksumOpt,
                           InetSocketAddress[] favoredNodes) throws IOException {
  checkOpen();
  if (permission == null) {
    permission = FsPermission.getFileDefault();
  }
  FsPermission masked = permission.applyUMask(dfsClientConf.uMask);
  if(LOG.isDebugEnabled()) {
    LOG.debug(src + &amp;quot;: masked=&amp;quot; + masked);
  }
  String[] favoredNodeStrs = null;
  if (favoredNodes != null) {
    favoredNodeStrs = new String[favoredNodes.length];
    for (int i = 0; i &amp;lt; favoredNodes.length; i++) {
      favoredNodeStrs[i] = 
          favoredNodes[i].getHostName() + &amp;quot;:&amp;quot; 
                       + favoredNodes[i].getPort();
    }
  }
  final DFSOutputStream result = DFSOutputStream.newStreamForCreate(this,
      src, masked, flag, createParent, replication, blockSize, progress,
      buffersize, dfsClientConf.createChecksum(checksumOpt),
      favoredNodeStrs);
  beginFileLease(result.getFileId(), result);
  return result;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;两个方法比较关键：&lt;/p&gt;

&lt;p&gt;DFSOutputStream.newStreamForCreate 和 beginFileLease(result.getFileId(), result)&lt;/p&gt;

&lt;h2 id=&#34;1-newstreamforcreate-方法是第一次创建真正的-流-类是-dfsoutputstream&#34;&gt;1. newStreamForCreate 方法是第一次创建真正的 流，类是 DFSOutputStream&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;static DFSOutputStream newStreamForCreate(DFSClient dfsClient, String src,
      FsPermission masked, EnumSet&amp;lt;CreateFlag&amp;gt; flag, boolean createParent,
      short replication, long blockSize, Progressable progress, int buffersize,
      DataChecksum checksum, String[] favoredNodes) throws IOException {
    ...
    while (shouldRetry) {
      shouldRetry = false;
      try {
        stat = dfsClient.namenode.create(src, masked, dfsClient.clientName,
            new EnumSetWritable&amp;lt;CreateFlag&amp;gt;(flag), createParent, replication,
            blockSize, SUPPORTED_CRYPTO_VERSIONS);
        break;
      } catch (RemoteException re) {...}
    Preconditions.checkNotNull(stat, &amp;quot;HdfsFileStatus should not be null!&amp;quot;);
    final DFSOutputStream out = new DFSOutputStream(dfsClient, src, stat,
        flag, progress, checksum, favoredNodes);
    out.start();
    return out;
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;rpc
这部分没法调试，因为在远程。只能自己观看
通过 RPC 调用 NameNodeRpcServer.create -&amp;gt; namesystem.startFile -&amp;gt; startFileInt -&amp;gt; startFileInternal ，先跳过，&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;new DFSOutputStream(dfsClient, src, stat,flag, progress, checksum, favoredNodes);&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;/** Construct a new output stream for creating a file. */
  private DFSOutputStream(DFSClient dfsClient, String src, HdfsFileStatus stat,
      EnumSet&amp;lt;CreateFlag&amp;gt; flag, Progressable progress,
      DataChecksum checksum, String[] favoredNodes) throws IOException {
    this(dfsClient, src, progress, stat, checksum);
    this.shouldSyncBlock = flag.contains(CreateFlag.SYNC_BLOCK);

    computePacketChunkSize(dfsClient.getConf().writePacketSize, bytesPerChecksum);

    Span traceSpan = null;
    if (Trace.isTracing()) {
      traceSpan = Trace.startSpan(this.getClass().getSimpleName()).detach();
    }
    streamer = new DataStreamer(stat, traceSpan);
    if (favoredNodes != null &amp;amp;&amp;amp; favoredNodes.length != 0) {
      streamer.setFavoredNodes(favoredNodes);
    }
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;新建 DFSOutputStream 中有个重要的线程 DataStreamer，功能后续研究。
DFSOutputStream 中的成员变量我们可以好好看看，什么是 checksum，chunk，packet。另外它的父类 FSOutputSummer 也很重要。&lt;/p&gt;

&lt;h2 id=&#34;2-beginfilelease-这个可以暂时忽略-后面专门研究-lease&#34;&gt;2. beginFileLease 这个可以暂时忽略，后面专门研究 lease&lt;/h2&gt;

&lt;p&gt;dfs.createWrappedOutputStream(dfsos, statistics) 对上面创建的流就行一个 包装&lt;/p&gt;

&lt;p&gt;返回 return new HdfsDataOutputStream(dfsos, statistics, startPos);&lt;/p&gt;

&lt;p&gt;至此创建流的过程就完成了。我们大概回顾一下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;DistributedFileSystem.create(final Path f, final FsPermission permission,
final EnumSet&amp;lt;CreateFlag&amp;gt; cflags, final int bufferSize,
    final short replication, final long blockSize, final Progressable progress,
    final ChecksumOpt checksumOpt)
{
    // 1. 
    DFSOutputStream dfsos = DfsClient.create(getPathName(p), permission,
                cflags, replication, blockSize, progress, bufferSize,
                checksumOpt)
    {
        // 1. 
        DFSOutputStream.newStreamForCreate(this,
        	src, masked, flag, createParent, replication, blockSize, progress,
        	buffersize, dfsClientConf.createChecksum(checksumOpt),
        	favoredNodeStrs);
        {
            // 1.
            stat = dfsClient.namenode.create(src, masked, dfsClient.clientName,
                new EnumSetWritable&amp;lt;CreateFlag&amp;gt;(flag), createParent, replication,
                blockSize, SUPPORTED_CRYPTO_VERSIONS);
            {
                // RPC
            }
            // 2.
            final DFSOutputStream out = new DFSOutputStream(dfsClient, src, stat,
            flag, progress, checksum, favoredNodes);
            {
                // 
                class DataStreamer
                class Patket
                streamer = new DataStreamer(stat, traceSpan);
            }
            
        }
        // 2.
        beginFileLease(result.getFileId(), result);
    }
                
    // 2. 
    return dfs.createWrappedOutputStream(dfsos, statistics);
    {
        return new HdfsDataOutputStream(dfsos, statistics, startPos);
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;首先是 DistributedFileSystem 创建，然后调用 DfsClient 的 create ，DfsClient需要创建流和lease，创建流 由 DFSOutputStream 完成，
DFSOutputStream 需要分别和namenode、datanode通信。DFSOutputStream 内部有 Packet 和 DataStreamer，继承自 FSOutputSummer ，FSOutputSummer 完成了write的真正逻辑&lt;/p&gt;

&lt;h1 id=&#34;2-out-write-buf-0-bytesread&#34;&gt;2. out.write(buf, 0, bytesRead)&lt;/h1&gt;

&lt;p&gt;创建完成后就是写数据，HdfsDataOutputStream 写数据比较复杂，先写到缓存，然后发送。需要做 checksum 检验，然后做成一个 chuck，然后将多个 chuck 合成一个 Packet，然后发送 Packet。&lt;/p&gt;

&lt;p&gt;FSDataOutputStream.out.write(byte[])
调用过程：
out.write(bytes) -&amp;gt; FilterOutputStream.write -&amp;gt; DataOutputStream.write -&amp;gt; out.write(byte[], off, len) -&amp;gt; FSOutputSummer.write(byte b[], int off, int len)&lt;/p&gt;

&lt;p&gt;FSOutputSummer.write&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public synchronized void write(byte b[], int off, int len)
      throws IOException {
    
    checkClosed();
    
    if (off &amp;lt; 0 || len &amp;lt; 0 || off &amp;gt; b.length - len) {
      throw new ArrayIndexOutOfBoundsException();
    }
    // 循环调用 write ，每次写入 #write1() 长度
    for (int n=0;n&amp;lt;len;n+=write1(b, off+n, len-n)) {
    }
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个 byte b[] 就是我们的数据流，通过断点我们可以看到是600个97，也就是600个a。&lt;/p&gt;

&lt;p&gt;write1，这里有几个比较核心的内容，如果写入长度比较大，直接写入流，如果写入比较少，先写到 Buffer，到达一定长度再统一进行写到流。这么做是为了减少拷贝&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;private int write1(byte b[], int off, int len) throws IOException {
  
  // 写入长度大于本地buf的长度时，直接写入本地buf的长度。
  if(count==0 &amp;amp;&amp;amp; len&amp;gt;=buf.length) {
    // local buffer is empty and user buffer size &amp;gt;= local buffer size, so
    // simply checksum the user buffer and send it directly to the underlying
    // stream
    final int length = buf.length;
    writeChecksumChunks(b, off, length);
    return length;
  }
  // 当len小于本地buf的长度时，先写入buf，当buf写满之后，flushBuffer
  // copy user data to local buffer
  
  int bytesToCopy = buf.length-count; // 这个 count 代表以及复制的数据长度，第一次是 0
  bytesToCopy = (len&amp;lt;bytesToCopy) ? len : bytesToCopy;  // 这时候就是要复制的数据长度，600
  System.arraycopy(b, off, buf, count, bytesToCopy);
  count += bytesToCopy;
  if (count == buf.length) {
    // local buffer is full
    flushBuffer();
  } 
  return bytesToCopy;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，写入数据大的话，直接调用 writeChecksumChunks 将buf长度大小的数据生成 chunksum ，
（chunksum 是检查数据完整性的，相关知识可以查看计算机网络。）并写入 packet中。如果写入数据比较少，直接放进 buffer，等待buffer比较大，再统一flush&lt;/p&gt;

&lt;p&gt;数据写完了关闭流的时候会再调用一次 fulshBuffer ,会调用 writeChecksumChunks&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;private void writeChecksumChunks(byte b[], int off, int len)
throws IOException {
  // 计算checksum
  sum.calculateChunkedSums(b, off, len, checksum, 0);
  for (int i = 0; i &amp;lt; len; i += sum.getBytesPerChecksum()) {
    int chunkLen = Math.min(sum.getBytesPerChecksum(), len - i);
    int ckOffset = i / sum.getBytesPerChecksum() * getChecksumSize();
    // 一个chunk一个chunk的写入packet
    writeChunk(b, off + i, chunkLen, checksum, ckOffset, getChecksumSize());
  }

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;sum.calculateChunkedSums 计算校验值，计算完以后b还是 600个97，off和len分别是 0和600，checksum是一个36位的数组，
但是只有前八位有值：0 = 111,1 = 50,2 = -90,3 = 31,4 = -99,5 = 97,6 = -69,7 = 102。因为每512位生成4个校验码。现在是600位，需要8个。
然后写出 chunk，chunk的长度为： 512，所以这里会调用两次 writeChunk。&lt;/p&gt;

&lt;p&gt;writeChunk 先将 chunk 写入 currentPacket 中，当currentPacket写满之后调用 waitAndQueueCurrentPacket，
将packet放入dataQueue队列，等待DataStreamer线程将packet写入pipeline中，整个block发送完毕之后将发送一个空的packet。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;writeChunk{}
currentPacket.writeChecksum(checksum, ckoff, cklen);
currentPacket.writeData(b, offset, len);
currentPacket.numChunks++;
bytesCurBlock += len;
waitAndQueueCurrentPacket()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们把这段代码跑两遍以后，去看看生成的Packet，里面有4个属性：checksumStart = 33,checksumPos = 41,dataStart = 541,dataPos = 1141&lt;/p&gt;

&lt;p&gt;可以看出这几个的意思分别是 check 的开始位置和结束为止，data的开始位置和结束为止，之所以留了 33个位置，是 Packet 的 header。然后还有一个 buffer数组，里面就是按照检查数据和数据。&lt;/p&gt;

&lt;p&gt;这时候 Packet 的结构我们已经一清二楚了。&lt;/p&gt;

&lt;p&gt;相关详细内容可以继续深入查看源码。waitAndQueueCurrentPacket() 可以看具体代码，这时候已经报连接超时异常了。接下来我们调试发送数据到 DataNode。&lt;/p&gt;

&lt;h1 id=&#34;三-dfsoutputstream-datastreamer-发送-packet&#34;&gt;三.DFSOutputStream.DataStreamer 发送 packet&lt;/h1&gt;

&lt;p&gt;上面已经调试到：waitAndQueueCurrentPacket() ,会将 Packet 发送给 DFSOutputStream.DataStreamer&lt;/p&gt;

&lt;p&gt;DFSOutputStream.DataStreamer 是一个线程，里面有个 stage 标识到了哪一步。还有一个 response 用来回复消息。&lt;/p&gt;

&lt;p&gt;DataStreamer 会从 dataQueue 中拿出 packet 发送到 pipeline , 相关代码很长。取出部分：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;synchronized (dataQueue) {

	 // 发送packet，dataQueue为null，则发送一个心跳
	 if (dataQueue.isEmpty()) {
	   one = createHeartbeatPacket();
	 } else {
	   one = dataQueue.getFirst(); // regular data packet
	 }
	}
	
        // 建立pipeline
        setPipeline(nextBlockOutputStream());
        // 启动ResponseProcessor线程，更新DataStreamer的状态为DATA_STREAMING

// 当前packet是block的最后一个packet，等待接收之前所有packet的ack
      if (one.lastPacketInBlock) {
        // wait for all data packets have been successfully acked
        synchronized (dataQueue) {
          while (!streamerClosed &amp;amp;&amp;amp; !hasError &amp;amp;&amp;amp; 
              ackQueue.size() != 0 &amp;amp;&amp;amp; dfsClient.clientRunning) {
            try {
              // wait for acks to arrive from datanodes
              dataQueue.wait(1000);
            } catch (InterruptedException  e) {
              DFSClient.LOG.warn(&amp;quot;Caught exception &amp;quot;, e);
            }
          }
        }
        if (streamerClosed || hasError || !dfsClient.clientRunning) {
          continue;
        }
        stage = BlockConstructionStage.PIPELINE_CLOSE;
      }
       
     
     // 将 packet 从 dataQueue 移到 ackQueue，准备发送packet
      synchronized (dataQueue) {
        // move packet from dataQueue to ackQueue
        if (!one.isHeartbeatPacket()) {
          dataQueue.removeFirst();
          ackQueue.addLast(one);
          dataQueue.notifyAll();
        }
      }
     // 将packet写入pipeline
        one.writeTo(blockStream);
        blockStream.flush();   
    
      // 如果当前packet是最后一个，则继续等待此packet的ack，
      // 然后endBlock
    if (one.lastPacketInBlock) {
        // wait for the close packet has been acked
        synchronized (dataQueue) {
          while (!streamerClosed &amp;amp;&amp;amp; !hasError &amp;amp;&amp;amp; 
              ackQueue.size() != 0 &amp;amp;&amp;amp; dfsClient.clientRunning) {
            dataQueue.wait(1000);// wait for acks to arrive from datanodes
          }
        }
        if (streamerClosed || hasError || !dfsClient.clientRunning) {
          continue;
        }
        endBlock();
      }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;代码很长也比较杂乱，我们主要看一下 setPipeline(nextBlockOutputStream()); 和 one.writeTo(blockStream);blockStream.flush();  等&lt;/p&gt;

&lt;p&gt;第一步是：one = dataQueue.getFirst();&lt;br /&gt;
我们看看取到的 Packet 是什么样子：和我们上面发送的一样，buf=[0,0,0(33个0),11,50..(6个检测码),0,0,(很多0)，97,97,97(600个97)，],lastPacketInBlock=false &amp;hellip;&lt;/p&gt;

&lt;p&gt;nextBlockOutputStream 是通过 namenode 得到一个 LocatedBlock ，而 setPipeline 是和 Datanode 的通道。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;nextBlockOutputStream(){
lb = locateFollowingBlock(startTime,excluded.length &amp;gt; 0 ? excluded : null); 
-- dfsClient.namenode.addBlock(src, dfsClient.clientName,block, excludedNodes, fileId, favoredNodes);

success = createBlockOutputStream(nodes, storageTypes, 0L, false);

blockStream = out; //给写出流赋值。
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;createBlockOutputStream 对于我们第一次调试来说，关注三个就够了。
创建连接： /到 datanode 的 socket连接 Socket[addr=/127.0.0.1,port=50010,localport=58006]，
new Sender(out).writeBlock 建立 block&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;createBlockOutputStream{}
// 到 datanode 的 socket连接 Socket[addr=/127.0.0.1,port=50010,localport=58006]
s = createSocketForPipeline(nodes[0], nodes.length, dfsClient);
long writeTimeout = dfsClient.getDatanodeWriteTimeout(nodes.length);

OutputStream unbufOut = NetUtils.getOutputStream(s, writeTimeout);
InputStream unbufIn = NetUtils.getInputStream(s);
IOStreamPair saslStreams = dfsClient.saslClient.socketSend(s,
  unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);
unbufOut = saslStreams.out;
unbufIn = saslStreams.in;
out = new DataOutputStream(new BufferedOutputStream(unbufOut,
    HdfsConstants.SMALL_BUFFER_SIZE));
    
-- 发送写block请求，
new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,
dfsClient.clientName, nodes, nodeStorageTypes, null, bcs, 
nodes.length, block.getNumBytes(), bytesSent, newGS,
checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile);

blockStream = out; //给写出流赋值。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;回到 run 方法，通过 initDataStreaming ，然后调用 one.writeTo(blockStream); 这个 blockStream 上面已经赋值了，就是通过 socket 建立 的连接。这里只是将packet写入pipeline中的第一个dn。&lt;/p&gt;

&lt;p&gt;看看 Packet 的 writeTo 方法，Packet 的成员变量上面写了，是检查点位置，检查长度，数据位置，数据长度，等。&lt;/p&gt;

&lt;p&gt;主要步骤：
- 计算 pktLen 头长度+检查长度+数据长度
- 新建Header，包括 packet 是否是最后一个packet的信息等。
- 判断 checksumPos != dataStart 不等于需要删掉中间的空缺数据，&lt;code&gt;System.arraycopy(buf, checksumStart, buf, dataStart - checksumLen , checksumLen)&lt;/code&gt;
这个是因为 packet 容量是好几万，我们假设 10000，能容纳的 chunk 是 20个左右，所以应该有 20 * 4 = 80 的检查数据。所以前80个位置留给了检查位置，数据从81开始写。
但是如果我们没写满一个 Packet，检查数据就不需要80个，数据也是从 81开始写，这就空缺了一些数据。
- 将 header.getBytes() [0 = 0,1 = 0,2 = 2,3 = 100,4 = 0,5 = 25 &amp;hellip;.(一共33个值)] 的值放到 Packet 的头部。
- stm.write(buf, headerStart, header.getSerializedSize() + checksumLen + dataLen);&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;void writeTo(DataOutputStream stm) throws IOException {
  
  final int dataLen = dataPos - dataStart;  1411 - 541
  final int checksumLen = checksumPos - checksumStart; 
  final int pktLen = HdfsConstants.BYTES_IN_INTEGER + dataLen + checksumLen;
  PacketHeader header = new PacketHeader(
    pktLen, offsetInBlock, seqno, lastPacketInBlock, dataLen, syncBlock);
    
  // checksumPos不等于dataStart时，将checksum移动到data前面，
  // 紧挨着data，为header空出足够的空间
  if (checksumPos != dataStart) {
    // Move the checksum to cover the gap. This can happen for the last
    // packet or during an hflush/hsync call.
    System.arraycopy(buf, checksumStart, buf, 
                     dataStart - checksumLen , checksumLen); 
    checksumPos = dataStart;
    checksumStart = checksumPos - checksumLen;
  }
  
  final int headerStart = checksumStart - header.getSerializedSize();
  assert checksumStart + 1 &amp;gt;= header.getSerializedSize();
  assert checksumPos == dataStart;
  assert headerStart &amp;gt;= 0;
  assert headerStart + header.getSerializedSize() == checksumStart;
  
  // Copy the header data into the buffer immediately preceding the checksum
  // data.
  
  // 将header复制到packet的buf中，组成一个完整的packet
  System.arraycopy(header.getBytes(), 0, buf, headerStart,
      header.getSerializedSize());
  
  // corrupt the data for testing.
  if (DFSClientFaultInjector.get().corruptPacket()) {
    buf[headerStart+header.getSerializedSize() + checksumLen + dataLen-1] ^= 0xff;
  }
  // Write the now contiguous full packet to the output stream.
  // 将buf写入输出流中
  stm.write(buf, headerStart, header.getSerializedSize() + checksumLen + dataLen);
  
  // undo corruption.
  if (DFSClientFaultInjector.get().uncorruptPacket()) {
    buf[headerStart+header.getSerializedSize() + checksumLen + dataLen-1] ^= 0xff;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以看一下 最后的 stm.write， 通过跟踪我们发现最终调用： channel.write(buf);  是通过 NIO实现的。&lt;/p&gt;

&lt;p&gt;最后我们可以看到打印了错误信息： Exception in thread &amp;ldquo;main&amp;rdquo; java.io.IOException: All datanodes 127.0.0.1:50010 are bad. Aborting&amp;hellip;
这是因为我们调试时间长，连接已经断了。这个错误是哪一步打印的我们后续可以研究研究。&lt;/p&gt;

&lt;p&gt;其实这里有个疑问，这个 Packet 是最后一个 Packet，但是 它的 lastPacketInBlock=false？，其实这个确实不是最后一个，后面还要发送一个空的 Packet，只有 37个字节的头信息。&lt;/p&gt;

&lt;h1 id=&#34;四-dataxceiver-线程写入-datanode&#34;&gt;四、DataXceiver 线程写入 DataNode&lt;/h1&gt;

&lt;p&gt;注意：&lt;/p&gt;

&lt;p&gt;服务端的调试和客户端调试是反过来的，
需要先 stop-dfs.sh, 然后 &lt;code&gt;hadoop-daemons.sh start namenode&lt;/code&gt; &lt;code&gt;hadoop-daemons.sh start secondarynamenode&lt;/code&gt;  启动 namenode，然后我们在 IDEA 中运行 datanode
然后我们通过客户端命令: hadoop fs -copyFromLocal data.txt /tmp/data.txt
另外多线程调试不太方便，每次只能在一个线程打赏断点，否则会跳来跳去很麻烦，我们就在 DataXCerver 的 writeBlock 方法打上断点&lt;/p&gt;

&lt;p&gt;以上的流程可以看做是client端，client端将数据发送到dn上，由dn负责将packet写入本地磁盘，并向下一个dn发送。
DataXceiverServer 是在 DataNode 启动的线程，run 方法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public void run() {
  Peer peer = null;
  while (datanode.shouldRun &amp;amp;&amp;amp; !datanode.shutdownForUpgrade) {
    try {
      // 接收client的socket请求
      peer = peerServer.accept();
     
     // 一个 Daemon线程
      new Daemon(datanode.threadGroup,
          DataXceiver.create(peer, datanode, this))
          .start();
    } catch 
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里先循环接受请求，每次接受到一个就新建一个线程，加入线程组中。每个 DataXceiver 线程的 run 方法就是处理请求的：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;dataXceiverServer.addPeer(peer, Thread.currentThread(), this);
peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);
InputStream input = socketIn;
try {
  IOStreamPair saslStreams = datanode.saslServer.receive(peer, socketOut,
    socketIn, datanode.getXferAddress().getPort(),
    datanode.getDatanodeId());
  input = new BufferedInputStream(saslStreams.in,
    HdfsConstants.SMALL_BUFFER_SIZE);
  socketOut = saslStreams.out;
} catch 

super.initialize(new DataInputStream(input));

op = readOp();
processOp(op);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;processOp():&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;switch(op) {
    case READ_BLOCK:
      opReadBlock();
      break;
    case WRITE_BLOCK:
      opWriteBlock(in);
      break;

    default:
      throw new IOException(&amp;quot;Unknown op &amp;quot; + op + &amp;quot; in data stream&amp;quot;);
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;private void opWriteBlock(DataInputStream in) throws IOException {
    final OpWriteBlockProto proto = OpWriteBlockProto.parseFrom(vintPrefixed(in));
    final DatanodeInfo[] targets = PBHelper.convert(proto.getTargetsList());
    TraceScope traceScope = continueTraceSpan(proto.getHeader(),
        proto.getClass().getSimpleName());
    try {
      writeBlock(PBHelper.convert(proto.getHeader().getBaseHeader().getBlock()),
          PBHelper.convertStorageType(proto.getStorageType()),
          PBHelper.convert(proto.getHeader().getBaseHeader().getToken()),
          proto.getHeader().getClientName(),
          targets,
          PBHelper.convertStorageTypes(proto.getTargetStorageTypesList(), targets.length),
          PBHelper.convert(proto.getSource()),
          fromProto(proto.getStage()),
          proto.getPipelineSize(),
          proto.getMinBytesRcvd(), proto.getMaxBytesRcvd(),
          proto.getLatestGenerationStamp(),
          fromProto(proto.getRequestedChecksum()),
          (proto.hasCachingStrategy() ?
              getCachingStrategy(proto.getCachingStrategy()) :
            CachingStrategy.newDefaultStrategy()),
            (proto.hasAllowLazyPersist() ? proto.getAllowLazyPersist() : false));
     } finally {
      if (traceScope != null) traceScope.close();
     }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个方法有我们的断点，我们会在里面进行调试 writeBlock ，大致步骤为：&lt;/p&gt;

&lt;p&gt;新建 给客户端发确认消息的 replyOut 流，
新建 给其他DataNode 发消息的 mirror 流
新建 BlockReceiver  接收数据，接收到 Packet 后进行判断，如果不是最后一个，写到输出流，如果是最后一个或者长度为0，不做处理。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;
public void writeBlock(final ExtendedBlock block,
    final StorageType storageType, 
    final Token&amp;lt;BlockTokenIdentifier&amp;gt; blockToken,
    final String clientname,
    final DatanodeInfo[] targets,
    final StorageType[] targetStorageTypes, 
    final DatanodeInfo srcDataNode,
    final BlockConstructionStage stage,
    final int pipelineSize,
    final long minBytesRcvd,
    final long maxBytesRcvd,
    final long latestGenerationStamp,
    DataChecksum requestedChecksum,
    CachingStrategy cachingStrategy,
    final boolean allowLazyPersist) throws IOException {
    
    // 我们简单取一部分代码
    
    // 输出流，现在在 DataXCerver 中，输入流就是客户端写，输出流自然即时发送到客户端的
    final DataOutputStream replyOut = new DataOutputStream(
        new BufferedOutputStream(
            getOutputStream(),
            HdfsConstants.SMALL_BUFFER_SIZE));
    
    }
    
    // 这里一大堆的 mirrorOut 开头的流都是 数据写到 DataNode 后复制副本用的，由于在本地只有一个副本，所以都是 null 
    DataOutputStream mirrorOut = null;  // stream to next target
    DataInputStream mirrorIn = null;    // reply from next target
    Socket mirrorSock = null;           // socket to next target
    String mirrorNode = null;           // the name:port of next target
    String firstBadLink = &amp;quot;&amp;quot;;           // first datanode that failed in connection setup
    Status mirrorInStatus = SUCCESS;
    final String storageUuid;
    
    // 新建 BlockReceiver， BlockReceiver 的作用可以好好看注释，另外它的构造方法也有很大的信息量，其中有个 in 输入流，
    // 是在 DataXCerver 的 run 方法中：super.initialize(new DataInputStream(input)); 也就是对输入的 socket 流的二层包装
    blockReceiver = new BlockReceiver(block, storageType, in,
    peer.getRemoteAddressString(),
    peer.getLocalAddressString(),
    stage, latestGenerationStamp, minBytesRcvd, maxBytesRcvd,
    clientname, srcDataNode, datanode, requestedChecksum,
    cachingStrategy, allowLazyPersist)
    
    // 然后是一大堆的副本拷贝，我们本地调试线跳过。
    if (targets.length &amp;gt; 0) {// 跳过}
    
      // 给客户端发一个应答消息
      if (isClient &amp;amp;&amp;amp; !isTransfer) {
        if (LOG.isDebugEnabled() || mirrorInStatus != SUCCESS) {
          LOG.info(&amp;quot;Datanode &amp;quot; + targets.length +
                   &amp;quot; forwarding connect ack to upstream firstbadlink is &amp;quot; +
                   firstBadLink);
        }
        BlockOpResponseProto.newBuilder()
          .setStatus(mirrorInStatus)
          .setFirstBadLink(firstBadLink)
          .build()
          .writeDelimitedTo(replyOut);
        replyOut.flush();
      }
      
      // 这里开始写数据，
    blockReceiver.receiveBlock(mirrorOut, mirrorIn, replyOut,mirrorAddr, null, targets, false);
    
    // 后续处理
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们看一下 blockReceiver.receiveBlock&lt;/p&gt;

&lt;p&gt;new PacketResponder
receivePacket()&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;// 如果是来自客户端而且传输，新建回复的线程
if (isClient &amp;amp;&amp;amp; !isTransfer) {
        responder = new Daemon(datanode.threadGroup, 
            new PacketResponder(replyOut, mirrIn, downstreams));
        responder.start(); // start thread to processes responses
      }

// 循环写
while (receivePacket() &amp;gt;= 0) { /* R} 这里就是写所在逻辑了

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;receivePacket 方法：&lt;/p&gt;

&lt;p&gt;packetReceiver.receiveNextPacket(in); 接受一个 Packet&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;receivePacket{
packetReceiver.receiveNextPacket(in);

PacketHeader header = packetReceiver.getHeader();

// 这里把数据写到 副本上面
//First write the packet to the mirror:
    if (mirrorOut != null &amp;amp;&amp;amp; !mirrorError) {
      try {
        long begin = Time.monotonicNow();
        packetReceiver.mirrorPacketTo(mirrorOut);
        mirrorOut.flush();
        long duration = Time.monotonicNow() - begin;
        if (duration &amp;gt; datanodeSlowLogThresholdMs) {
          LOG.warn(&amp;quot;Slow BlockReceiver write packet to mirror took &amp;quot; + duration
              + &amp;quot;ms (threshold=&amp;quot; + datanodeSlowLogThresholdMs + &amp;quot;ms)&amp;quot;);
        }
      } catch (IOException e) {
        handleMirrorOutError(e);
      }
    }
    
    // 得到 数据和 检查数据
    ByteBuffer dataBuf = packetReceiver.getDataSlice();
    ByteBuffer checksumBuf = packetReceiver.getChecksumSlice();
    
    // 如果是最后一个 packet
    if (lastPacketInBlock || len == 0) {
      if(LOG.isDebugEnabled()) {
        LOG.debug(&amp;quot;Receiving an empty packet or the end of the block &amp;quot; + block);
      }
      // sync block if requested
      if (syncBlock) {
        flushOrSync(true);
      }
    } else {
        。。。。。
        out.write(dataBuf.array(), startByteToDisk, numBytesToDisk);
    }
    // 这里就行 check 很复杂，部分数据的 check 需要重新计算。
    checksumOut.write(buf);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里的代码太复杂，而且调试的时候稍微时间长一点就出现 连接异常。可以通过配置参数解决。
然后得到了 dataBuf 和 checksumBuf，如果不是最后一个，我们需要写到 DataNode 的具体数据中。
到这里整个写的操作就完成。中间的 和客户端心跳应答、失败处理。都没有涉及。简单梳理一下：&lt;/p&gt;

&lt;h1 id=&#34;五-pipelineack-和-packetresponder-信息处理&#34;&gt;五、PipeLineAck 和 PacketResponder 信息处理&lt;/h1&gt;

&lt;p&gt;上面的 one.writeTo(blockStream); 代码是将 Packet 写入到输出流，写出之前有一段代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt; // send the packet
 synchronized (dataQueue) {
   // move packet from dataQueue to ackQueue
   if (!one.isHeartbeatPacket()) {
     dataQueue.removeFirst();
     ackQueue.addLast(one);
     dataQueue.notifyAll();
   }
 }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就是将 Packet 放进 ackQueue 中，很明显这又是一个 消费者生产者。&lt;/p&gt;

&lt;p&gt;新建 DataStreamer 的run中，得到输出流，
建立 pipeLine 之后，会有 initDataStreaming 操作，还要启一个新的线程 ResponseProcessor 接收packet的ack，这个线程在initDataStreaming中启动，并更新DataStreamer线程的状态为DATA_STREAMING
ResponseProcessor，在 ResponseProcessor 的run方法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;synchronized (dataQueue) {
  one = ackQueue.getFirst();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意每次锁住的都是 dataQueue 对象。
与他对应的是 BlockReceiver 中的 PacketResponder 。 PacketResponder 的 run 方法比较长，主要是按照 packet 的写入顺序发送 ack。&lt;/p&gt;

&lt;p&gt;由于调试器的问题，这里一直跳不进去，能看到大概逻辑： 当数据节点顺利处理完数据，而且当前节点处在数据节点中间（收到下游DataNode的消息）
如果 ackQueue 中有数据，获取一个记录，接下来如果当前节点位于数据管道的中间，就在mirror流读取下游的确认，我们在本地调试是没有这一步的。&lt;/p&gt;

&lt;p&gt;如果当前节点位于数据管道的最后， 调用 pkt = waitForAckHead(seqno) 从 ackQueue 取出对应的 pkt ，然后调用 lastPacketInBlock = pkt.lastPacketInBlock;
然后是  if (lastPacketInBlock) {finalizeBlock(startTime);} ，然后是 sendAckUpstream ，这个就是给客户端发送 ack&lt;/p&gt;

&lt;p&gt;然后 ResponseProcessor 中收到了 ack，进行处理。逻辑比较简单，ack.readFields(blockReplyStream) 读取 ask，从输入流中读取的ack的seqno与ackQueue中取得的seqno不一样则抛出异常&lt;/p&gt;

&lt;p&gt;// 接收到ack后，从ackQueue中移除packet
      synchronized (dataQueue) {
        lastAckedSeqno = seqno;
        ackQueue.removeFirst();
        dataQueue.notifyAll();
        one.releaseBuffer(byteArrayManager);
      }
。&lt;/p&gt;

&lt;h1 id=&#34;6-namenode-处理&#34;&gt;6.NameNode 处理&lt;/h1&gt;

&lt;p&gt;上面我们说了 NameNode 的处理我们没法调试所以我们跳过了NameNode，原因是客户端通过远程的RPC调用执行，这次我们在IDEA启动NameNode，然后查看NameNode是如何处理写文件的。&lt;/p&gt;

&lt;p&gt;通过 RPC 调用 NameNodeRpcServer.create -&amp;gt; namesystem.startFile -&amp;gt; startFileInt -&amp;gt; startFileInternal&lt;/p&gt;

&lt;p&gt;中间有上锁的步骤，还有 BlockManager 的验证，namesystem 中有一个 dir:FSDirectory 的属性，a pure in-memory data structure ，
里面又有一个 rootDir：INodeDirectory 代表根节点，rootDir 中有一个 List&lt;INode&gt; children 代表所以的文件 ，还有一个 INodeMap 对象，保存。
注意还有一个 INodesInPath 类，从他的方法我们看出，它代表一个文件递归得到所有父目录的 InpPath 文件。&lt;/p&gt;

&lt;p&gt;startFileInternal 方法主要步骤如下：
- 根据 src 得到 INodesInPath，再得到 INodeFile 进行检验。
- newNode = dir.addFile(src, permissions, replication, blockSize,holder, clientMachine);
- getEditLog().logOpenFile(src, newNode, overwrite, logRetryEntry); 记录日志
- leaseManager.addLease(newNode.getFileUnderConstructionFeature()&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt; private BlocksMapUpdateInfo startFileInternal(FSPermissionChecker pc, 
      String src, PermissionStatus permissions, String holder, 
      String clientMachine, boolean create, boolean overwrite, 
      boolean createParent, short replication, long blockSize, 
      boolean isLazyPersist, CipherSuite suite, CryptoProtocolVersion version,
      EncryptedKeyVersion edek, boolean logRetryEntry)

//得到文件
final INodeFile myFile = INodeFile.valueOf(inode, src, true);

try {
      BlocksMapUpdateInfo toRemoveBlocks = null;
      if (myFile == null) {
      // 判断是否创建
        if (!create) {
          throw new FileNotFoundException(&amp;quot;Can&#39;t overwrite non-existent &amp;quot; +
              src + &amp;quot; for client &amp;quot; + clientMachine);
        }
      } else {
      // 是否覆盖
        if (overwrite) {
          toRemoveBlocks = new BlocksMapUpdateInfo();
          List&amp;lt;INode&amp;gt; toRemoveINodes = new ChunkedArrayList&amp;lt;INode&amp;gt;();
          long ret = dir.delete(src, toRemoveBlocks, toRemoveINodes, now());
          if (ret &amp;gt;= 0) {
            incrDeletedFileCount(ret);
            removePathAndBlocks(src, null, toRemoveINodes, true);
          }
        } else {
        // 存在而且不覆盖的情况？操作 lease
          // If lease soft limit time is expired, recover the lease
          recoverLeaseInternal(myFile, src, holder, clientMachine, false);
          throw new FileAlreadyExistsException(src + &amp;quot; for client &amp;quot; +
              clientMachine + &amp;quot; already exists&amp;quot;);
        }
      }

// Always do an implicit mkdirs for parent directory tree.
// 父目录
      Path parent = new Path(src).getParent();
      if (parent != null &amp;amp;&amp;amp; mkdirsRecursively(parent.toString(),
              permissions, true, now())) {
              // 添加到 namespace
        newNode = dir.addFile(src, permissions, replication, blockSize,
                              holder, clientMachine);
      }
      // 操作 lease
      leaseManager.addLease(newNode.getFileUnderConstructionFeature()
          .getClientName(), src);

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;INodesInPath：&lt;/p&gt;

&lt;p&gt;newNode = dir.addFile(src, permissions, replication, blockSize,holder, clientMachine);&lt;/p&gt;

&lt;p&gt;这个 dir 是 FSDirectory 类型的变量，最终调用了 INodeDirectory.addChild(INode) 方法，时间上就是给他的 children add 一个 INode&lt;/p&gt;

&lt;p&gt;和 addlease :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;synchronized Lease addLease(String holder, String src) {
    Lease lease = getLease(holder);
    if (lease == null) {
      lease = new Lease(holder);
      leases.put(holder, lease);
      sortedLeases.add(lease);
    } else {
      renewLease(lease);
    }
    sortedLeasesByPath.put(src, lease);
    lease.paths.add(src);
    return lease;
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;  /** Get a lease and start automatic renewal */
  private void beginFileLease(final long inodeId, final DFSOutputStream out)
      throws IOException {
    getLeaseRenewer().put(inodeId, out, this);
  }
  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;调用完 startFileInternal 后，会调用
stat = dir.getFileInfo(src, false, FSDirectory.isReservedRawName(srcArg), true);
实际上就是得到一个 HdfsFileStatus 的对象，返回给客户端。NameNode 穿件文件元数据信息的过程大致如此。&lt;/p&gt;

&lt;h1 id=&#34;七-datanode-创建文件&#34;&gt;七、DataNode 创建文件&lt;/h1&gt;

&lt;p&gt;上面讲了在 DataNode 的 DataXCeiver 的写数据过程，但是我们忽略了一些和流式接口无关的部分，包括创建数据块等。&lt;/p&gt;

&lt;p&gt;实际上客户端 DFSClient 发送创建元数据给 NameNode 以后，就要根据 HdfsFileStatus 创建到 DataNode 的输出流。
在 DataStreamer 的 createBlockOutputStream 方法中创建了 blockStream = out，实际上创建之前有个步骤：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;// send the request
new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,
    dfsClient.clientName, nodes, nodeStorageTypes, null, bcs, 
    nodes.length, block.getNumBytes(), bytesSent, newGS,
    checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个 Sender 和 DataXCerver 一样，继承自 DataTransferProtocol ，我们可以理解为 DataTransferProtocol 是客户端和 DataNode 通信协议，通信实现包含了创建 数据块和发送数据，
而 Sender 和 DataXCerver 就分别是创建数据块和发送数据的实现，分别是 TCP 和 RPC 通信方式实现。&lt;/p&gt;

&lt;p&gt;new Sender(out).writeBlock 只是简单的将信息以 ProtoBuf 的格式发送出去。实际上就是发送了一个 op ，这个 op 服务端会收到并解析，然后根据 op 的值调用不同方法，最后调用了：&lt;/p&gt;

&lt;p&gt;前面分析过： BlockReceiver.receiveBlock 。里面就有构建输出流写出，但是我们没有相关这个输出流是怎么来的，实际上就是一个文件流。我们看一下 BlockReceiver 的构造方法。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;this.block = block;

switch (stage) {
        case PIPELINE_SETUP_CREATE:
          replicaInfo = datanode.data.createRbw(storageType, block, allowLazyPersist);
          datanode.notifyNamenodeReceivingBlock(
              block, replicaInfo.getStorageUuid());
          break;


streams = replicaInfo.createStreams(isCreate, requestedChecksum);
this.out = streams.getDataOut();
this.checksumOut = new DataOutputStream(new BufferedOutputStream(
          streams.getChecksumOut(), HdfsConstants.SMALL_BUFFER_SIZE));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里有个 switch (stage)，实际上写文件都是一个 PIPELINE ，包括输入某个 client - DataNode - DataNode - DataNode ，由于我是本地模式调试，所以感觉不到这种 PIPELINE 的模式而已。&lt;/p&gt;

&lt;p&gt;datanode.data 是一个 FSDataSetImpl 类型的变量，控制着整个 DataNode 的文件信息。类型结构大致如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FSDataSetImpl 
    DataStorage
        bpStorageMap&amp;lt;bp,BlockPoolSliceStorage&amp;gt;
    FSVolumnList
        List&amp;lt;FSVolumnImpl&amp;gt;
             bpSlices&amp;lt;bp, BlockPoolSlice&amp;gt;
    volumeMap:ReplicaMap&amp;lt;bp, Map&amp;lt;blockid, ReplicaInfo&amp;gt;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;FSDataSetImpl 代表整个 DataNode 的文件存储，FSVolumnList 是因为我们配置的 data 存储路径，可以用逗号隔开，一般情况下配置1个路径，FSVolumnList 里面就一个 FsVolumeImpl。
FsVolumeImpl 里面有分为 多个 blockpool 存储，一个 blockpool 对应一个文件夹而已，这在一开始版本是没有的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Storage

  List&amp;lt;StorageDirectory&amp;gt; storageDirs
  public int   layoutVersion;   // layout version of the storage data
  public int   namespaceID;     // id of the file system
  public String clusterID;      // id of the cluster
  public long  cTime;   
  static class StorageDirectory
      File root

DataStorage
   private String datanodeUuid = null;
   Map&amp;lt;String, BlockPoolSliceStorage&amp;gt; bpStorageMap
   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;FsVolumeImpl&lt;/p&gt;

&lt;p&gt;replicaInfo 保存了文件的位置信息，所以可以用来创建输出流。&lt;/p&gt;

&lt;h1 id=&#34;八-租约处理&#34;&gt;八、租约处理&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;synchronized void put(final long inodeId, final DFSOutputStream out,
      final DFSClient dfsc) {
    if (dfsc.isClientRunning()) {
      if (!isRunning() || isRenewerExpired()) {
        //start a new deamon with a new id.
        final int id = ++currentId;
        daemon = new Daemon(new Runnable() {
          @Override
          public void run() {
            try {
              if (LOG.isDebugEnabled()) {
                LOG.debug(&amp;quot;Lease renewer daemon for &amp;quot; + clientsString()
                    + &amp;quot; with renew id &amp;quot; + id + &amp;quot; started&amp;quot;);
              }
              LeaseRenewer.this.run(id);
            } catch(InterruptedException e) {
              if (LOG.isDebugEnabled()) {
                LOG.debug(LeaseRenewer.this.getClass().getSimpleName()
                    + &amp;quot; is interrupted.&amp;quot;, e);
              }
            } finally {
              synchronized(LeaseRenewer.this) {
                Factory.INSTANCE.remove(LeaseRenewer.this);
              }
              if (LOG.isDebugEnabled()) {
                LOG.debug(&amp;quot;Lease renewer daemon for &amp;quot; + clientsString()
                    + &amp;quot; with renew id &amp;quot; + id + &amp;quot; exited&amp;quot;);
              }
            }
          }
          
          @Override
          public String toString() {
            return String.valueOf(LeaseRenewer.this);
          }
        });
        daemon.start();
      }
      dfsc.putFileBeingWritten(inodeId, out);
      emptyTime = Long.MAX_VALUE;
    }
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最重要的就是  LeaseRenewer.this.run(id)， 在run中调用renew对租约续约。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;for(long lastRenewed = Time.now(); !Thread.interrupted();
        Thread.sleep(getSleepPeriod())) {
      final long elapsed = Time.now() - lastRenewed;
      if (elapsed &amp;gt;= getRenewalTime()) {
        try {
          renew();
          if (LOG.isDebugEnabled()) {
            LOG.debug(&amp;quot;Lease renewer daemon for &amp;quot; + clientsString()
                + &amp;quot; with renew id &amp;quot; + id + &amp;quot; executed&amp;quot;);
          }
          lastRenewed = Time.now();
        } catch (SocketTimeoutException ie) {
          LOG.warn(&amp;quot;Failed to renew lease for &amp;quot; + clientsString() + &amp;quot; for &amp;quot;
              + (elapsed/1000) + &amp;quot; seconds.  Aborting ...&amp;quot;, ie);
          synchronized (this) {
            while (!dfsclients.isEmpty()) {
              dfsclients.get(0).abort();
            }
          }
          break;
        } catch (IOException ie) {
          LOG.warn(&amp;quot;Failed to renew lease for &amp;quot; + clientsString() + &amp;quot; for &amp;quot;
              + (elapsed/1000) + &amp;quot; seconds.  Will retry shortly ...&amp;quot;, ie);
        }
      }

&lt;/code&gt;&lt;/pre&gt;
</description>
      
    </item>
    
    <item>
      <title>resourcemanager</title>
      <link>https://dengziming.github.io/post/hadoop/first/</link>
      <pubDate>Tue, 22 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/hadoop/first/</guid>
      
        <description>&lt;p&gt;参考资料：&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://hortonworks.com/blog/apache-hadoop-yarn-resourcemanager/&#34;&gt;https://hortonworks.com/blog/apache-hadoop-yarn-resourcemanager/&lt;/a&gt;&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>yarn-nodemanager-剖析</title>
      <link>https://dengziming.github.io/post/hadoop/yarn-nodemanager-1/</link>
      <pubDate>Tue, 22 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/hadoop/yarn-nodemanager-1/</guid>
      
        <description>

&lt;h1 id=&#34;架构&#34;&gt;架构&lt;/h1&gt;

&lt;p&gt;ContainerManagementImpl&lt;/p&gt;

&lt;h1 id=&#34;container-生命周期&#34;&gt;Container 生命周期&lt;/h1&gt;

&lt;p&gt;第一步是 RM 的 applicationMasterLauncher ，创建 ApplicationMasterLauncher 后，遇到 launch 时间 ，
case LAUNCH: launch(application); =&amp;gt; new AMLauncher(context, application, event, getConfig());&lt;/p&gt;

&lt;p&gt;这个任务放进 队列里面等待执行，一旦执行会调用 launch() 方法，然后调用 containerMgrProxy.startContainers(allRequests); 这是 RPC 调用&lt;/p&gt;

&lt;p&gt;实际上就是 ContainerManagementImpl ，然后会调用 startContainerInternal ，然后就是 new ContainerImpl 。&lt;/p&gt;

&lt;p&gt;这是 APPMaster 启动需要的 container ，实际上还有 APPMaster 调度任务需要更多的 Container ，继续向 ContainerManagementImpl 请求&lt;/p&gt;

&lt;h2 id=&#34;1-资源本地化&#34;&gt;1. 资源本地化&lt;/h2&gt;

&lt;p&gt;实际上就是 ContainerManagementImpl ，然后会调用 startContainerInternal ，然后就是 new ContainerImpl 。
然后通过 if (null == context.getApplications().putIfAbsent(applicationID,application)) 判断是否是该 NodeManager 第一个 Container ，如果是的话，new ApplicationImpl
向 ApplicationImpl 发送 ApplicationInitEvent 事件，同时发送 ApplicationContainerInitEvent 事件。&lt;/p&gt;

&lt;p&gt;这些事件会触发 ACL、log等相关的事件， 收到 ApplicationContainerInitEvent 后将 Container 加入 ApplicationImpl 的维护列表。&lt;/p&gt;

&lt;p&gt;logHandle 处理完成之后会发送一个 log 事件，applicationImpl 收到后向 ResourceLocalizeService 发送 事件，
为 private 和 application 级别的资源创建 LocalResourceTrackerImp ，为下载资源作准备。&lt;/p&gt;

&lt;p&gt;private 的资源用户可见，如果该用户已经提交过了，无需创建。同理，如果 application 已经启动过 container 了，则同一个 application 的新 container 不必在创建。&lt;/p&gt;

&lt;p&gt;经过上面操作后，ResourceLocalizeService 向 ApplicationImpl 发送 Application_Init&lt;/p&gt;

&lt;p&gt;ApplicationImpl 收到 INIT 后，向所有的 ContainerImpl 发送 InitContainer ，ApplicationImpl 也从 ApplicationState.INITING 变为 ApplicationState.RUNNING,&lt;/p&gt;

&lt;p&gt;InitContainer 命令后，和 AuxService 交互，然后从 ContainerLaunchContext 得到各类可见性资源并保存到相应数据结构，然后发送给 ResourceLocalizeService 。&lt;/p&gt;

&lt;p&gt;ResourceLocalizeService 调用 handleInitContainerResources((ContainerLocalizationRequestEvent) event); 实际是 是发送给 LocalResourcesTrackerImpl 。&lt;/p&gt;

&lt;p&gt;LocalResourcesTrackerImpl 会 判断是否需要下载等，为对应的资源创建 LocalizedResource 状态机，将 Request 发送给 LocalizedResource。&lt;/p&gt;

&lt;p&gt;后续还是这样的时间驱动，总之可以概括为 ： NodeManager 上同一个 App 所有的 ContainerImpl 异步并发向向资源下载服务 ResourceLocalizeService 发送待下载的资源，
ResourceLocalizeService下载完成后会通知依赖资源的所以 Container ，当一个 Container 依赖的资源全部下载完毕，Container 将会进入 运行阶段&lt;/p&gt;

&lt;h2 id=&#34;2-container-运行&#34;&gt;2. Container 运行&lt;/h2&gt;

&lt;p&gt;运行是 ContainerLauncher 服务实现的，主要过程为： 将待运行 Container 所需要的环境变量和运行命令写到 &lt;code&gt;launch_container.sh&lt;/code&gt; 中，
将启动该脚本的命令写入：&lt;code&gt;default_container_executor.sh&lt;/code&gt; 中。&lt;/p&gt;

&lt;p&gt;通过运行该脚本启动 Container 。主要有四步：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;ContainerImpl 向 ContainersLauncher 发送 Launch_container ，请求启动 container。
dispatcher.getEventHandler().handle(new ContainersLauncherEvent(this, launcherEvent));&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ContainersLauncher 收到后，&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;Application app =context.getApplications().get(containerId.getApplicationAttemptId().getApplicationId());
ContainerLaunch launch = new ContainerLaunch(context, getConfig(), dispatcher, exec, app,event.getContainer(), dirsHandler, containerManager);
containerLauncher.submit(launch);
running.put(containerId, launch);
break;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ContainerLaunch 放到线程池执行，对应的 call 方法为：&lt;/p&gt;

&lt;p&gt;为 Container 创建 token 文件 和 &lt;code&gt;launch_container.sh&lt;/code&gt; ，将他们保存到 NodeManager 私有目录 nmPrivate 下面， &lt;code&gt;launch_container.sh&lt;/code&gt;包含了运行所以的命令。
一般都是前面 export 环境变量，最后有个 exec 命令 。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;准备好了 命令，
&lt;code&gt;Container_Launcher&lt;/code&gt; 首先向 ContainerImpl 发送 &lt;code&gt;Container_LANUCHED&lt;/code&gt; 命令，然他启动监控等。然后调用 ContainerExector launchContainer 启动 Container 。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;然后是启动监控，汇报信息等。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>yarn-resourcemanager-1</title>
      <link>https://dengziming.github.io/post/hadoop/yarn-resourcemanager-1/</link>
      <pubDate>Tue, 22 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/hadoop/yarn-resourcemanager-1/</guid>
      
        <description>

&lt;p&gt;参考资料：&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://hortonworks.com/blog/apache-hadoop-yarn-resourcemanager/&#34;&gt;https://hortonworks.com/blog/apache-hadoop-yarn-resourcemanager/&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;提交应用程序的过程&#34;&gt;提交应用程序的过程&lt;/h1&gt;

&lt;h2 id=&#34;1-yarnclient-submitapplication-appcontext&#34;&gt;1. yarnClient.submitApplication(appContext);&lt;/h2&gt;

&lt;p&gt;新建请求，最终调用： rmClient.submitApplication(request);&lt;/p&gt;

&lt;p&gt;实际上会通过RPC调用 ClientRMService.submitApplication(SubmitApplicationRequest request)&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;得到APPID：ApplicationId applicationId = submissionContext.getApplicationId();&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;rmAppManager.submitApplication(submissionContext, System.currentTimeMillis(), user);&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;放到 rmAppManager 中，rmAppManager 中存放了所有的 application。
跟进去，发现调用了：&lt;/p&gt;

&lt;p&gt;this.rmContext.getDispatcher().getEventHandler().handle(new RMAppEvent(applicationId, RMAppEventType.START));&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public void handle(RMAppEvent event) {
      ApplicationId appID = event.getApplicationId();
      RMApp rmApp = this.rmContext.getRMApps().get(appID);
      if (rmApp != null) {
        try {
          rmApp.handle(event);
        } catch (Throwable t) {
          LOG.error(&amp;quot;Error in handling event type &amp;quot; + event.getType()
              + &amp;quot; for application &amp;quot; + appID, t);
        }
      }
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后导致这个 applicationId 所在的 RMAppEvent 状态机发生变化。&lt;/p&gt;

&lt;h2 id=&#34;2-registerapplicationmasterresponse-response-amrmclient-registerapplicationmaster-appmasterhostname-appmasterrpcport-appmastertrackingurl&#34;&gt;2.RegisterApplicationMasterResponse response = amRMClient.registerApplicationMaster(appMasterHostname, appMasterRpcPort,appMasterTrackingUrl);&lt;/h2&gt;

&lt;p&gt;注册 ApplicationMaster，注意这段代码是在用户编写的 ApplicationMaster 类中，所以这段代码运行在yarn给APPMaster分配的Container中。&lt;/p&gt;

&lt;p&gt;RegisterApplicationMasterResponse response = client.registerApplicationMaster(appHostName, appHostPort, appTrackingUrl);&lt;/p&gt;

&lt;p&gt;会调用：RegisterApplicationMasterResponse response = rmClient.registerApplicationMaster(request);&lt;/p&gt;

&lt;p&gt;最终会通过RPC调用：ApplicationMasterServeice.registerApplicationMaster(RegisterApplicationMasterRequest request)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;this.rmContext
        .getDispatcher()
        .getEventHandler()
        .handle(
          new RMAppAttemptRegistrationEvent(applicationAttemptId, request
            .getHost(), request.getRpcPort(), request.getTrackingUrl()));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这种 RMAppAttemptEventType 类型的会 通过handle进行处理：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public void handle(RMAppAttemptEvent event) {
      ApplicationAttemptId appAttemptID = event.getApplicationAttemptId();
      ApplicationId appAttemptId = appAttemptID.getApplicationId();
      RMApp rmApp = this.rmContext.getRMApps().get(appAttemptId);
      if (rmApp != null) {
        RMAppAttempt rmAppAttempt = rmApp.getRMAppAttempt(appAttemptID);
        if (rmAppAttempt != null) {
          try {
            rmAppAttempt.handle(event);
          } catch (Throwable t) {
            LOG.error(&amp;quot;Error in handling event type &amp;quot; + event.getType()
                + &amp;quot; for applicationAttempt &amp;quot; + appAttemptId, t);
          }
        }
      }
    }
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;和上面的 RMAppEvent 一样，会进入一个状态机进行处理。&lt;/p&gt;

&lt;h3 id=&#34;1-状态机相互转换细节&#34;&gt;1.状态机相互转换细节&lt;/h3&gt;

&lt;p&gt;上面的过程细化一下：&lt;/p&gt;

&lt;p&gt;RMAppImpl 收到 RMAppEventType.START 事件后，会调用 RMStateStore#storeApplication，以日志记录 RMAppImpl 当前信息，&lt;/p&gt;

&lt;p&gt;至此，RMAppImpl 的运行状态由 NEW 转移为 NEW_SAVING。该步骤就较为复杂了，下面详细介绍下。&lt;/p&gt;

&lt;p&gt;其中 RMAppEventType 注册到中央异步调度器的地方在 ResourceManager.java 中，new ApplicationEventDispatcher(rmContext) 进行处理，
处理方式很简单：通过appid得到得到 RMAppImpl ，最终会给  RMAppImpl自己处理，进入他的状态机处理。状态机有这么一个事件：&lt;/p&gt;

&lt;p&gt;addTransition(RMAppState.NEW, RMAppState.NEW_SAVING, RMAppEventType.START, new RMAppNewlySavingTransition())&lt;/p&gt;

&lt;p&gt;RMAppNewlySavingTransition 的 transition 就是 app.rmContext.getStateStore().storeNewApplication(app);  实际上就是保存应用的相关信息。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public synchronized void storeNewApplication(RMApp app) {  
    //app=RMAppImpl  
    LOG.info(&amp;quot;begin to storeNewApplication,app=&amp;quot;+app.toString());  
    ApplicationSubmissionContext context = app.getApplicationSubmissionContext();  
    assert context instanceof ApplicationSubmissionContextPBImpl;  
    ApplicationState appState =  
        new ApplicationState(app.getSubmitTime(), app.getStartTime(), context,app.getUser());  
    dispatcher.getEventHandler().handle(new RMStateStoreAppEvent(appState));  
  }  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意： dispatcher.getEventHandler().handle(new RMStateStoreAppEvent(appState));  这里会调用 RMStateStore 状态机的 transition，实际上就是 store + notifyDoneStoringApplication&lt;/p&gt;

&lt;p&gt;&lt;code&gt;rmDispatcher.getEventHandler().handle(new RMAppNewSavedEvent(appId, storedException));&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;这个事件又会进入 RMAppImpl 的状态机，对应代码 addTransition(RMAppState.NEW_SAVING, RMAppState.SUBMITTED, RMAppEventType.APP_NEW_SAVED, new AddApplicationToSchedulerTransition())&lt;/p&gt;

&lt;p&gt;调用：app.handler.handle(new AppAddedSchedulerEvent(app.applicationId,app.submissionContext.getQueue(), app.user));&lt;/p&gt;

&lt;p&gt;会触发： RMAppImpl 处理 AppAddedSchedulerEvent&lt;/p&gt;

&lt;p&gt;然后这个事件会分配给：CapacityScheduler ，&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;case APP_ADDED:  
    {  
      AppAddedSchedulerEvent appAddedEvent = (AppAddedSchedulerEvent) event;  
      addApplication(appAddedEvent.getApplicationId(),  
        appAddedEvent.getQueue(), appAddedEvent.getUser());  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;addApplication 会调用 rmContext.getDispatcher().getEventHandler().handle(new RMAppEvent(applicationId, RMAppEventType.APP_ACCEPTED));&lt;/p&gt;

&lt;p&gt;RMAppImpl 会触发 ：addTransition(RMAppState.SUBMITTED, RMAppState.ACCEPTED,  RMAppEventType.APP_ACCEPTED, new StartAppAttemptTransition())&lt;/p&gt;

&lt;p&gt;对应的transition： createNewAttempt(); handler.handle(new RMAppStartAttemptEvent(currentAttempt.getAppAttemptId(),  transferStateFromPreviousAttempt));&lt;br /&gt;
实际上就是触发 RMAppAttemptImpl 状态机操作。&lt;/p&gt;

&lt;p&gt;RMAppAttemptImpl 接受 RMAppAttemptEventType.START 事件后，进行一系列初始化工作。将自身状态由NEW转换为SUBMITTED，并调用 AttemptStartedTransition。&lt;/p&gt;

&lt;p&gt;AttemptStartedTransition appAttempt.eventHandler.handle(new AppAttemptAddedSchedulerEvent(  appAttempt.applicationAttemptId, transferStateFromPreviousAttempt));&lt;/p&gt;

&lt;p&gt;AppAttemptAddedSchedulerEvent 会交给 CapacityScheduler 。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;case APP_ATTEMPT_ADDED:  
    {  
      AppAttemptAddedSchedulerEvent appAttemptAddedEvent =  
          (AppAttemptAddedSchedulerEvent) event;  
      addApplicationAttempt(appAttemptAddedEvent.getApplicationAttemptId(),  
        appAttemptAddedEvent.getTransferStateFromPreviousAttempt(),  
        appAttemptAddedEvent.getShouldNotifyAttemptAdded());  
    }  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实际上就是讲这个 attempt 放进队列，等待处理。并且：rmContext.getDispatcher().getEventHandler().handle( new RMAppAttemptEvent(applicationAttemptId, RMAppAttemptEventType.ATTEMPT_ADDED));&lt;/p&gt;

&lt;p&gt;RMAppAttemptImpl 接受到事件 RMAppAttemptEventType.ATTEMPT_ADDED 后，状态由SUBMITTED转换为SCHEDULED。进入内部类ScheduleTransition的transition函数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;private static final class ScheduleTransition  
      implements  
      MultipleArcTransition&amp;lt;RMAppAttemptImpl, RMAppAttemptEvent, RMAppAttemptState&amp;gt; {  
    @Override  
    public RMAppAttemptState transition(RMAppAttemptImpl appAttempt,  
        RMAppAttemptEvent event) {  
        LOG.info(&amp;quot;class::ScheduleTransition, func::transition, begin.&amp;quot;);  
      if (!appAttempt.submissionContext.getUnmanagedAM()) {  
        // Request a container for the AM.  
        ResourceRequest request =  
            BuilderUtils.newResourceRequest(  
                AM_CONTAINER_PRIORITY, ResourceRequest.ANY, appAttempt  
                    .getSubmissionContext().getResource(), 1);  
  
        // SchedulerUtils.validateResourceRequests is not necessary because  
        // AM resource has been checked when submission  
        Allocation amContainerAllocation = appAttempt.scheduler.allocate(  
            appAttempt.applicationAttemptId,  
            Collections.singletonList(request), EMPTY_CONTAINER_RELEASE_LIST, null, null);  
        if (amContainerAllocation != null  
            &amp;amp;&amp;amp; amContainerAllocation.getContainers() != null) {  
          assert (amContainerAllocation.getContainers().size() == 0);  
        }  
        return RMAppAttemptState.SCHEDULED;  
      } else {  
        // save state and then go to LAUNCHED state  
        appAttempt.storeAttempt();  
        return RMAppAttemptState.LAUNCHED_UNMANAGED_SAVING;  
      }  
    }  
  } 
   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里面就是：新建资源 ResourceRequest ，然后 appAttempt.scheduler.allocate&lt;/p&gt;

&lt;p&gt;&amp;mdash;&amp;mdash; 这里断层了,谁触发了 AMContainerImpl 启动和分配 Container，需要后续再看。&lt;/p&gt;

&lt;p&gt;这里有个疑问需要解答一下，之前一直好奇是哪里启动了 AMContainerImpl，上面的 schedule.allocate 将需要的资源提交给 schedule ，实际上 schedule 会分配。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;application.updateResourceRequests(ask);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这一句话，&lt;/p&gt;

&lt;p&gt;以  FairScheduler 为例，启动服务会调用 initScheduler(conf); 里面有三行代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;schedulingThread = new ContinuousSchedulingThread();
schedulingThread.setName(&amp;quot;FairSchedulerContinuousScheduling&amp;quot;);
schedulingThread.setDaemon(true);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;会有守护线程调用 continuousSchedulingAttempt(); 实际上会调用：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;    for (NodeId nodeId : nodeIdList) {
      FSSchedulerNode node = getFSSchedulerNode(nodeId);
      try {
        if (node != null &amp;amp;&amp;amp; Resources.fitsIn(minimumAllocation,
            node.getAvailableResource())) {
          attemptScheduling(node);
        }
      } catch (Throwable ex) {
        LOG.error(&amp;quot;Error while attempting scheduling for node &amp;quot; + node +
            &amp;quot;: &amp;quot; + ex.toString(), ex);
      }
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个 attemptScheduling(node); 就会创建 AMContainerImpl 实例，至于怎么创建，需要了解各个 Schedule 的内部细节。&lt;/p&gt;

&lt;p&gt;ResourceManager 为应用程序的 AM 分配资源后，创建一个 RMContainerImpl，并向它发送一个 RMContainerEventType.START 事件。&lt;/p&gt;

&lt;p&gt;RMContainerImpl 收到 RMContainerEventType.START 事件后，直接向 RMAppAttemptImpl 发送一个 RMAppAttemptEventType.CONTAINER_ALLOCATED&lt;/p&gt;

&lt;p&gt;RMAppAttemptImpl 收到 RMAppAttemptEventType.CONTAINER_ALLOCATED 事件后：调用 AMContainerAllocatedTransition：&lt;/p&gt;

&lt;p&gt;transition函数中，调用 scheduler.allocate 获取分配的资源，scheduler 返回资源之前，会向 RMContainerImpl 发送 RMContainerEventType.ALLOCATED事件。&lt;/p&gt;

&lt;p&gt;RMAppAttemptImpl 收到资源后，向 RMStateStore 发送 MStateStoreEventType.STORE_APP_ATTEMPT 事件请求记录日志。&lt;/p&gt;

&lt;p&gt;至此，RMAppAttemptImpl 状态从 SCHEDULED 转换为 ALLOCATED_SAVING。&lt;/p&gt;

&lt;p&gt;日志记录完成后，RMStateStore 向 RMAppAttemptImpl 发送 RMAppAttemptEventType.ATTEMPT_NEW_SAVED 事件。&lt;/p&gt;

&lt;p&gt;RMAppAttemptImpl 收到 RMAppAttemptEventType.ATTEMPT_NEW_SAVED 事件后，
向 ApplicationMasterLauncher 发送 AMLauncherEventType.LAUNCH 事件，
至此，RMAppAttemptImpl 状态从 ALLOCATED_SAVING 转换为 ALLOCATED。&lt;/p&gt;

&lt;p&gt;后面的和这里类似，不过涉及到了 RMContainer状态机，先跳过。&lt;/p&gt;

&lt;h2 id=&#34;3-总结&#34;&gt;3.总结&lt;/h2&gt;

&lt;p&gt;通过这个实例我们大概了解了yarn中的RPC、调度器、服务、状态机配合的过程。
一般是客户端（可以使用户的client、nodeManager进程或者它启动的container进程）发送请求，中间通过RPC调用了ResourceManager中的某个服务，这个服务会触发一定的事件，并且返回。&lt;/p&gt;

&lt;p&gt;例如客户端提交一个应用程序，首先有个 appid，每个appid对应的有一个 RMApp ，放在 rmAppManager 的一个map中。这个 RMApp 是一个状态机。&lt;/p&gt;

&lt;p&gt;然后会调用 this.rmContext.getDispatcher().getEventHandler().handle(new RMAppEvent(applicationId, RMAppEventType.START));&lt;/p&gt;

&lt;p&gt;调度器会启动对应的 EventHandle 去处理这个事件，而 对应的 EventHandle 会根据appid 通过 rmAppManager 得到对应的 RMApp，调用对应的状态转化函数就实现了状态转化。&lt;/p&gt;

&lt;p&gt;再例如某个container启动 APPMaster，也是调用
this.rmContext.getDispatcher().getEventHandler().handle
(new RMAppAttemptRegistrationEvent(applicationAttemptId, request.getHost(), request.getRpcPort(), request.getTrackingUrl()));&lt;/p&gt;

&lt;p&gt;然后调度器会启动对应的 EventHandle 去处理这个事件，而 对应的 EventHandle 会根据appid 通过 rmAppManager 得到对应的 RMApp，
这时候事件类似是 RMAppAttemptEvent，处理逻辑变了，会在另一个状态机进行操作。&lt;/p&gt;

&lt;h2 id=&#34;4-rmcontainer状态机&#34;&gt;4.RMContainer状态机&lt;/h2&gt;

&lt;p&gt;上面分析了 两个状态机，实际上还有一个 RMContainer ，这个和上面两个类似吧。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>yarn-基础库</title>
      <link>https://dengziming.github.io/post/hadoop/yarn-%E5%9F%BA%E7%A1%80%E5%BA%93/</link>
      <pubDate>Tue, 22 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/hadoop/yarn-%E5%9F%BA%E7%A1%80%E5%BA%93/</guid>
      
        <description>

&lt;h1 id=&#34;yarn-事件库和服务库&#34;&gt;yarn-事件库和服务库&lt;/h1&gt;

&lt;h2 id=&#34;使用&#34;&gt;使用&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;新建Event和EventType&lt;/li&gt;
&lt;li&gt;新建 AsyncDispatcher 并给 AsyncDispatcher 注册 Event 和对应的 EventHandler&lt;Event&gt;&lt;/li&gt;
&lt;li&gt;调用 AsyncDispatcher 的 getEventHandler 得到 EventHandler 然后调用 handler 的 handle 方法处理 Event&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;基本原理&#34;&gt;基本原理：&lt;/h2&gt;

&lt;p&gt;AsyncDispatcher 注册 EventHandler&lt;Event&gt; 的过程实际上生成了一个 map，保存了每个事件对应的handler。同时有一个 队列，用于放置 Event&lt;/p&gt;

&lt;p&gt;调用 handle 的时候 将Event放进queue中，内部启动一个线程不断处理 queue的任务。&lt;/p&gt;

&lt;h1 id=&#34;yarn-状态机&#34;&gt;yarn-状态机&lt;/h1&gt;

&lt;h2 id=&#34;使用-1&#34;&gt;使用&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;初始化&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;StateMachineFactory
.addTransition(JobStateInternal.NEW, JobStateInternal.INITED, JobEventType.JOB_INIT,new InitTransition())
.addTransition(JobStateInternal.INITED, JobStateInternal.SETUP, JobEventType.JOB_START,new StartTransition())
.installTopology()
.make()
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;新建对应的 Transition&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public static class InitTransition implements SingleArcTransition&amp;lt;JobStateMachine,JobEvent&amp;gt;{

        @Override
        public void transition(JobStateMachine job, JobEvent event) {
            System.out.println(&amp;quot;Receiving event &amp;quot; + event);
        }

    }
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;调用 StateMachine 的 doTransition(event.getType(), event)&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;原理&#34;&gt;原理&lt;/h2&gt;

&lt;p&gt;installTopology的时候创建一个拓扑图，记录每个 State 能接受的 Event，以及接受该 Event 后的操作，以及操作后的 State。&lt;/p&gt;

&lt;p&gt;每次有Event传入，调用对应的 Transition ，并且将 此时刻 的状态变为 操作后的状态。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>janusgraph一次给janusgraph提交源码的过程</title>
      <link>https://dengziming.github.io/post/titan/janusgraph%E4%BD%BF%E7%94%A8%E9%97%AE%E9%A2%98/</link>
      <pubDate>Thu, 26 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/titan/janusgraph%E4%BD%BF%E7%94%A8%E9%97%AE%E9%A2%98/</guid>
      
        <description>

&lt;p&gt;#
研究了好久的 neo4j源码，现在公司要换 janusgraph，只要半途而废开始研究 janusgraph 了
&lt;code&gt;https://github.com/JanusGraph/janusgraph&lt;/code&gt;和&lt;code&gt;http://janusgraph.org/&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&#34;一-相关问题&#34;&gt;一、相关问题&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/JanusGraph/janusgraph/issues/1157&#34;&gt;https://github.com/JanusGraph/janusgraph/issues/1157&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;reindex 的时候，一直等待三分钟。并且打印日志：
&amp;ldquo;&amp;ldquo;2018-06-10 09:03:19 [Thread-15] ERROR o.j.g.d.management.ManagementLogger - Evicted [6@6d56b8c524955-pc-jblur-com3] from cache but waiting too long for transactions to close. Stale transaction alert on: [standardjanusgraphtx[0x67a3ba21], standardjanusgraphtx[0x6cf78315], standardjanusgraphtx[0x48ce7bcd], standardjanusgraphtx[0x1862c45e], standardjanusgraphtx[0x04c1309d], standardjanusgraphtx[0x13bda0b2], standardjanusgraphtx[0x1187c9e8]]&lt;/p&gt;

&lt;p&gt;实际上原因是
There is a bug when we are reindexing a new index in an empty database. The process tries to fetch data from the database but there are no data. Because of that, the process tries to get some data for 3 minutes with the similar error logged as this one:&lt;/p&gt;

&lt;p&gt;我已经做了修复并提交到 janusgraph 的源码，等待merge。
&lt;a href=&#34;https://github.com/JanusGraph/janusgraph/pull/1162&#34;&gt;https://github.com/JanusGraph/janusgraph/pull/1162&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;现在已经被合并了，我也成了janusgraph的contributor&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>janusgraph源码分析1-下载编译启动</title>
      <link>https://dengziming.github.io/post/titan/janusgraph%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%901-%E4%B8%8B%E8%BD%BD%E7%BC%96%E8%AF%91%E5%90%AF%E5%8A%A8/</link>
      <pubDate>Thu, 26 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/titan/janusgraph%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%901-%E4%B8%8B%E8%BD%BD%E7%BC%96%E8%AF%91%E5%90%AF%E5%8A%A8/</guid>
      
        <description>

&lt;p&gt;#
研究了好久的 neo4j源码，现在公司要换 janusgraph，只要半途而废开始研究 janusgraph 了
&lt;code&gt;https://github.com/JanusGraph/janusgraph&lt;/code&gt;和&lt;code&gt;http://janusgraph.org/&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&#34;一-下载编译&#34;&gt;一、下载编译&lt;/h2&gt;

&lt;p&gt;我直接使用github desktop打开了 janusgraph 的源码，使用IDEA打开，然后编译：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 编译完整的
mvn -settings ~/opt/soft/apache-maven-3.5.0/conf/settings.xml -Dlicense.skip=true -DskipTests clean install
# 只编译core部分
mvn -pl janusgraph-core -am clean install -Dlicense.skip=true -DskipTests -P prod

-rf :janusgraph-test
mvn -pl janusgraph-test -am clean install -Dlicense.skip=true -DskipTests -P prod
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们在 &lt;code&gt;janusgraph-test&lt;/code&gt; 下面编写一个例子 &lt;code&gt;FirstTest&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public class FirstTest {

    public static void main(String[] args) {

        /*
         * The example below will open a JanusGraph graph instance and load The Graph of the Gods dataset diagrammed above.
         * JanusGraphFactory provides a set of static open methods,
         * each of which takes a configuration as its argument and returns a graph instance.
         * This tutorial calls one of these open methods on a configuration
         * that uses the BerkeleyDB storage backend and the Elasticsearch index backend,
         * then loads The Graph of the Gods using the helper class GraphOfTheGodsFactory.
         * This section skips over the configuration details, but additional information about storage backends,
         * index backends, and their configuration are available in
         * Part III, “Storage Backends”, Part IV, “Index Backends”, and Chapter 13, Configuration Reference.
         */

        // Loading the Graph of the Gods Into JanusGraph
        JanusGraph graph = JanusGraphFactory
                .open(&amp;quot;janusgraph-dist/src/assembly/cfilter/conf/janusgraph-berkeleyje-es.properties&amp;quot;);

        GraphOfTheGodsFactory.load(graph);
        GraphTraversalSource g = graph.traversal();

        /*
         * The typical pattern for accessing data in a graph database is to first locate the entry point into the graph
         * using a graph index. That entry point is an element (or set of elements) 
         * — i.e. a vertex or edge. From the entry elements,
         * a Gremlin path description describes how to traverse to other elements in the graph via the explicit graph structure.
         * Given that there is a unique index on name property, the Saturn vertex can be retrieved.
         * The property map (i.e. the key/value pairs of Saturn) can then be examined.
         * As demonstrated, the Saturn vertex has a name of &amp;quot;saturn, &amp;quot; an age of 10000, and a type of &amp;quot;titan.&amp;quot;
         * The grandchild of Saturn can be retrieved with a traversal that expresses:
         * &amp;quot;Who is Saturn’s grandchild?&amp;quot; (the inverse of &amp;quot;father&amp;quot; is &amp;quot;child&amp;quot;). The result is Hercules.
         */
        // Global Graph Indices
        Vertex saturn = g.V().has(&amp;quot;name&amp;quot;, &amp;quot;saturn&amp;quot;).next();
        GraphTraversal&amp;lt;Vertex, Map&amp;lt;String, Object&amp;gt;&amp;gt; vertexMapGraphTraversal = g.V(saturn).valueMap();

        GraphTraversal&amp;lt;Vertex, Object&amp;gt; values = g.V(saturn).in(&amp;quot;father&amp;quot;).in(&amp;quot;father&amp;quot;).values(&amp;quot;name&amp;quot;);

        /*
         * The property place is also in a graph index. The property place is an edge property.
         * Therefore, JanusGraph can index edges in a graph index.
         * It is possible to query The Graph of the Gods for all events that have happened within 50 kilometers of Athens
          * (latitude:37.97 and long:23.72).
          * Then, given that information, which vertices were involved in those events.
         */
		System.out.println(g.E().has(&amp;quot;place&amp;quot;, geoWithin(Geoshape.circle(37.97, 23.72, 50))));
        System.out.println(g.E().has(&amp;quot;place&amp;quot;, geoWithin(Geoshape.circle(37.97, 23.72, 50)))
                .as(&amp;quot;source&amp;quot;).inV()
                .as(&amp;quot;god2&amp;quot;)
                .select(&amp;quot;source&amp;quot;).outV()
                .as(&amp;quot;god1&amp;quot;).select(&amp;quot;god1&amp;quot;, &amp;quot;god2&amp;quot;)
                .by(&amp;quot;name&amp;quot;));
    }

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后在&amp;rdquo;janusgraph-dist/src/assembly/cfilter/conf/janusgraph-berkeleyje-es.properties&amp;rdquo; 文件中，将注释掉的内容取消注释。&lt;/p&gt;

&lt;p&gt;运行发现依赖挺麻烦。
首先运行报错了：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Exception in thread &amp;quot;main&amp;quot; java.lang.IllegalArgumentException: Could not find implementation class: org.janusgraph.diskstorage.berkeleyje.BerkeleyJEStoreManager
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;找到报错处的代码，我们发现 &lt;code&gt;janusgraph-core&lt;/code&gt; 中通过反射创建一个类，但是这个类在 &lt;code&gt;janusgraph-berkeleyje&lt;/code&gt; 中，而前者不依赖后者，所以找不到这个类，我们可以将后者加到前者的依赖，
但是我们发现后者依赖前者，如果加了依赖两个就相互依赖了，这是 Janus 官方设计的问题。我们只好在 FirstTest 所在的module中把两个依赖都加进来试试。
（注意，如果我们将所有的都打进一个包，这个问题就不存在了，但是在本地运行是不一样的，各自模块的编译输出文件在不同的地方。）在 &lt;code&gt;janusgraph-test&lt;/code&gt; 中添加：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;org.janusgraph&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;janusgraph-berkeleyje&amp;lt;/artifactId&amp;gt;
            &amp;lt;version&amp;gt;0.3.0-SNAPSHOT&amp;lt;/version&amp;gt;
        &amp;lt;/dependency&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;发现 &lt;code&gt;janusgraph-berkeleyje&lt;/code&gt;也依赖了 &lt;code&gt;janusgraph-test&lt;/code&gt;,又相互依赖了，好麻烦。我们写写代码一定要注意这个问题。这里我的解决方法是直接把 代码放到 &lt;code&gt;janusgraph-berkeleyje&lt;/code&gt; 中运行。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Exception in thread &amp;quot;main&amp;quot; java.lang.IllegalArgumentException: Could not find implementation class: org.janusgraph.diskstorage.es.ElasticSearchIndex
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;和上面一样，还依赖了 &lt;code&gt;janusgraph-es&lt;/code&gt;,我只好吧代码复制到 &lt;code&gt;janusgraph-es&lt;/code&gt; 的test代码块中运行（注意一点是test代码中），顺便在 &lt;code&gt;janusgraph-es&lt;/code&gt; 中 添加上&lt;code&gt;janusgraph-berkeleyje&lt;/code&gt;的依赖。
运行成功了，但是报了连接失败，是因为我本地没有启动es，我启动一下es：&lt;code&gt;elasticsearch&lt;/code&gt;
然后在运行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;Exception in thread &amp;quot;main&amp;quot; org.janusgraph.core.SchemaViolationException: Adding this property for key [~T$SchemaName] and value [rtname] violates a uniqueness constraint [SystemIndex#~T$SchemaName]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;经过google查到原因： &lt;a href=&#34;https://groups.google.com/forum/#!topic/aureliusgraphs/vZ_nTXlXj4k&#34;&gt;https://groups.google.com/forum/#!topic/aureliusgraphs/vZ_nTXlXj4k&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;This exception is thrown only when you already have added property key to index. So &amp;quot;name&amp;quot; is already added and next time when you run your program somewhere it is again adding &amp;quot;name&amp;quot; property key. So check if that particular code is running twice
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后我们可以在我们传入的配置文件找到：storage.directory=../db/berkeley  ，直接删除这个目录，再重新运行，就成功了：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;11:20:17,051  INFO GraphDatabaseConfiguration:1285 - Set default timestamp provider MICRO
11:20:17,296  INFO GraphDatabaseConfiguration:1492 - Generated unique-instance-id=c0a815a789637-dengzimings-MacBook-Pro-local1
11:20:17,547  INFO Backend:462 - Configuring index [search]
11:20:19,279  INFO Backend:177 - Initiated backend operations thread pool of size 8
11:20:19,461  INFO KCVSLog:753 - Loaded unidentified ReadMarker start time 2018-04-26T03:20:19.408Z into org.janusgraph.diskstorage.log.kcvs.KCVSLog$MessagePuller@73cd37c0
[GraphStep(edge,[]), HasStep([place.geoWithin(BUFFER (POINT (23.72 37.97), 0.44966))])]
[GraphStep(edge,[]), HasStep([place.geoWithin(BUFFER (POINT (23.72 37.97), 0.44966))])@[source], EdgeVertexStep(IN)@[god2], SelectOneStep(last,source), EdgeVertexStep(OUT)@[god1], SelectStep(last,[god1, god2],[value(name)])]
11:20:29,578  INFO ManagementLogger:192 - Received all acknowledgements for eviction [1]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后我们可以去 ../db/berkeley  目录查看，多了一些文件，这些文件的作用我们后续再分析。
然后我们取es查看：&lt;code&gt;curl -XGET &#39;localhost:9200/_cat/indices?v&amp;amp;pretty&#39;&lt;/code&gt; ，发现多了两个index:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yellow open   janusgraph_edges    QT-E7AV6SMWr8Cu_ywKsXg   5   1          6            0     13.7kb         13.7kb
yellow open   janusgraph_vertices gE4TSXFATnSZUWYdAf46Xg   5   1          6            0     10.9kb         10.9kb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;还可以具体查看内容。例如名字是titan的内容：&lt;code&gt;curl -XGET &#39;localhost:9200/janusgraph_vertices/_search?q=name:titan&amp;amp;pretty&#39;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;到现在我们第一个案例就结束了。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;g.E().has(&amp;quot;place&amp;quot;, geoWithin(Geoshape.circle(37.97, 23.72, 50)))
                .as(&amp;quot;source&amp;quot;).inV()
                .as(&amp;quot;god2&amp;quot;)
                .select(&amp;quot;source&amp;quot;).outV()
                .as(&amp;quot;god1&amp;quot;).select(&amp;quot;god1&amp;quot;, &amp;quot;god2&amp;quot;)
                .by(&amp;quot;name&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这种风格的代码实际上是groovy语言的代码，大家可以研究一下groovy语言。&lt;/p&gt;

&lt;p&gt;注意事项：
上述第一次运行问题的原因是 &lt;code&gt;janusgraph-core&lt;/code&gt;需要用到 &lt;code&gt;janusgraph-berkeleyje&lt;/code&gt;的类，
但是&lt;code&gt;janusgraph-berkeleyje&lt;/code&gt;是依赖 &lt;code&gt;janusgraph-core&lt;/code&gt;的，所以两个相互依赖了。
janus的做法是在core中使用反射，所以编译通过了，打包到了一起就没问题了。但是本地运行没法成功。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>janusgraph源码分析1-下载编译启动</title>
      <link>https://dengziming.github.io/post/titan/janusgraph%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%9010-gremin/</link>
      <pubDate>Thu, 26 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/titan/janusgraph%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%9010-gremin/</guid>
      
        <description>

&lt;p&gt;#
研究了好久的 neo4j源码，现在公司要换 janusgraph，只要半途而废开始研究 janusgraph 了
&lt;code&gt;https://github.com/JanusGraph/janusgraph&lt;/code&gt;和&lt;code&gt;http://janusgraph.org/&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&#34;一-调试&#34;&gt;一、调试&lt;/h2&gt;

&lt;p&gt;首先阅读以下 &lt;a href=&#34;http://tinkerpop.apache.org/docs/3.3.3/reference/#traversal&#34;&gt;http://tinkerpop.apache.org/docs/3.3.3/reference/#traversal&lt;/a&gt; ，了解一下。&lt;/p&gt;

&lt;p&gt;GraphTraversalSource g = graph.traversal();&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;
&lt;/code&gt;&lt;/pre&gt;
</description>
      
    </item>
    
    <item>
      <title>janusgraph源码分析2-实例debug</title>
      <link>https://dengziming.github.io/post/titan/janusgraph%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%902-%E5%AE%9E%E4%BE%8Bdebug/</link>
      <pubDate>Thu, 26 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/titan/janusgraph%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%902-%E5%AE%9E%E4%BE%8Bdebug/</guid>
      
        <description>

&lt;p&gt;#
研究了好久的 neo4j源码，现在公司要换 janusgraph，只要半途而废开始研究 janusgraph了
&lt;code&gt;https://github.com/JanusGraph/janusgraph&lt;/code&gt;和&lt;code&gt;http://janusgraph.org/&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&#34;一-第一遍调试&#34;&gt;一、第一遍调试&lt;/h2&gt;

&lt;p&gt;还是上次的例子 &lt;code&gt;FirstTest&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public class FirstTest {

    public static void main(String[] args) {

        /*
         * The example below will open a JanusGraph graph instance and load The Graph of the Gods dataset diagrammed above.
         * JanusGraphFactory provides a set of static open methods,
         * each of which takes a configuration as its argument and returns a graph instance.
         * This tutorial calls one of these open methods on a configuration
         * that uses the BerkeleyDB storage backend and the Elasticsearch index backend,
         * then loads The Graph of the Gods using the helper class GraphOfTheGodsFactory.
         * This section skips over the configuration details, but additional information about storage backends,
         * index backends, and their configuration are available in
         * Part III, “Storage Backends”, Part IV, “Index Backends”, and Chapter 13, Configuration Reference.
         */

        // Loading the Graph of the Gods Into JanusGraph
        JanusGraph graph = JanusGraphFactory
                .open(&amp;quot;janusgraph-dist/src/assembly/cfilter/conf/janusgraph-berkeleyje-es.properties&amp;quot;);

        GraphOfTheGodsFactory.load(graph);
        GraphTraversalSource g = graph.traversal();

        /*
         * The typical pattern for accessing data in a graph database is to first locate the entry point into the graph
         * using a graph index. That entry point is an element (or set of elements) 
         * — i.e. a vertex or edge. From the entry elements,
         * a Gremlin path description describes how to traverse to other elements in the graph via the explicit graph structure.
         * Given that there is a unique index on name property, the Saturn vertex can be retrieved.
         * The property map (i.e. the key/value pairs of Saturn) can then be examined.
         * As demonstrated, the Saturn vertex has a name of &amp;quot;saturn, &amp;quot; an age of 10000, and a type of &amp;quot;titan.&amp;quot;
         * The grandchild of Saturn can be retrieved with a traversal that expresses:
         * &amp;quot;Who is Saturn’s grandchild?&amp;quot; (the inverse of &amp;quot;father&amp;quot; is &amp;quot;child&amp;quot;). The result is Hercules.
         */
        // Global Graph Indices
        Vertex saturn = g.V().has(&amp;quot;name&amp;quot;, &amp;quot;saturn&amp;quot;).next();
        GraphTraversal&amp;lt;Vertex, Map&amp;lt;String, Object&amp;gt;&amp;gt; vertexMapGraphTraversal = g.V(saturn).valueMap();

        GraphTraversal&amp;lt;Vertex, Object&amp;gt; values = g.V(saturn).in(&amp;quot;father&amp;quot;).in(&amp;quot;father&amp;quot;).values(&amp;quot;name&amp;quot;);

        /*
         * The property place is also in a graph index. The property place is an edge property.
         * Therefore, JanusGraph can index edges in a graph index.
         * It is possible to query The Graph of the Gods for all events that have happened within 50 kilometers of Athens
          * (latitude:37.97 and long:23.72).
          * Then, given that information, which vertices were involved in those events.
         */
		System.out.println(g.E().has(&amp;quot;place&amp;quot;, geoWithin(Geoshape.circle(37.97, 23.72, 50))));
        System.out.println(g.E().has(&amp;quot;place&amp;quot;, geoWithin(Geoshape.circle(37.97, 23.72, 50)))
                .as(&amp;quot;source&amp;quot;).inV()
                .as(&amp;quot;god2&amp;quot;)
                .select(&amp;quot;source&amp;quot;).outV()
                .as(&amp;quot;god1&amp;quot;).select(&amp;quot;god1&amp;quot;, &amp;quot;god2&amp;quot;)
                .by(&amp;quot;name&amp;quot;));
    }

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;删除 db 文件夹，打上断点，开始debug，首先进入：JanusGraphFactory.open&lt;/p&gt;

&lt;p&gt;JanusGraphFactory is used to open or instantiate a JanusGraph graph database.
Opens a {@link JanusGraph} database configured according to the provided configuration.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public static JanusGraph open(ReadConfiguration configuration, String backupName) {
    final ModifiableConfiguration config = new ModifiableConfiguration(ROOT_NS, (WriteConfiguration) configuration, BasicConfiguration.Restriction.NONE);
    final String graphName = config.has(GRAPH_NAME) ? config.get(GRAPH_NAME) : backupName;
    final JanusGraphManager jgm = JanusGraphManagerUtility.getInstance();
    if (null != graphName) {
        Preconditions.checkState(jgm != null, JANUS_GRAPH_MANAGER_EXPECTED_STATE_MSG);
        return (JanusGraph) jgm.openGraph(graphName, gName -&amp;gt; new StandardJanusGraph(new GraphDatabaseConfiguration(configuration)));
    } else {
        if (jgm != null) {
            log.warn(&amp;quot;...&amp;quot;);
        }
        return new StandardJanusGraph(new GraphDatabaseConfiguration(configuration));
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;前面的部分先跳过，然后进入：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1. return new StandardJanusGraph(new GraphDatabaseConfiguration(configuration));
    // 构造方法，分为静态代码和构造方法，这部分目前是跳过，但是后续是重点和核心。
    1. 父类：JanusGraphBlueprintsGraph
        static {
        TraversalStrategies graphStrategies = TraversalStrategies.GlobalCache.getStrategies(Graph.class).clone()
                .addStrategies(AdjacentVertexFilterOptimizerStrategy.instance(), JanusGraphLocalQueryOptimizerStrategy.instance(), JanusGraphStepStrategy.instance());

        //Register with cache
        TraversalStrategies.GlobalCache.registerStrategies(StandardJanusGraph.class, graphStrategies);
        TraversalStrategies.GlobalCache.registerStrategies(StandardJanusGraphTx.class, graphStrategies);
        }
    2. 新建配置，A graph database configuration is uniquely associated with a graph database and must not be used for multiple databases
    
    new GraphDatabaseConfiguration(configuration)
        1. storeManager 
        final KeyColumnValueStoreManager storeManager = Backend.getStorageManager(localBasicConfiguration);
        final StoreFeatures storeFeatures = storeManager.getFeatures();
        2. 检查参数，配置等
    
    3. 然后是构造方法
        1. 成员变量
        private final SchemaCache.StoreRetrieval typeCacheRetrieval = new SchemaCache.StoreRetrieval() {}
        2. backend
        this.backend = configuration.getBackend();
            1. Backend backend = new Backend(configuration);
                1. KeyColumnValueStoreManager manager = getStorageManager(configuration);
                2. indexes = getIndexes(configuration);
                
                3. //这里的 KCVS 是 keycolumnvaluestorageManager
                managementLogManager = getKCVSLogManager(MANAGEMENT_LOG);
        		txLogManager = getKCVSLogManager(TRANSACTION_LOG);
        		userLogManager = getLogManager(USER_LOG);
        		
        		4. scanner = new StandardScanner(storeManager);
                
            2. backend.initialize(configuration);
                1. store 新建
                KeyColumnValueStore idStore = storeManager.openDatabase(config.get(IDS_STORE_NAME));
                KeyColumnValueStore edgeStoreRaw = storeManagerLocking.openDatabase(EDGESTORE_NAME);
            	KeyColumnValueStore indexStoreRaw = storeManagerLocking.openDatabase(INDEXSTORE_NAME);
                
                2. cacheEnabled
                edgeStore = new NoKCVSCache(edgeStoreRaw);
                indexStore = new NoKCVSCache(indexStoreRaw);
            3. storeFeatures = backend.getStoreFeatures();
        3. 初始化
        this.idAssigner = config.getIDAssigner(backend);
        this.idManager = idAssigner.getIDManager();
        this.serializer = config.getSerializer();
        StoreFeatures storeFeatures = backend.getStoreFeatures();
        this.indexSerializer = new IndexSerializer(configuration.getConfiguration(), this.serializer,
        this.backend.getIndexInformation(), storeFeatures.isDistributed() &amp;amp;&amp;amp; storeFeatures.isKeyOrdered());
        this.edgeSerializer = new EdgeSerializer(this.serializer);
        this.vertexExistenceQuery = edgeSerializer.getQuery(BaseKey.VertexExists, Direction.OUT, new EdgeSerializer.TypedInterval[0]).setLimit(1);
        this.queryCache = new RelationQueryCache(this.edgeSerializer);
        this.schemaCache = configuration.getTypeCache(typeCacheRetrieval);
        this.times = configuration.getTimestampProvider();
        
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后是open完成后：GraphOfTheGodsFactory.load(graph);&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;1. 得到management
JanusGraphManagement management = graph.openManagement();
    
    1. new ManagementSystem
        1. 启动 tx
        this.transaction = (StandardJanusGraphTx) graph.buildTransaction().disableBatchLoading().start();
            1.  graph.newTransaction(immutable);
                StandardJanusGraphTx tx = new StandardJanusGraphTx(this, configuration);
            	tx.setBackendTransaction(openBackendTransaction(tx));
            	openTransactions.add(tx);
2. 得到 PropertyKey
final PropertyKey name = management.makePropertyKey(&amp;quot;name&amp;quot;).dataType(String.class).make();
    1. return transaction.makePropertyKey(name);
        1. return new StandardPropertyKeyMaker(this, name, indexSerializer, attributeHandler);
            1. super(tx, name, indexSerializer, attributeHandler);
    2. public StandardPropertyKeyMaker dataType(Class&amp;lt;?&amp;gt; clazz)
    3. public PropertyKey make()
        1. TypeDefinitionMap definition = makeDefinition();        
        2. return tx.makePropertyKey(getName(), definition);
            1. return (PropertyKey) makeSchemaVertex(JanusGraphSchemaCategory.PROPERTYKEY, name, definition);
                1. ... 先跳过。
            
3. 新建 index
JanusGraphManagement.IndexBuilder nameIndexBuilder = management.buildIndex(&amp;quot;name&amp;quot;, Vertex.class).addKey(name);
    1. 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;调用：JanusGraphManagement management = graph.openManagement();然后：management.makeEdgeLabel(&amp;ldquo;father&amp;rdquo;).multiplicity(Multiplicity.MANY2ONE).make();&lt;/p&gt;

&lt;p&gt;然后就是查询数据库：&lt;code&gt;Vertex saturn = g.V().has(&amp;quot;name&amp;quot;, &amp;quot;saturn&amp;quot;).next();&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&#34;二-第2遍调试&#34;&gt;二、第2遍调试&lt;/h2&gt;

&lt;p&gt;这次我们多关注一点细节实现，包括几个部分：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;Backend backend = new Backend(configuration);
backend.~~~

this.idAssigner = config.getIDAssigner(backend);
this.idManager = idAssigner.getIDManager();

JanusGraphManagement management = graph.openManagement();
management.makePropertyKey(&amp;quot;name&amp;quot;).dataType(String.class).make();
management.buildIndex(&amp;quot;name&amp;quot;, Vertex.class).addKey(name);

Vertex tartarus = tx.addVertex(T.label, &amp;quot;location&amp;quot;, &amp;quot;name&amp;quot;, &amp;quot;tartarus&amp;quot;);
jupiter.addEdge(&amp;quot;father&amp;quot;, saturn);


&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;backend&#34;&gt;Backend&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public StandardJanusGraph(GraphDatabaseConfiguration configuration) 
{
    this.backend = configuration.getBackend();
    {
        Backend backend = new Backend(configuration);
        {
            this.configuration = configuration;
            KeyColumnValueStoreManager manager = getStorageManager(configuration);
            {
                反射生成一个 KeyColumnValueStoreManager 实现类
            }
            indexes = getIndexes(configuration);
            {
                IndexProvider provider = getImplementationClass(config.restrictTo(index), config.get(INDEX_BACKEND,index),
                    StandardIndexProvider.getAllProviderClasses());
                -- org.janusgraph.diskstorage.es.ElasticSearchIndex
                builder.put(index, provider);
                builder.build();
            }
            storeFeatures = storeManager.getFeatures();
            {
                ...
            }
            ...
        }
        
        backend.initialize(configuration);
        {
            KeyColumnValueStore idStore = storeManager.openDatabase(config.get(IDS_STORE_NAME));
            {
                openDatabase(&amp;quot;janusgraph_ids&amp;quot;, EMPTY)
                {
                    if (!stores.containsKey(name) || stores.get(name).isClosed()) {
                         OrderedKeyValueStoreAdapter store = wrapKeyValueStore(manager.openDatabase(name), keyLengths);
                         {
                             public BerkeleyJEKeyValueStore openDatabase(String name) throws BackendException 
                             {
                                 Database db = environment.openDatabase(null, name, dbConfig);
                                 BerkeleyJEKeyValueStore store = new BerkeleyJEKeyValueStore(name, db, this);
                                 stores.put(name, store);
                             }
                         }
                         stores.put(name, store);
                     }
                     return stores.get(name);
                }
            }
            
            KeyColumnValueStore edgeStoreRaw = storeManagerLocking.openDatabase(EDGESTORE_NAME);
            {
                同上：  
                openDatabase(&amp;quot;edgestore&amp;quot;, EMPTY)
            }
            KeyColumnValueStore indexStoreRaw = storeManagerLocking.openDatabase(INDEXSTORE_NAME);
            {
                同上：  
                openDatabase(&amp;quot;graphindex&amp;quot;, EMPTY)
            }
            
            txLogManager.openLog(SYSTEM_TX_LOG_NAME);
            managementLogManager.openLog(SYSTEM_MGMT_LOG_NAME);
            txLogStore = new NoKCVSCache(storeManager.openDatabase(SYSTEM_TX_LOG_NAME));
            
            KeyColumnValueStore systemConfigStore = storeManagerLocking.openDatabase(SYSTEM_PROPERTIES_STORE_NAME);
            {
                同上：  
                openDatabase(&amp;quot;system_properties&amp;quot;, EMPTY)
            }
            
        }
        storeFeatures = backend.getStoreFeatures();
    }
    
    this.idAssigner = config.getIDAssigner(backend);
    this.idManager = idAssigner.getIDManager();
    
}

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;management&#34;&gt;management&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;JanusGraphManagement management = graph.openManagement();
{
   new ManagementSystem(this,backend.getGlobalSystemConfig(),backend.getSystemMgmtLog(), managementLogger, schemaCache);
   //参数分别是 graph config Log managementLogger schemaCache
   {
       this.transaction = (StandardJanusGraphTx) graph.buildTransaction().disableBatchLoading().start();
       {
           graph.buildTransaction()
           {
               new StandardTransactionBuilder(getConfiguration(), this);
               {
                   
               }
           }
           disableBatchLoading()
           {
               
           }
           start()
           {
               new ImmutableTxCfg
               graph.newTransaction(immutable);
               {
                    StandardJanusGraphTx tx = new StandardJanusGraphTx(this, configuration);
                    {
                        父类： JanusGraphBlueprintsTransaction
                        太过复杂，跳过
                    }
                    tx.setBackendTransaction(openBackendTransaction(tx));
                    {
                        openBackendTransaction(tx)
                        {
                            IndexSerializer.IndexInfoRetriever retriever = indexSerializer.getIndexInfoRetriever(tx);
                            return backend.beginTransaction(tx.getConfiguration(), retriever);
                            {
                                StoreTransaction tx = storeManagerLocking.beginTransaction(configuration);
                                CacheTransaction cacheTx = new CacheTransaction(tx, storeManagerLocking, bufferSize, maxWriteTime, configuration.hasEnabledBatchLoading());
                                final Map&amp;lt;String, IndexTransaction&amp;gt; indexTx = new HashMap&amp;lt;&amp;gt;(indexes.size());
        						for (Map.Entry&amp;lt;String, IndexProvider&amp;gt; entry : indexes.entrySet()) {
        						    indexTx.put(entry.getKey(), new IndexTransaction(entry.getValue(), indexKeyRetriever.get(entry.getKey()), configuration, maxWriteTime));
        						}
        						return new BackendTransaction(cacheTx, configuration, storeFeatures,
                					edgeStore, indexStore, txLogStore,
                					maxReadTime, indexTx, threadPool);
                            }
                        }
                    }
                    openTransactions.add(tx);
                    return tx;
               }
           }
           
       }
   }
}

final PropertyKey name = management.makePropertyKey(&amp;quot;name&amp;quot;).dataType(String.class).make();
{
    management.makePropertyKey(&amp;quot;name&amp;quot;)
    {
        transaction.makePropertyKey(name);
        {
            new StandardPropertyKeyMaker(this, name, indexSerializer, attributeHandler);
            {
                super
                {
                    StandardRelationTypeMaker
                }
            }
        }
    }
    dataType(String.class)
    {
        dataType = clazz;
    }
    make();
    {
        new TypeDefinitionMap();
        tx.makePropertyKey(getName(), definition);
        {
            (PropertyKey) makeSchemaVertex(JanusGraphSchemaCategory.PROPERTYKEY, name, definition);
            {
                schemaVertex = new PropertyKeyVertex(this, IDManager.getTemporaryVertexID(IDManager.VertexIDType.UserPropertyKey, temporaryIds.nextID()), ElementLifeCycle.New);
                {
                    //一层层嵌套
                    
                }
            }
        }
    }
}

management.buildIndex(&amp;quot;name&amp;quot;, Vertex.class).addKey(name).unique().buildCompositeIndex();
{
    new IndexBuilder(indexName, ElementCategory.getByClazz(elementType));
    {
        
    }
    addKey(name)
    {
        keys.put(key, null);
    }
    unique()
    {
        unique = true;
    }
    buildCompositeIndex()
    {
        createCompositeIndex(indexName, elementCategory, unique, constraint, keyArr);
        {
            JanusGraphSchemaVertex indexVertex = transaction.makeSchemaVertex(JanusGraphSchemaCategory.GRAPHINDEX, indexName, def);
            {
                schemaVertex = new JanusGraphSchemaVertex(this, IDManager.getTemporaryVertexID(IDManager.VertexIDType.GenericSchemaType,temporaryIds.nextID()), ElementLifeCycle.New);
                {
                    //一层层嵌套
                    
                }
            }
            addSchemaEdge(indexVertex, keys[i], TypeDefinitionCategory.INDEX_FIELD, paras);
            
            updateSchemaVertex(indexVertex);
            JanusGraphIndexWrapper index = new JanusGraphIndexWrapper(indexVertex.asIndexType());
            updateIndex(index, SchemaAction.REGISTER_INDEX);
            return index;
        }
    }
    
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;containsvertexlabel&#34;&gt;containsVertexLabel&lt;/h3&gt;

&lt;p&gt;mgmt.getVertexLabels().iterator()
mgmt.containsVertexLabel(label)
这两个方法都可以得到 VertexLABEL&lt;/p&gt;

&lt;p&gt;首先看 mgmt.getVertexLabels().iterator(), 这里面首先通过了 guava 的 abstractIterator 转到一个 ResultSetIterator&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;
public ResultSetIterator(Iterator&amp;lt;R&amp;gt; inner, int limit) {
    this.iter = inner;
    this.limit = limit;
    count = 0;
    this.current = null;
    this.next = nextInternal();
    {
        QueryProcessor$LimitAdajustingIterator.hasNext()
        {
            ....省去一步调用
            executor.execute(query, backendQuery, executionInfo, profiler);
            {
                iter = new SubqueryIterator(indexQuery.getQuery(0), indexSerializer, txHandle, indexCache, indexQuery.getLimit(), getConversionFunction(query.getResultType()),
                        retrievals.isEmpty() ? null: QueryUtil.processIntersectingRetrievals(retrievals, indexQuery.getLimit()));
                {
                    stream = indexSerializer.query(subQuery, tx).map(r -&amp;gt; {
                        currentIds.add(r);
                        return r;
                    });
                    {
                        final List&amp;lt;EntryList&amp;gt; rs = sq.execute(tx);
                        {
                            EntryList next =tx.indexQuery(ksq.updateLimit(getLimit()-total));
                            {
                                return exe.call();
                                {
                                    return cacheEnabled?indexStore.getSlice(query, storeTx):
                                        indexStore.getSliceNoCache(query, storeTx);
                                    {
                                        CassandraThriftKeyColumnValueStore.getNamesSlice(ImmutableList.of(key),query,txh);
                                    }
                                }
                            }
                        }
                        
                    }
                }
            }
        }
        
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这上面已经是省略很多步骤的调用栈。。。&lt;/p&gt;

&lt;p&gt;mgmt.containsVertexLabel(label) 调用栈稍微少了一点：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;JanusGraphSchemaVertex getSchemaVertex(String schemaName)
{
    id = retriever.retrieveSchemaByName(schemaName);
    {
        JanusGraphVertex v = Iterables.getOnlyElement(QueryUtil.getVertices(consistentTx, BaseKey.SchemaName, typeName), null);
        {
            new ResultSetIterator()
            {
                ....
                runWithMetrics
                iter = new SubqueryIterator(indexQuery.getQuery(0), indexSerializer, txHandle, indexCache, indexQuery.getLimit(), getConversionFunction(query.getResultType()),
                        retrievals.isEmpty() ? null: QueryUtil.processIntersectingRetrievals(retrievals, indexQuery.getLimit()));
                {
                    类似上面
                }
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;makevertexlabel&#34;&gt;makeVertexLabel&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;mgmt.makeVertexLabel(vType.toString()).make();
{
    StandardVertexLabelMaker.make
    return (VertexLabelVertex)tx.makeSchemaVertex(JanusGraphSchemaCategory.VERTEXLABEL,name,def);
    {
        
        public final JanusGraphSchemaVertex makeSchemaVertex(JanusGraphSchemaCategory schemaCategory, String name, TypeDefinitionMap definition) 
        {
            1. new VertexLabelVertex
            schemaVertex = new VertexLabelVertex(this, IDManager.getTemporaryVertexID(IDManager.VertexIDType.GenericSchemaType,temporaryIds.nextID()), ElementLifeCycle.New);
            2. graph.assignID(schemaVertex, BaseVertexLabel.DEFAULT_VERTEXLABEL);
            
            3. addProperty(schemaVertex, BaseKey.SchemaName, schemaCategory.getSchemaName(name));
            
            4. updateSchemaVertex(schemaVertex);
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;assignID应该是 生产者消费者模式。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;IDBlock idBlock = idAuthority.getIDBlock(partition, idNamespace, renewTimeout);
{
    long nextStart = getCurrentID(partitionKey);
    {
        ......
        return idStore.getSlice(new KeySliceQuery(partitionKey, LOWER_SLICE, UPPER_SLICE).setLimit(5), txh);
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;containspropertykey&#34;&gt;containsPropertyKey&lt;/h3&gt;

&lt;h3 id=&#34;makepropertykey&#34;&gt;makePropertyKey&lt;/h3&gt;

&lt;h3 id=&#34;containsedgelabel&#34;&gt;containsEdgeLabel&lt;/h3&gt;

&lt;h3 id=&#34;makeedgelabel&#34;&gt;makeEdgeLabel&lt;/h3&gt;

&lt;p&gt;基本上和上面类似，接下来深入分析一下这些调用栈涉及到的类。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>janusgraph源码分析3-调用栈</title>
      <link>https://dengziming.github.io/post/titan/janusgraph%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%903-%E8%B0%83%E7%94%A8%E6%A0%88/</link>
      <pubDate>Thu, 22 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/titan/janusgraph%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%903-%E8%B0%83%E7%94%A8%E6%A0%88/</guid>
      
        <description>

&lt;p&gt;我们可以在比较关键的地方大断点，然后分析整个调用栈，进行进一步分析。哪里是关键点是需要一定经验判断的。&lt;/p&gt;

&lt;p&gt;例如我们基于 hadoop spark 等框架的时候，我们写的代码就是关键的，打断点可以看到合适调用，怎么被调用。
我们关心怎么写数据，可以在和底层数据交互的地方打断点。总之我们关心谁就在哪里打断点。&lt;/p&gt;

&lt;p&gt;记住：打断点的地方基本上是最终的调用点。&lt;/p&gt;

&lt;h2 id=&#34;整体调试找关键&#34;&gt;整体调试找关键&lt;/h2&gt;

&lt;p&gt;首先是存储类，我们使用本地文件存储，存储使用类是：&lt;code&gt;com.sleepycat.je.Database&lt;/code&gt; 这个类具体功能是啥可以具体研究。我们发现它有 get delete put 等方法，我们可以打上断点。然后查看调用栈。&lt;/p&gt;

&lt;p&gt;得到 普通 的调用信息：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;&amp;quot;main@1&amp;quot; prio=5 tid=0x1 nid=NA runnable
  java.lang.Thread.State: RUNNABLE
	  at com.sleepycat.je.Database.put(Database.java:1574)
	  at com.sleepycat.je.Database.put(Database.java:1627)
	  at org.janusgraph.diskstorage.berkeleyje.BerkeleyJEKeyValueStore.insert(BerkeleyJEKeyValueStore.java:195)
	  at org.janusgraph.diskstorage.berkeleyje.BerkeleyJEKeyValueStore.insert(BerkeleyJEKeyValueStore.java:184)
	  at org.janusgraph.diskstorage.keycolumnvalue.keyvalue.OrderedKeyValueStoreAdapter.mutate(OrderedKeyValueStoreAdapter.java:99)
	  at org.janusgraph.diskstorage.configuration.backend.KCVSConfiguration$2.call(KCVSConfiguration.java:154)
	  at org.janusgraph.diskstorage.configuration.backend.KCVSConfiguration$2.call(KCVSConfiguration.java:149)
	  at org.janusgraph.diskstorage.util.BackendOperation.execute(BackendOperation.java:147)
	  at org.janusgraph.diskstorage.util.BackendOperation$1.call(BackendOperation.java:161)
	  at org.janusgraph.diskstorage.util.BackendOperation.executeDirect(BackendOperation.java:68)
	  at org.janusgraph.diskstorage.util.BackendOperation.execute(BackendOperation.java:54)
	  at org.janusgraph.diskstorage.util.BackendOperation.execute(BackendOperation.java:158)
	  at org.janusgraph.diskstorage.configuration.backend.KCVSConfiguration.set(KCVSConfiguration.java:149)
	  at org.janusgraph.diskstorage.configuration.backend.KCVSConfiguration.set(KCVSConfiguration.java:126)
	  at org.janusgraph.diskstorage.configuration.ModifiableConfiguration.set(ModifiableConfiguration.java:40)
	  at org.janusgraph.diskstorage.configuration.ModifiableConfiguration.setAll(ModifiableConfiguration.java:47)
	  at org.janusgraph.graphdb.configuration.GraphDatabaseConfiguration.&amp;lt;init&amp;gt;(GraphDatabaseConfiguration.java:1266)
	  at org.janusgraph.core.JanusGraphFactory.open(JanusGraphFactory.java:160)
	  at org.janusgraph.core.JanusGraphFactory.open(JanusGraphFactory.java:131)
	  at org.janusgraph.core.JanusGraphFactory.open(JanusGraphFactory.java:78)
	  at org.janusgraph.test.dengziming.FirstTest.main(FirstTest.java:37)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从下往上可以看出，顺序：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;new GraphDatabaseConfiguration
ModifiableConfiguration.setAll(getGlobalSubset(localBasicConfiguration.getAll())); 
KCVSConfiguration.set(key,value,null,false);
BackendOperation.execute(new BackendOperation.Transactional&amp;lt;Boolean&amp;gt;() {@Override public Boolean call}
然后调用 上面new 的 BackendOperation.Transactional 的 call 方法
然后是 store.mutate
status = db.put(tx, key.as(ENTRY_FACTORY), value.as(ENTRY_FACTORY));
put(txn, key, data, Put.OVERWRITE, null);
result = cursor.putInternal(key, data, putType, options);
最终调用的是 cursor.putNotify 插入数据。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个 put 会多次调用，config 会设置 &amp;ldquo;startup-time&amp;rdquo; 等属性，都是通过这个put方法实现。&lt;/p&gt;

&lt;p&gt;第二次用到这个方法是 创建 VertexLabel 的时候会分配 id， 这时候我们可以看一下更详细的调用栈：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;&amp;quot;JanusGraphID(0)(4)[0]@5358&amp;quot; prio=5 tid=0x24 nid=NA runnable
  java.lang.Thread.State: RUNNABLE
	  at com.sleepycat.je.dbi.CursorImpl.insertRecordInternal(CursorImpl.java:1364)
	  at com.sleepycat.je.dbi.CursorImpl.insertOrUpdateRecord(CursorImpl.java:1221)
	  at com.sleepycat.je.Cursor.putNoNotify(Cursor.java:2962)
	  at com.sleepycat.je.Cursor.putNotify(Cursor.java:2800)
	  at com.sleepycat.je.Cursor.putNoDups(Cursor.java:2647)
	  at com.sleepycat.je.Cursor.putInternal(Cursor.java:2478)
	  - locked &amp;lt;0x1536&amp;gt; (a com.sleepycat.je.Transaction)
	  at com.sleepycat.je.Cursor.putInternal(Cursor.java:830)
	  at com.sleepycat.je.Database.put(Database.java:1574)
	  at com.sleepycat.je.Database.put(Database.java:1627)
	  at org.janusgraph.diskstorage.berkeleyje.BerkeleyJEKeyValueStore.insert(BerkeleyJEKeyValueStore.java:195)
	  at org.janusgraph.diskstorage.berkeleyje.BerkeleyJEKeyValueStore.insert(BerkeleyJEKeyValueStore.java:184)
	  at org.janusgraph.diskstorage.keycolumnvalue.keyvalue.OrderedKeyValueStoreAdapter.mutate(OrderedKeyValueStoreAdapter.java:99)
	  at org.janusgraph.diskstorage.idmanagement.ConsistentKeyIDAuthority.lambda$getIDBlock$1(ConsistentKeyIDAuthority.java:261)
	  at org.janusgraph.diskstorage.idmanagement.ConsistentKeyIDAuthority$$Lambda$71.1795053717.call(Unknown Source:-1)
	  at org.janusgraph.diskstorage.util.BackendOperation.execute(BackendOperation.java:147)
	  at org.janusgraph.diskstorage.idmanagement.ConsistentKeyIDAuthority.getIDBlock(ConsistentKeyIDAuthority.java:260)
	  - locked &amp;lt;0x14f8&amp;gt; (a org.janusgraph.diskstorage.idmanagement.ConsistentKeyIDAuthority)
	  at org.janusgraph.graphdb.database.idassigner.StandardIDPool$IDBlockGetter.call(StandardIDPool.java:288)
	  at org.janusgraph.graphdb.database.idassigner.StandardIDPool$IDBlockGetter.call(StandardIDPool.java:255)
	  ...
	  at java.lang.Thread.run(Thread.java:745)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面的调用栈没有显示这么多，实际上我们也没必要关心 &lt;code&gt;com.sleepycat.je.Database.put(Database.java:1627)&lt;/code&gt; 之后的东西，
因为这些东西都是 数据库的写 API，而生产环境我们会使用 hbase和cassandra ，所以每次只要 debug 到 KeyColumnValueStore 的 相应方法即可，再 debug 就是数据库的方法。&lt;/p&gt;

&lt;p&gt;到这里我们明白，增删改查都是 通过 KeyColumnValueStore 类完成。接下来我们直接在 BerkeleyJEKeyValueStore 的 增删改查方法 打断点就行。&lt;/p&gt;

&lt;h3 id=&#34;management-commit&#34;&gt;management.commit();&lt;/h3&gt;

&lt;p&gt;management 是用来操作 schema 的类，我们可以猜测 schema 也是以系统属性的方式存在数据库中。通过打断点发现，前面的操作都没有触发 BerkeleyJEKeyValueStore 的insert ，直到 commit，
先取出调用栈：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;&amp;quot;main@1&amp;quot; prio=5 tid=0x1 nid=NA runnable
  java.lang.Thread.State: RUNNABLE
	  at org.janusgraph.diskstorage.berkeleyje.BerkeleyJEKeyValueStore.insert(BerkeleyJEKeyValueStore.java:195)
	  at org.janusgraph.diskstorage.berkeleyje.BerkeleyJEKeyValueStore.insert(BerkeleyJEKeyValueStore.java:184)
	  at org.janusgraph.diskstorage.berkeleyje.BerkeleyJEStoreManager.mutateMany(BerkeleyJEStoreManager.java:208)
	  at org.janusgraph.diskstorage.keycolumnvalue.keyvalue.OrderedKeyValueStoreManagerAdapter.mutateMany(OrderedKeyValueStoreManagerAdapter.java:125)
	  at org.janusgraph.diskstorage.keycolumnvalue.cache.CacheTransaction$1.call(CacheTransaction.java:94)
	  at org.janusgraph.diskstorage.keycolumnvalue.cache.CacheTransaction$1.call(CacheTransaction.java:91)
	  at org.janusgraph.diskstorage.util.BackendOperation.executeDirect(BackendOperation.java:68)
	  at org.janusgraph.diskstorage.util.BackendOperation.execute(BackendOperation.java:54)
	  at org.janusgraph.diskstorage.keycolumnvalue.cache.CacheTransaction.persist(CacheTransaction.java:91)
	  at org.janusgraph.diskstorage.keycolumnvalue.cache.CacheTransaction.flushInternal(CacheTransaction.java:139)
	  at org.janusgraph.diskstorage.keycolumnvalue.cache.CacheTransaction.commit(CacheTransaction.java:196)
	  at org.janusgraph.diskstorage.BackendTransaction.commitStorage(BackendTransaction.java:134)
	  at org.janusgraph.graphdb.database.StandardJanusGraph.commit(StandardJanusGraph.java:733)
	  at org.janusgraph.graphdb.transaction.StandardJanusGraphTx.commit(StandardJanusGraphTx.java:1372)
	  - locked &amp;lt;0x113a&amp;gt; (a org.janusgraph.graphdb.transaction.StandardJanusGraphTx)
	  at org.janusgraph.graphdb.database.management.ManagementSystem.commit(ManagementSystem.java:239)
	  - locked &amp;lt;0x102b&amp;gt; (a org.janusgraph.graphdb.database.management.ManagementSystem)
	  at org.janusgraph.example.GraphOfTheGodsFactory.load(GraphOfTheGodsFactory.java:111)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里面好像还有锁，这个先不讨论。&lt;/p&gt;

&lt;p&gt;主要的几个调用：&lt;/p&gt;

&lt;p&gt;StandardJanusGraphTx.commit()&lt;/p&gt;

&lt;p&gt;StandardJanusGraph.commit(addedRelations.getAll(), deletedRelations.values(), this); &amp;ndash; 这个 commit 的逻辑挺复杂，需要仔细查看。&lt;/p&gt;

&lt;p&gt;BackendTransaction.commitStorage();&lt;/p&gt;

&lt;p&gt;CacheTransaction.commit()&lt;/p&gt;

&lt;p&gt;OrderedKeyValueStoreManagerAdapter.mutateMany&lt;/p&gt;

&lt;p&gt;BerkeleyJEStoreManager.mutateMany(subMutations, tx);&lt;/p&gt;

&lt;p&gt;BerkeleyJEKeyValueStore.insert();&lt;/p&gt;

&lt;p&gt;然后接下来就是一个个分析这几个类每一个的属性和方法。&lt;/p&gt;

&lt;p&gt;首先看一下类的继承结构&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SchemaInspector	
	StandardJanusGraphTx (org.janusgraph.graphdb.transaction)
	SchemaManager (org.janusgraph.core.schema)
	    Transaction (org.janusgraph.core)
	        JanusGraphTransaction (org.janusgraph.core)
	            JanusGraphBlueprintsTransaction (org.janusgraph.graphdb.tinkerpop)
	                StandardJanusGraphTx (org.janusgraph.graphdb.transaction)
	        JanusGraph (org.janusgraph.core)
	            JanusGraphBlueprintsGraph (org.janusgraph.graphdb.tinkerpop)
	                StandardJanusGraph (org.janusgraph.graphdb.database)
	    JanusGraphManagement (org.janusgraph.core.schema)
	        ManagementSystem (org.janusgraph.graphdb.database.management)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;SchemaInspector 接口定义了检查 schema 的一些方法，
例如：containsRelationType getRelationType containsPropertyKey getOrCreatePropertyKey getEdgeLabel getOrCreateVertexLabel
这些方法有四类，分别是是 RelationType 相关的，PropertyKey 相关，EdgeLabel 相关，VertexLabel 相关。这四个代表啥大家应该都清楚了。&lt;/p&gt;

&lt;p&gt;SchemaManager 接口 在 SchemaInspector 的基础上添加了 6 个方法 ：makePropertyKey makeEdgeLabel makeVertexLabel addProperties addProperties addConnection 。
其实前三个返回的是 Maker，后面三个返回的就是 Label。这六个方法左右主要是给 schema 添加更多信息，例如添加 properties。&lt;/p&gt;

&lt;p&gt;Transaction 继承自 SchemaManager 和 Graph ，定义了 addVertex 和 query 等操作。很奇怪为什么只有 addVertex 没有 addEdge 和 addProperty 的操作。&lt;/p&gt;

&lt;p&gt;JanusGraphManagement 继承自 SchemaManager 和 JanusGraphConfiguration ，定义了 buildEdgeIndex buildPropertyIndex commit 等操作
大部分都和 index 相关，例如构建查询更新。还有 getRelationTypes getVertexLabels 两个方法。&lt;/p&gt;

&lt;p&gt;ManagementSystem 继承自 JanusGraphManagement ，通过代理 StandardJanusGraphTx ，实现了 getGraphIndex commit 等操作。&lt;/p&gt;

&lt;p&gt;JanusGraphTransaction 继承自 Transaction ，定义了 addVertex getVertex commit rollback 等，和 Transaction 不同的是他的这些方法操作的都是 id，而后者操作的是 用户传入的 String&lt;/p&gt;

&lt;p&gt;JanusGraphBlueprintsTransaction 继承自 JanusGraphTransaction ，目前看到的就是简单封装一下抽象方法，同时实现了 addVertex 方法。&lt;/p&gt;

&lt;p&gt;StandardJanusGraphTx 继承自 JanusGraphBlueprintsTransaction ，实现了抽象的方法。&lt;/p&gt;

&lt;p&gt;JanusGraph 继承自 Transaction， 定义了 buildTransaction openManagement close 等方法。&lt;/p&gt;

&lt;p&gt;JanusGraphBlueprintsGraph 继承自 JanusGraph ，通过 ThreadLocal 实现线程隔离。
StandardJanusGraph 继承自 JanusGraphBlueprintsGraph 就是我们使用的 Graph 。&lt;/p&gt;

&lt;p&gt;所以了解janus比较重要的是 StandardJanusGraphTx ，了解多线程的 JanusGraphBlueprintsGraph。&lt;/p&gt;

&lt;p&gt;从继承结构大概可以看出所有的操作分为数据操作和 schema 操作，而分别由 JanusGraph 和 JanusGraphManagement 完成，实际上都是代理或者适配装饰了 StandardJanusGraphTx。StandardJanusGraphTx 内容很多。&lt;/p&gt;

&lt;h3 id=&#34;standardjanusgraph&#34;&gt;StandardJanusGraph&lt;/h3&gt;

&lt;p&gt;上面我们已经看出了实际上最重要的就是  StandardJanusGraphTx 的实现逻辑，我们就以他为入口，而不是 main 方法。它的构造方法里面需要用到 StandardJanusGraph ，我们先大概了解一下 。&lt;/p&gt;

&lt;h4 id=&#34;我们先看一下它的属性&#34;&gt;我们先看一下它的属性：&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;log
config
backend
idManager
idAssigner
times
indexSerializer
edgeSerializer
serializer
vertexExistenceQuery
queryCache
schemaCache
managementLogger
shutdownHook
isOpen
txCounter
openTransactions
name
typeCacheRetrieval
SCHEMA_FILTER
NO_SCHEMA_FILTER
NO_FILTER
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;GraphDatabaseConfiguration config 是图的配置，由于配置也是保存在数据库，所以也是需要访问数据库的。&lt;/p&gt;

&lt;p&gt;Backend backend 是在 config.getBackend 中初始化的，Backend 的构造方法很复杂，主要创建出了 StoreManager indexes txLogManager 等管理存储很重要的属性。&lt;/p&gt;

&lt;p&gt;idManager 和 idAssigner 都是和 id 相关的。 所属类为 IDManager ，VertexIDAssigner，有比较复杂的id分配算法。&lt;/p&gt;

&lt;p&gt;IndexSerializer 和 EdgeSerializer 、Serializer 用于序列化，Serializer 在 config 中初始化，其他两个都是基于 Serializer 的封装。&lt;/p&gt;

&lt;p&gt;vertexExistenceQuery:SliceQuery queryCache:RelationQueryCache schemaCache:SchemaCache 都是 cache 相关。&lt;/p&gt;

&lt;p&gt;managementLogger 是 用来记录操作日志的。&lt;/p&gt;

&lt;p&gt;typeCacheRetrieval ，看到 Retrieval 就知道是获取某些属性用的，他通过 &lt;code&gt;QueryUtil.getVertices(consistentTx, BaseKey.SchemaName, typeName)&lt;/code&gt; 获得 JanusGraphVertex。&lt;/p&gt;

&lt;h4 id=&#34;然后再看方法&#34;&gt;然后再看方法&lt;/h4&gt;

&lt;p&gt;除了 getset 以外，主要是：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;isOpen
isClosed
close
closeInternal
prepareCommit
commit
openManagement
newTransaction
buildTransaction
newThreadBoundTransaction
newTransaction
openBackendTransaction
closeTransaction
getVertexIDs
edgeQuery
edgeMultiQuery
assignID
assignID
acquireLock
acquireLock
getTTL
getTTL
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;和 transaction 有关的打开关闭提交等，查询边和顶点，分配id，获得锁。这里的 edgeQuery 并不是查询边，而是查询 edgestore 这个表格，这个表格存放了所有的数据。
细心分析发现，这些方法主要都是进行查询操作，得到查询结果 List&lt;EntryList&gt;，并没有进行数据增删改查的操作 API。&lt;/p&gt;

&lt;h3 id=&#34;managementsystem&#34;&gt;ManagementSystem&lt;/h3&gt;

&lt;p&gt;StandardJanusGraph 用来操作数据，而 ManagementSystem 主要是管理 schema。&lt;/p&gt;

&lt;h4 id=&#34;属性&#34;&gt;属性：&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;LOGGER
CURRENT_INSTANCE_SUFFIX
graph
sysLog
managementLogger
transactionalConfig
modifyConfig
userConfig
schemaCache
transaction
updatedTypes
evictGraphFromCache
updatedTypeTriggers
txStartTime
graphShutdownRequired
isOpen
configVerifier
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;graph 和 managementLogger 就是上面的 StandardJanusGraph 和 managementLogger。sysLog 也是和日志有关。&lt;/p&gt;

&lt;p&gt;TransactionalConfiguration 是事务的配置，实际上他应该是记录了变化，能够判断是否有改变，从而进行 commit 和 rollback&lt;/p&gt;

&lt;p&gt;SchemaCache 就是 StandardJanusGraph 的 SchemaCache。&lt;/p&gt;

&lt;p&gt;transaction 是 StandardJanusGraphTx。&lt;/p&gt;

&lt;p&gt;updatedTypes 应该也是记录更新&lt;/p&gt;

&lt;p&gt;其他的暂时还不太懂。&lt;/p&gt;

&lt;h4 id=&#34;方法&#34;&gt;方法：&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;IndexBuilder
GraphCacheEvictionCompleteTrigger
EmptyIndexJobFuture
UpdateStatusTrigger
IndexJobStatus
IndexIdentifier
ManagementSystem

getOpenInstancesInternal
getOpenInstances
forceCloseInstance
ensureOpen
commit
rollback
isOpen
close
getWrappedTx
addSchemaEdge
getSchemaElement
buildEdgeIndex
buildEdgeIndex
buildPropertyIndex
buildPropertyIndex
buildRelationTypeIndex
composeRelationTypeIndexName
containsRelationIndex
getRelationIndex
getRelationIndexes
getGraphIndexDirect
containsGraphIndex
getGraphIndex
getGraphIndexes
awaitGraphIndexStatus
awaitRelationIndexStatus
checkIndexName
createMixedIndex
addIndexKey
createCompositeIndex
buildIndex
updateIndex
evictGraphFromCache
setUpdateTrigger
setStatus
setStatusVertex
setStatusEdges
getIndexJobStatus
changeName
updateConnectionEdgeConstraints
getSchemaVertex
updateSchemaVertex
getConsistency
setConsistency
getTTL
setTTL
setTypeModifier
containsRelationType
getRelationType
containsPropertyKey
getPropertyKey
containsEdgeLabel
getOrCreateEdgeLabel
getOrCreatePropertyKey
getEdgeLabel
makePropertyKey
makeEdgeLabel
getRelationTypes
containsVertexLabel
getVertexLabel
getOrCreateVertexLabel
makeVertexLabel
addProperties
addProperties
addConnection
getVertexLabels
get
set
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;强制关闭、操作事务、添加顶点边Label属性索引。&lt;/p&gt;

&lt;p&gt;索引都是  buildRelationTypeIndex 方法，说明 RelationType(PropertyKey 和 EdgeLabel)才有索引，分别是 graphIndex 和 vertexIncdicentIndex ，VertexLabel 没有索引。
而 getVertexLabels 等带s的方法 是 调用 QueryUtil.getVertices ，说明得到所有的需要查询数据库。&lt;/p&gt;

&lt;p&gt;很多方法都是直接调用 StandardJanusGraphTx 的 对应方法。但是 build Index 并没有使用到 StandardJanusGraphTx。说明 index 并不是马上就插入数据库？或者因为 Index 建完以后还要等待？？&lt;/p&gt;

&lt;h3 id=&#34;standardjanusgraphtx&#34;&gt;StandardJanusGraphTx&lt;/h3&gt;

&lt;p&gt;上面大致了解了  StandardJanusGraph 和 ManagementSystem ，StandardJanusGraphTx 内部才是最重要的，&lt;/p&gt;

&lt;h4 id=&#34;属性-1&#34;&gt;属性：&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;log
EMPTY_DELETED_RELATIONS
UNINITIALIZED_LOCKS
LOCK_TIMEOUT
MIN_VERTEX_CACHE_SIZE
graph
config
idManager
idInspector
attributeHandler
txHandle
edgeSerializer
indexSerializer
vertexCache
addedRelations
deletedRelations
indexCache
newVertexIndexEntries
uniqueLocks
newTypeCache
temporaryIds
times
isOpen
existingVertexRetriever
externalVertexRetriever
internalVertexRetriever
edgeProcessor
edgeProcessorImpl
elementProcessor
elementProcessorImpl
vertexIDConversionFct
edgeIDConversionFct
propertyIDConversionFct
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;前面的属性都是在 graph 获得的&lt;/p&gt;

&lt;p&gt;vertexCache = new GuavaVertexCache(effectiveVertexCacheSize,concurrencyLevel,config.getDirtyVertexSize()); 是缓存 vertex 的。&lt;/p&gt;

&lt;p&gt;addedRelations = new ConcurrentBufferAddedRelations(); 是缓存 Relation 的。&lt;/p&gt;

&lt;p&gt;deletedRelations 同上&lt;/p&gt;

&lt;p&gt;indexCache 缓存 index ， 类似 vertexCache ，需要传入一个  retrival&lt;/p&gt;

&lt;p&gt;existingVertexRetriever externalVertexRetriever internalVertexRetriever 都是给 vertexCache 用来查 vertex 的。&lt;/p&gt;

&lt;p&gt;edgeProcessor 是一个 QueryExecutor。用来查询的。&lt;/p&gt;

&lt;p&gt;elementProcessor 一样是用来查询的。&lt;/p&gt;

&lt;h4 id=&#34;方法-1&#34;&gt;方法&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;StandardJanusGraphTx
setBackendTransaction
verifyWriteAccess
verifyAccess
verifyOpen
getNextTx
getConfiguration
getGraph
getTxHandle
getEdgeSerializer
getIdInspector
isPartitionedVertex
getCanonicalVertex
getOtherPartitionVertex
getAllRepresentatives
containsVertex
isValidVertexId
getVertex
getVertices
getExistingVertex
getInternalVertex
addVertex
addVertex
addVertex
getInternalVertices
validDataType
verifyAttribute
removeRelation
isRemovedRelation
getLock
getLock
getUniquenessLock
checkPropertyConstraintForVertexOrCreatePropertyConstraint
checkPropertyConstraintForEdgeOrCreatePropertyConstraint
checkConnectionConstraintOrCreateConnectionConstraint
addEdge
connectRelation
addProperty
addProperty
getEdges
makeSchemaVertex
updateSchemaVertex
makePropertyKey
makeEdgeLabel
addSchemaEdge
addProperties
addProperties
addConnection
getSchemaVertex
containsRelationType
getRelationType
containsPropertyKey
containsEdgeLabel
getExistingRelationType
getPropertyKey
getOrCreatePropertyKey
getOrCreatePropertyKey
getEdgeLabel
getOrCreateEdgeLabel
makePropertyKey
makeEdgeLabel
getExistingVertexLabel
containsVertexLabel
getVertexLabel
getOrCreateVertexLabel
makeVertexLabel
query
multiQuery
multiQuery
executeMultiQuery
getConversionFunction
query
indexQuery
commit
rollback
releaseTransaction
isOpen
isClosed
hasModifications
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;schema 操作的 makeEdgeLabel makePropertyKey 等，数据操作的 getVertex addEdge 等，事务操作的 rollback 等。
好像没有 index ？因为 index 属于 schema， 相关的方法都是在 management 中完成的。
实际上，StandardJanusGraphTx 有 addEdge addProperties addVertex 等操作数据的方法，同时还有 makePropertyKey，EdgeLabel 等操作 schema 的方法。
原因是 makePropertyKey 等 schema 实际上也是以顶点的形式保存在 janus 中，所以 schema 操作本质还是数据操作，只不过这部分数据都会被读入内存。
所以 schema 操作都会触发 makeSchemaVertex 的方法，makeSchemaVertex 就是添加一个顶点，只不过是 schema 的订单。&lt;/p&gt;

&lt;h3 id=&#34;backendtransaction&#34;&gt;BackendTransaction&lt;/h3&gt;

&lt;p&gt;我们在看 StandardJanusGraphTx 代码的时候 ，发现 BackendTransaction 也很重要，看看他的继承体系&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;BaseTransaction
	LoggableTransaction (org.janusgraph.diskstorage)
	    CacheTransaction (org.janusgraph.diskstorage.keycolumnvalue.cache)
	    IndexTransaction (org.janusgraph.diskstorage.indexing)
	    BackendTransaction (org.janusgraph.diskstorage)
	BaseTransactionConfigurable (org.janusgraph.diskstorage)
	    Transaction in LuceneIndex (org.janusgraph.diskstorage.lucene)
	    DefaultTransaction (org.janusgraph.diskstorage.util)
	    StoreTransaction (org.janusgraph.diskstorage.keycolumnvalue)
	        AbstractStoreTransaction (org.janusgraph.diskstorage.common)
	            CQLTransaction (org.janusgraph.diskstorage.cql)
	            BerkeleyJETx (org.janusgraph.diskstorage.berkeleyje)
	            CassandraTransaction (org.janusgraph.diskstorage.cassandra)
	            HBaseTransaction (org.janusgraph.diskstorage.hbase)
	            NoOpStoreTransaction (org.janusgraph.diskstorage.common)
	            InMemoryTransaction in InMemoryStoreManager (org.janusgraph.diskstorage.keycolumnvalue.inmemory)
	        CacheTransaction (org.janusgraph.diskstorage.keycolumnvalue.cache)
	        ExpectedValueCheckingTransaction (org.janusgraph.diskstorage.locking.consistentkey)
	IndexTransaction (org.janusgraph.diskstorage.indexing)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;BaseTransaction 只有 comimit 和 roolback 两个方法。LoggableTransaction 只有 LoggableTransaction ，BaseTransactionConfigurable 多了一个 getConfiguration 。&lt;/p&gt;

&lt;p&gt;IndexTransaction BackendTransaction CacheTransaction 继承自 LoggableTransaction ， 前者是处理索引，后者可以处理其他的读写事务，最后的是内存中的事务处理。&lt;/p&gt;

&lt;p&gt;IndexTransaction 中有一个 BaseTransaction 属性，用来实现真正的事务读写，实现一般是 IndexProvider 生成，主要是 ES、LUCENE、Solr 三种实现。
CacheTransaction 中有 StoreTransaction 属性，用来实现持久化。
BackendTransaction 中则有 CacheTransaction edgeStore indexStore txLogStore Map&lt;String, IndexTransaction&gt; indexTx; 等属性，显然这才是最重要的实现事务管控的类。&lt;/p&gt;

&lt;p&gt;我们通过代码分析可以看出 BackendTransaction 的创建是在 StandardJanusGraph 完成，而使用主要是 StandardJanusGraphTx 。
StandardJanusGraph 的 newTransaction 创建 BackendTransaction 和 StandardJanusGraphTx ，并进行赋值。
StandardJanusGraph 什么时候会调用 newTransaction ？一个在 typeCacheRetrieval 中，另一个就是我们代码创建新的 transaction，还有一个是 在没有 transactional isolation 的存储系统上面， commit 的时候需要操作 schema&lt;/p&gt;

&lt;h2 id=&#34;关键类分析&#34;&gt;关键类分析&lt;/h2&gt;

&lt;p&gt;上面整体的调试已经找到了比较关键的大类，以及事务相关的类的关系，我们可以反过来再看一遍调用栈，就更清晰了。现在反过来从细节开始研究具体的类的功能。&lt;/p&gt;

&lt;h3 id=&#34;storemanager&#34;&gt;StoreManager&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;StoreManager
	KeyValueStoreManager (org.janusgraph.diskstorage.keycolumnvalue.keyvalue)
	    OrderedKeyValueStoreManager (org.janusgraph.diskstorage.keycolumnvalue.keyvalue)
	        BerkeleyJEStoreManager (org.janusgraph.diskstorage.berkeleyje)
	AbstractStoreManager (org.janusgraph.diskstorage.common)
	    DistributedStoreManager (org.janusgraph.diskstorage.common)
	        CQLStoreManager (org.janusgraph.diskstorage.cql)
	            CachingCQLStoreManager (org.janusgraph.diskstorage.cql)
	        AbstractCassandraStoreManager (org.janusgraph.diskstorage.cassandra)
	            CassandraThriftStoreManager (org.janusgraph.diskstorage.cassandra.thrift)
	            CassandraEmbeddedStoreManager (org.janusgraph.diskstorage.cassandra.embedded)
	            AstyanaxStoreManager (org.janusgraph.diskstorage.cassandra.astyanax)
	        HBaseStoreManager (org.janusgraph.diskstorage.hbase)
	    LocalStoreManager (org.janusgraph.diskstorage.common)
	        BerkeleyJEStoreManager (org.janusgraph.diskstorage.berkeleyje)
	                    LocalStoreManagerSampleImplementation in LocalStoreManagerTest (org.janusgraph.diskstorage.common)
	            KeyColumnValueStoreManager (org.janusgraph.diskstorage.keycolumnvalue)
	    CQLStoreManager (org.janusgraph.diskstorage.cql)
	        CachingCQLStoreManager (org.janusgraph.diskstorage.cql)
	    OrderedKeyValueStoreManagerAdapter (org.janusgraph.diskstorage.keycolumnvalue.keyvalue)
	    InMemoryStoreManager (org.janusgraph.diskstorage.keycolumnvalue.inmemory)
	    KCVSManagerProxy (org.janusgraph.diskstorage.keycolumnvalue)
	        ExpectedValueCheckingStoreManager (org.janusgraph.diskstorage.locking.consistentkey)
	        TTLKCVSManager (org.janusgraph.diskstorage.keycolumnvalue.ttl)
	    MetricInstrumentedStoreManager (org.janusgraph.diskstorage.util)
	    AbstractCassandraStoreManager (org.janusgraph.diskstorage.cassandra)
	        CassandraThriftStoreManager (org.janusgraph.diskstorage.cassandra.thrift)
	        CassandraEmbeddedStoreManager (org.janusgraph.diskstorage.cassandra.embedded)
	        AstyanaxStoreManager (org.janusgraph.diskstorage.cassandra.astyanax)
	    HBaseStoreManager (org.janusgraph.diskstorage.hbase)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;StoreManager 接口主要功能 beginTransaction 得到一个 StoreTransaction 和 close ，clean 等，还有得到 store 相关的信息。看着上面好像很多继承类，实际上是因为有重复继承导致的。&lt;/p&gt;

&lt;p&gt;KeyValueStoreManager 是测试的。DistributedStoreManager 和 KeyColumnValueStoreManager 是两个抽象，我们使用的 cassandra 和 hbase 的 storeManager 都继承自这两个。
这几个 storeManager 就有我们需要的操作数据的方法。&lt;/p&gt;

&lt;h3 id=&#34;keycolumnvaluestore-keyvaluestore&#34;&gt;KeyColumnValueStore &amp;amp; KeyValueStore&lt;/h3&gt;

&lt;p&gt;KeyValueStore 是测试的，KeyColumnValueStore 是真正的。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;KeyColumnValueStore
	KCVSProxy (org.janusgraph.diskstorage.keycolumnvalue)
	    TTLKCVS (org.janusgraph.diskstorage.keycolumnvalue.ttl)
	    ExpectedValueCheckingStore (org.janusgraph.diskstorage.locking.consistentkey)
	    KCVSCache (org.janusgraph.diskstorage.keycolumnvalue.cache)
	        ExpirationKCVSCache (org.janusgraph.diskstorage.keycolumnvalue.cache)
	        NoKCVSCache (org.janusgraph.diskstorage.keycolumnvalue.cache)
	    ReadOnlyKeyColumnValueStore (org.janusgraph.diskstorage.keycolumnvalue)
	BaseKeyColumnValueAdapter (org.janusgraph.diskstorage.keycolumnvalue.keyvalue)
	    OrderedKeyValueStoreAdapter (org.janusgraph.diskstorage.keycolumnvalue.keyvalue)
	CQLKeyColumnValueStore (org.janusgraph.diskstorage.cql)
	HBaseKeyColumnValueStore (org.janusgraph.diskstorage.hbase)
	CassandraEmbeddedKeyColumnValueStore (org.janusgraph.diskstorage.cassandra.embedded)
	CassandraThriftKeyColumnValueStore (org.janusgraph.diskstorage.cassandra.thrift)
	AstyanaxKeyColumnValueStore (org.janusgraph.diskstorage.cassandra.astyanax)
	MetricInstrumentedStore (org.janusgraph.diskstorage.util)
	CounterKCVS in KCVSCacheTest (org.janusgraph.diskstorage.cache)
	InMemoryKeyColumnValueStore (org.janusgraph.diskstorage.keycolumnvalue.inmemory)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;KeyColumnValueStore 的作用我暂时不是很清楚，从继承类的构造方法看，需要传入一个 StoreManager connection table columnFamily store 。大概能猜出一个 Store 代表一个表格，或者代表一个列族，应该是代表某种数据，例如索引，日志等。&lt;/p&gt;

&lt;p&gt;从他的方法可以看出主要是查询库， 如 getKeySlice mutate mutateMany 。&lt;/p&gt;

&lt;p&gt;KCVSCache 也继承自 KeyColumnValueStore，名字可以看出是放在内存的 store ，自然也有 getSlice 等方法，我们可以看他的实现类 ExpirationKCVSCache。
这个类里面有一个 Cache&lt;KeySliceQuery,EntryList&gt; cache 的对象，用来缓存查询结果。而 KCVSCache 继承自 KCVSProxy ，这个类则代理 KeyColumnValueStore 对象。其实还有一个 TTLKCVS ，应该是带过期时间的 store&lt;/p&gt;

&lt;h3 id=&#34;logmanager&#34;&gt;LogManager&lt;/h3&gt;

&lt;p&gt;LogManager 的注释：Manager interface for opening {@link Log}s against a particular Log implementation.&lt;/p&gt;

&lt;p&gt;KCVSLogManager 实现类的注释：
Implementation of {@link LogManager} against an arbitrary {@link KeyColumnValueStoreManager}.
Issues {@link Log} instances which wrap around a {@link KeyColumnValueStore}.&lt;/p&gt;

&lt;p&gt;可以看出 LogManager 主要是将 通过 KeyColumnValueStoreManager 实现 Log，而 log 则是 围绕 KeyColumnValueStore 。&lt;/p&gt;

&lt;p&gt;而我们的log包括三部分： managementLogManager txLogManager userLogManager&lt;/p&gt;

&lt;h3 id=&#34;log&#34;&gt;Log&lt;/h3&gt;

&lt;p&gt;Log 的注释：
Represents a log that allows content to be added to it in the form of messages and
to read messages and their content from the log via registered {@link MessageReader}s.&lt;/p&gt;

&lt;p&gt;KCVSLog 的注释很长。可以看出主要通过 KeyColumnValueStore 实现。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/**
 * Implementation of {@link Log} wrapped around a {@link KeyColumnValueStore}. Each message is written as a column-value pair ({@link Entry})
 * into a timeslice slot. A timeslice slot is uniquely identified by:
 * &amp;lt;ul&amp;gt;
 *     &amp;lt;li&amp;gt;The partition id: On storage backends that are key-ordered, a partition bit width can be configured which configures the number of
 *     first bits that comprise the partition id. On unordered storage backends, this is always 0&amp;lt;/li&amp;gt;
 *     &amp;lt;li&amp;gt;A bucket id: The number of parallel buckets that should be maintained is configured by
 *     {@link org.janusgraph.graphdb.configuration.GraphDatabaseConfiguration#LOG_NUM_BUCKETS}. Messages are written to the buckets
 *     in round-robin fashion and each bucket is identified by a bucket id.
 *     Having multiple buckets per timeslice allows for load balancing across multiple keys in the storage backend.&amp;lt;/li&amp;gt;
 *     &amp;lt;li&amp;gt;The start time of the timeslice: Each time slice is {@link #TIMESLICE_INTERVAL} microseconds long. And all messages that are added between
 *     start-time and start-time+{@link #TIMESLICE_INTERVAL} end up in the same timeslice. For high throughput logs that might be more messages
 *     than the underlying storage backend can handle per key. In that case, ensure that (2^(partition-bit-width) x (num-buckets) is large enough
 *     to distribute the load.&amp;lt;/li&amp;gt;
 * &amp;lt;/ul&amp;gt;
 *
 * Each message is uniquely identified by its timestamp, sender id (which uniquely identifies a particular instance of {@link KCVSLogManager}), and the
 * message id (which is auto-incrementing). These three data points comprise the column of a log message. The actual content of the message
 * is written into the value.
 * &amp;lt;/p&amp;gt;
 * When {@link MessageReader} are registered, one reader thread per partition id and bucket is created which periodically (as configured) checks for
 * new messages in the storage backend and invokes the reader. &amp;lt;/br&amp;gt;
 * Read-markers are maintained (for each partition-id &amp;amp; bucket id combination) under a dedicated key in the same {@link KeyColumnValueStoreManager} as the
 * log messages. The read markers are updated to the current position before each new iteration of reading messages from the log. If the system fails
 * while reading a batch of messages, a subsequently restarted log reader may therefore read messages twice. Hence, {@link MessageReader} implementations
 * should exhibit correct behavior for the (rare) circumstance that messages are read twice.
 *
 * Note: All time values in this class are in microseconds. Hence, there are many cases where milliseconds are converted to microseconds.
 *
 * @author Matthias Broecheler (me@matthiasb.com)
 */
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;英语不好就为难了。
每个消息都由它的时间戳、发件人ID，以及消息ID（它是自动递增的）唯一标识。这三个数据组成包括日志消息的列名。消息的实际内容被写入值中。&lt;/p&gt;

&lt;h3 id=&#34;indexprovider&#34;&gt;IndexProvider&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;IndexProvider (org.janusgraph.diskstorage.indexing)
    LuceneIndex (org.janusgraph.diskstorage.lucene)
    TestMockIndexProvider (org.janusgraph.graphdb)
    SolrIndex (org.janusgraph.diskstorage.solr)
    ElasticSearchIndex (org.janusgraph.diskstorage.es)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面的 IndexTransaction 包含了 对 IndexProvider的操作。&lt;/p&gt;

&lt;h3 id=&#34;vertexidassigner-standardidpool-idblock&#34;&gt;VertexIDAssigner StandardIDPool IDBlock&lt;/h3&gt;

&lt;p&gt;负责分配 id ，分配原则我们通过运行 VertexIDAssignerTest 查看。&lt;/p&gt;

&lt;h3 id=&#34;element&#34;&gt;Element&lt;/h3&gt;

&lt;p&gt;我们在操作的过程中有很多的 Vertex Property Edge 等，实际上都继承自一个 Element，继承体系确实有点吓人，这里就不展示了，几个 schema 都这么多东西，我们先分类。&lt;/p&gt;

&lt;p&gt;首先我们思考一下，为什么会有这么多。其实 gremin 语法本身定义了一堆schema ，而 janus 也有自己的schema ，两个要进行适配器模式，所以还有一组适配器的schema。所以会比较多？&lt;/p&gt;

&lt;p&gt;我们先看一下 gremin 的接口 ,主要有三个，&lt;code&gt;org.apache.tinkerpop.gremlin.structure&lt;/code&gt;下面：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;VertexProperty
Vertex
Edge
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;分别代表了属性，顶点，边，然后 gremin 本身对他们进行了一些实现。然后死 janusgraph 的 &lt;code&gt;org.janusgraph.core&lt;/code&gt; 包下面，有很多一些接口：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;JanusGraphElement
    JanusGraphVertex
        InternalVertex (org.janusgraph.graphdb.internal)
        RelationType (org.janusgraph.core)
        VertexLabel (org.janusgraph.core)
	JanusGraphRelation
		JanusGraphEdge (org.janusgraph.core)
		    AbstractEdge (org.janusgraph.graphdb.relations)
		        CacheEdge (org.janusgraph.graphdb.relations)
		        StandardEdge (org.janusgraph.graphdb.relations)
		JanusGraphVertexProperty (org.janusgraph.core)
		    FulgoraVertexProperty (org.janusgraph.graphdb.olap.computer)
		    AbstractVertexProperty (org.janusgraph.graphdb.relations)
		        StandardVertexProperty (org.janusgraph.graphdb.relations)
		        CacheVertexProperty (org.janusgraph.graphdb.relations)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里展示的并不完整。整个 janus 的schema很复杂。只是大概从注释看出，
在 core 包中，JanusGraphVertex 是顶点，JanusGraphRelation 代表顶点关系，分为属性和边两种 ：JanusGraphVertexProperty 和 JanusGraphEdge。
在 internal 包中，对 core 包的类添加些 janus 特有的方法。&lt;/p&gt;

&lt;p&gt;另外在 schema 包中还有 RelationType 和 VertexLabel ，两个都是继承自 JanusGraphVertex ，意思是说 VertexLabel VertexProperty EdgeLabel 都是顶点？？？。
这样就好像明白一点，janus 中的 PropertyKey VertexLabel EdgeLabel 都是以顶点的形式保存起来的。&lt;/p&gt;

&lt;p&gt;所以我们看 Edge 类型继承体系比较简单，就是 CacheEdge (org.janusgraph.graphdb.relations) StandardEdge (org.janusgraph.graphdb.relations) 继承自
AbstractEdge ，然后继承 JanusGraphEdge，Edge。
而 Vertex 继承体系很复杂，除了类似 Edge 的继承体系以外，CacheVertex 还多了 JanusGraphSchemaVertex 这个子类，这个子类还有 RelationTypeVertex 和 VertexLabelVertex 两个子类，
实际上很明显，CacheVertex 的子类 JanusGraphSchemaVertex 代表的就是 graph 的 schema ，也是作为 Vertex 保存的。&lt;/p&gt;

&lt;p&gt;这个给别人讲一句话就懂了，但是自己分析可能要好几个小时才能明白。这也是学习和自己研究的不同。&lt;/p&gt;

&lt;h3 id=&#34;index&#34;&gt;Index&lt;/h3&gt;

&lt;p&gt;索引肯定是数据库的重点，我们到目前没有分析过和所以有关的内容。IndexTransaction 是我们遇到的可能和索引相关的内容了，就从 他开始。
IndexTransaction 中有个 BaseTransaction 的对象用来实现事务，通过 IndexProvider 来产生。我们以 ElasticSearchIndex 为例，可以看看他的方法。&lt;/p&gt;

&lt;p&gt;例如 register 方法会创建索引，还有 restore 等操作事务的方法。在 ManagementSystem 的 updateIndex 方法中，定义了各种操作 index 的方法。&lt;/p&gt;

&lt;p&gt;Index 类继承了 JanusGraphSchemaElement，主要有两类实现类 JanusGraphIndex 和 RelationTypeIndex 。&lt;/p&gt;

&lt;p&gt;JanusGraphIndex 的实现类是 JanusGraphIndexWrapper 。可以通过 JanusGraphManagement#buildIndex(String, Class) 新建 。&lt;/p&gt;

&lt;p&gt;RelationTypeIndex 的实现类是 RelationTypeIndexWrapper，可以通过
JanusGraphManagement#buildEdgeIndex(org.janusgraph.core.EdgeLabel, String, org.apache.tinkerpop.gremlin.structure.Direction, org.apache.tinkerpop.gremlin.process.traversal.Order, org.janusgraph.core.PropertyKey&amp;hellip;)
和 JanusGraphManagement#buildPropertyIndex(org.janusgraph.core.PropertyKey, String, org.apache.tinkerpop.gremlin.process.traversal.Order, org.janusgraph.core.PropertyKey&amp;hellip;)
两个方法建 RelationTypeIndex。&lt;/p&gt;

&lt;p&gt;IndexType 定义所有的 JanusGraphIndex，实现包括 CompositeIndexType 和 MixedIndexType。&lt;/p&gt;

&lt;p&gt;IndexType IndexProvider 和 Index 的不同在于，Index 和他的实现类 JanusGraphIndex RelationTypeIndexWrapper 都是继承自 JanusGraphSchemaElement ，和 Vertex 一样，代表的是 janus 中的一个顶点。
IndexType 代表了所以类型 ，IndexProvider 则代表的是和索引相关的操作方法 例如 ElasticSearchIndex SolrIndex LuceneIndex。&lt;/p&gt;

&lt;h3 id=&#34;standardscanner&#34;&gt;StandardScanner&lt;/h3&gt;

&lt;p&gt;在 Backend 构造方法最后有一句 new StandardScanner。我们看看这个是干啥用的，主要调用地方是  buildStoreIndexScanJob 这个方法，我们发现这个新建了一个 Job。
buildEdgeScanJob 主要就是在 ManagementSystem 的 updateIndex 方法使用，根据方法名可以看出，这是在遍历数据库的job。&lt;/p&gt;

&lt;p&gt;StandardScanner 的重点很明显就是它的内部类 Builder。Builder 内部有一个 ScanJob 的变量，实际上 Builder 就是有个 execute 方法，能够执行 ScanJob ，例如 IndexUpdateJob 和 IndexRepairJob。&lt;/p&gt;

&lt;p&gt;这个越看越复杂，还是后续分析吧。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>janusgraph线上schema过程Debug</title>
      <link>https://dengziming.github.io/post/titan/janusgraph%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%904-schema/</link>
      <pubDate>Thu, 22 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/titan/janusgraph%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%904-schema/</guid>
      
        <description>

&lt;h1 id=&#34;初步调试&#34;&gt;初步调试&lt;/h1&gt;

&lt;h2 id=&#34;回顾&#34;&gt;回顾&lt;/h2&gt;

&lt;p&gt;首先我们通过 debug 官方的 GraphOfGod 大概进行一个简单的调试，然后我们仔细查看 janusgraph 调用栈，分析了关键类。
这次我们主要看看schema 的建立过程，我们上次分析已经知道，其实 schema也是以Vertex的方式存储在内存和数据库中的。
通过 CacheVertex 的子类 JanusGraphSchemaVertex 实现。JanusGraphSchemaVertex 有两个个子类，&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;
AbstractElement (org.janusgraph.graphdb.internal)
	AbstractVertex (org.janusgraph.graphdb.vertices)
		StandardVertex (org.janusgraph.graphdb.vertices)
			CacheVertex (org.janusgraph.graphdb.vertices)
				JanusGraphSchemaVertex (org.janusgraph.graphdb.types.vertices)
					RelationTypeVertex (org.janusgraph.graphdb.types.vertices)
					    PropertyKeyVertex (org.janusgraph.graphdb.types.vertices)
					    EdgeLabelVertex (org.janusgraph.graphdb.types.vertices)
					VertexLabelVertex (org.janusgraph.graphdb.types)
````

中间省略了一些接口。

所以每次 new 一个 EdgeLabelVertex、VertexLabelVertex、PropertyKeyVertex 的时候，调用栈会非常深。

我们大概查看一下这些类的功能。

### AbstractElement

只有一个属性：private long id; 这个id是唯一的。小于0 的是临时id，事务提交时候会分配大于0的id，等于0的是虚拟的并不存在的，大雨0的是物理persist的。

### InternalVertex

图上没有展示 InternalVertex ，这是一个接口，继承自 JanusGraphVertex 和 InternalElement。凡是带 Internal 的都是比原来的多一个 janus 专属方法的类。
所以 InternalVertex 也是比 JanusGraphVertex 多一些 janus 专属的方法 例如： removeRelation addRelation tx()。
JanusGraphVertex 中则是 janus 和 gremin 都会有的方法 ， 例如 addEdge property label。
InternalVertex 有 query() 等方法，

### AbstractVertex

AbstractVertex 继承自 AbstractElement 和 InternalVertex，
AbstractVertex 比 AbstractElement 的 id 基础上多了一个 StandardJanusGraphTx tx 的属性。也就是多了一个事务空值对象。

### StandardVertex

StandardVertex 继承自 AbstractVertex，多了一个 lifecycle 属性和 volatile AddedRelationsContainer addedRelations 属性。应该是通过缓存空值。

### CacheVertex
CacheVertex 继承自 StandardVertex 。多了一个 queryCache 属性。

### JanusGraphSchemaVertex 

JanusGraphSchemaVertex 就是保存 Schema 的 Vertex ，分为两类 RelationTypeVertex 和 VertexLabelVertex。其中 RelationTypeVertex 分为 PropertyKeyVertex 和 EdgeLabelVertex。

## 预览

schema 操作是通过 ManagementSystem &amp;lt;: JanusGraphManagement 完成的。ManagementSystem 内容很复杂，上次已经大概看了他的方法和属性，这次我们着重看一下方法的实现，首先还是再次浏览一下方法。

### 属性

```java
private static final String CURRENT_INSTANCE_SUFFIX = &amp;quot;(current)&amp;quot;;

private final StandardJanusGraph graph;
private final Log sysLog;
private final ManagementLogger mgmtLogger;

private final KCVSConfiguration baseConfig;
private final TransactionalConfiguration transactionalConfig;
private final ModifiableConfiguration modifyConfig;
private final UserModifiableConfiguration userConfig;
private final SchemaCache schemaCache;

private final StandardJanusGraphTx transaction;

private final Set&amp;lt;JanusGraphSchemaVertex&amp;gt; updatedTypes;
private final List&amp;lt;Callable&amp;lt;Boolean&amp;gt;&amp;gt; updatedTypeTriggers;

private final Instant txStartTime;
private boolean graphShutdownRequired;
private boolean isOpen;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;构造方法&#34;&gt;构造方法&lt;/h3&gt;

&lt;p&gt;基本都是直接赋值。StandardJanusGraph 的 openManagement 方法返回一个 ManagementSystem 。&lt;/p&gt;

&lt;h3 id=&#34;instances-操作&#34;&gt;Instances 操作&lt;/h3&gt;

&lt;p&gt;getOpenInstancesInternal
getOpenInstances
forceCloseInstance&lt;/p&gt;

&lt;p&gt;判断正在运行的 instance 。&lt;/p&gt;

&lt;h3 id=&#34;commit-和-rollback&#34;&gt;commit 和 rollback&lt;/h3&gt;

&lt;p&gt;commit 方法有四步。
1. 判断 transactionalConfig 是否变化，如果变化，将变化写出。
2. transactionalConfig.commit();
3. transaction.commit();
4. 判断 updatedTypes 是否有更新，进行 expire 操作。&lt;/p&gt;

&lt;p&gt;rollback 方法，则很简单。直接调用两个 transaction 的 callback ，然后 close。&lt;/p&gt;

&lt;h3 id=&#34;getschemaelement&#34;&gt;getSchemaElement&lt;/h3&gt;

&lt;p&gt;这个方法返回一个 JanusGraphSchemaElement ，但是实际上返回的是 RelationTypeIndexWrapper 或者 JanusGraphIndexWrapper ，原因未知，这两个类上一节介绍过，。&lt;/p&gt;

&lt;h3 id=&#34;buildrelationtypeindex&#34;&gt;buildRelationTypeIndex&lt;/h3&gt;

&lt;p&gt;包括 buildPropertyIndex 和 buildEdgeIndex。
步骤都是先 生成对应的 RelationTypeMaker，然后 make，然后调用 addSchemaEdge， 最后调用 updateIndex&lt;/p&gt;

&lt;h3 id=&#34;getrelationindex&#34;&gt;getRelationIndex&lt;/h3&gt;

&lt;p&gt;得到 RelationType 的 Index。
调用 QueryUtil.getVertices(transaction, BaseKey.SchemaName, JanusGraphSchemaCategory.getRelationTypeName(composedName))
然后 return new RelationTypeIndexWrapper((InternalRelationType) v);&lt;/p&gt;

&lt;h3 id=&#34;getrelationindexes&#34;&gt;getRelationIndexes&lt;/h3&gt;

&lt;p&gt;得到所有的  Indexs。&lt;/p&gt;

&lt;h3 id=&#34;getgraphindexdirect&#34;&gt;getGraphIndexDirect&lt;/h3&gt;

&lt;p&gt;直接调用 transaction.getSchemaVertex(JanusGraphSchemaCategory.GRAPHINDEX.getSchemaName(name));
得到 GraphIndex ，GraphIndex 和 RelationTypeIndex 不一样，一个是基于关系的，前者是基于属性的。&lt;/p&gt;

&lt;h3 id=&#34;getgraphindexes&#34;&gt;getGraphIndexes&lt;/h3&gt;

&lt;p&gt;返回所有的 GraphIndex&lt;/p&gt;

&lt;h3 id=&#34;createmixedindex&#34;&gt;createMixedIndex&lt;/h3&gt;

&lt;p&gt;调用 JanusGraphSchemaVertex indexVertex = transaction.makeSchemaVertex(JanusGraphSchemaCategory.GRAPHINDEX, indexName, def); 得到 vertex
调用 addSchemaEdge(indexVertex, (JanusGraphSchemaVertex) constraint, TypeDefinitionCategory.INDEX_SCHEMA_CONSTRAINT, null); 添加关系
然后调用 updateSchemaVertex(indexVertex);
最终 new JanusGraphIndexWrapper(indexVertex.asIndexType());&lt;/p&gt;

&lt;p&gt;可以看出，这个方法其实只是添加了一个顶点，然后和另一个 constraint 顶点简历了一条关系。&lt;/p&gt;

&lt;h3 id=&#34;addindexkey&#34;&gt;addIndexKey&lt;/h3&gt;

&lt;p&gt;给已有的 index 添加一个key，这个应该很复杂，我们先跳过。&lt;/p&gt;

&lt;h3 id=&#34;createcompositeindex&#34;&gt;createCompositeIndex&lt;/h3&gt;

&lt;p&gt;创建 CompositeIndex ，GrpahIndex 分为 CompositeIndex 和 mixedIndex&lt;/p&gt;

&lt;p&gt;创建过程也是 addSchemaEdge ， updateSchemaVertex，updateIndex&lt;/p&gt;

&lt;h3 id=&#34;innerclass&#34;&gt;InnerClass&lt;/h3&gt;

&lt;p&gt;很多内部类：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;IndexBuilder   -- 构建 Index
EmptyIndexJobFuture -- 提交的job
UpdateStatusTrigger -- 更新status的触发器
IndexJobStatus -- job 的 status
IndexIdentifier  --标识
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;addschemaedge&#34;&gt;addSchemaEdge&lt;/h3&gt;

&lt;p&gt;上面好几个方法都会调用 addSchemaEdge updateSchemaVertex updateIndex ，我们看看这三个方法。&lt;/p&gt;

&lt;p&gt;addSchemaEdge 是私有方法，应该是在内部会调用的。根据名字可以得出这个方法是添加边，而且添加的是 schema 的边，
我们之前已经知道实际上 schema 都是保存为 vertex，而现在就是给这些 Vertex 添加 Edge，这个边的 EdgeLabel 是 BaseLabel.SchemaDefinitionEdge。
例如 某个 PropertyKey 添加一个 Index ，实际上会有两个 SchemaVertex，然后给他们建立一个关系。
我们可以通过查看方法调用时机，基本上是修改 index 或者 schemaVertex ，一般与 updateSchemaVertex 或者 updateIndex 配合执行。&lt;/p&gt;

&lt;p&gt;方法大概步骤就是调用 transaction.addEdge(out, in, BaseLabel.SchemaDefinitionEdge) 得到 Edge，
然后调用 edge.property(BaseKey.SchemaDefinitionDesc.name(), desc);最后返回 edge。&lt;/p&gt;

&lt;h3 id=&#34;updateschemavertex&#34;&gt;updateSchemaVertex&lt;/h3&gt;

&lt;p&gt;就一句话 transaction.updateSchemaVertex(schemaVertex);&lt;/p&gt;

&lt;h3 id=&#34;updateindex&#34;&gt;updateIndex&lt;/h3&gt;

&lt;p&gt;IndexJobFuture updateIndex(Index index, SchemaAction updateAction)&lt;/p&gt;

&lt;p&gt;SchemaAction 是一个枚举，包括 REGISTER_INDEX REINDEX ENABLE_INDEX DISABLE_INDEX REMOVE_INDEX 。
IndexJobFuture 代表的是提交了的 job，等待返回结果。&lt;/p&gt;

&lt;p&gt;方法步骤：&lt;/p&gt;

&lt;p&gt;JanusGraphSchemaVertex schemaVertex = getSchemaVertex(index);&lt;/p&gt;

&lt;p&gt;更新 dependentTypes ，实际上就是为了更新 updatedTypes。&lt;/p&gt;

&lt;p&gt;根据不同的请求，调用 setStatus setUpdateTrigger setJob 。这个过程很复杂，后面再讲解。&lt;/p&gt;

&lt;h2 id=&#34;编码调试&#34;&gt;编码调试&lt;/h2&gt;

&lt;h3 id=&#34;managementsystem-构造方法&#34;&gt;ManagementSystem 构造方法&lt;/h3&gt;

&lt;p&gt;调试整个过程总是很麻烦的，我们只能专注某些部分，首先我们主要看一下 ManagementSystem 的构造过程，和使用细节。&lt;/p&gt;

&lt;p&gt;打断点进入构造方法,看这一句代码：&lt;code&gt;this.transaction = (StandardJanusGraphTx) graph.buildTransaction().disableBatchLoading().start();&lt;/code&gt; 一步一步进入调用栈&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;this.transaction = (StandardJanusGraphTx) graph.buildTransaction().disableBatchLoading().start();
    return graph.newTransaction(immutable);
        tx.setBackendTransaction(openBackendTransaction(tx));
            return backend.beginTransaction(tx.getConfiguration(), retriever);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 Backend 的 beginTransaction 方法停下来，首先看看 &lt;code&gt;StoreTransaction tx = storeManagerLocking.beginTransaction(configuration)&lt;/code&gt; 的调用栈：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 这个 ExpectedValueCheckingStoreManager 继承自 KCVSManagerProxy ，它内部有个 KeyColumnValueStoreManager manager 。显然是代理模式，当然也可以认为是装饰模式。
StoreTransaction tx = storeManagerLocking.beginTransaction(configuration);
    StoreTransaction inconsistentTx = manager.beginTransaction(configuration);
        return new CassandraTransaction(config);
    StoreTransaction strongConsistentTx = manager.beginTransaction(consistentTxCfg);
        return new CassandraTransaction(config);
    ExpectedValueCheckingTransaction wrappedTx = new ExpectedValueCheckingTransaction(inconsistentTx, strongConsistentTx, maxReadTime);
    return wrappedTx;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看出 tx 的大概构造，里面有两个 CassandraTransaction 一个是强一致的，一个是非强一致的。&lt;/p&gt;

&lt;p&gt;然后是 &lt;code&gt;CacheTransaction cacheTx = new CacheTransaction(tx, storeManagerLocking, bufferSize, maxWriteTime, configuration.hasEnabledBatchLoading());&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;// CacheTransaction 继承自 StoreTransaction 和 LoggableTransaction ，内部有一个 StoreTransaction 对象，显然也是代理模式或者装饰模式。
CacheTransaction cacheTx = new CacheTransaction(tx, storeManagerLocking, bufferSize, maxWriteTime, configuration.hasEnabledBatchLoading());
    就是一堆赋值。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;CacheTransaction 内部有一个 StoreTransaction 也就是 上面的 tx， 然后还有一个 StoreManager storeManagerLocking。&lt;/p&gt;

&lt;p&gt;然后是 &lt;code&gt;indexTx.put(entry.getKey(), new IndexTransaction(entry.getValue() indexKeyRetriever.get(entry.getKey()), configuration, maxWriteTime));&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;indexTx.put(entry.getKey(), new IndexTransaction(entry.getValue(), indexKeyRetriever.get(entry.getKey()), configuration, maxWriteTime));
    new KeyInformation.IndexRetriever() {...省略代码}
    new IndexTransaction()
        index.beginTransaction(config);
            return new DefaultTransaction(config);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;backend.beginTransaction(tx.getConfiguration(), retriever); 方法中 有很多 Transaction 对象，包括了 cacheTx indexTx 等。BackendTransaction 的构造方法则比较简单，就是直接赋值。&lt;/p&gt;

&lt;p&gt;构造方法讨论到这里，我们可以猜测，ManagementSystem 无论是进行简单 schema 增删改查还是操作索引，
背后都是通过这个 BackendTransaction 完成，而 BackendTransaction 内部又有 cacheTx 和 indexTx 等对象完成。
当然还有一个 transactionalConfig 也有一些任务。&lt;/p&gt;

&lt;h3 id=&#34;managementsystem-getvertexlabels&#34;&gt;ManagementSystem getVertexLabels&lt;/h3&gt;

&lt;p&gt;getVertexLabels 方法返回 Iterable&lt;VertexLabel&gt; ，这里可能是通过 guava 进行封装，所以可能调用栈比较深。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;getVertexLabels
    QueryUtil.getVertices(transaction, BaseKey.SchemaCategory, JanusGraphSchemaCategory.VERTEXLABEL)
        tx.query().has(key,Cmp.EQUAL,equalityCondition).vertices();
            1. 
            return new GraphCentricQueryBuilder(this, graph.getIndexSerializer());
            3. 
            return has(key.name(),predicate,condition);
                // 这一步实际上就加了一个 条件，就是 `~T$SchemaCategory = VERTEXLABEL`
                constraints.add(new PredicateCondition&amp;lt;String, JanusGraphElement&amp;gt;(key, predicate, condition));
            3. 
            GraphCentricQuery query = constructQuery(ElementCategory.VERTEX);
                 GraphCentricQuery query = constructQueryWithoutProfile(resultType);
                     省略一大堆复杂代码。
                     return new GraphCentricQuery(resultType, conditions, orders, query, limit);
            // 这里是基于guava实现的懒加载模式的 filter
            Iterables.filter(new QueryProcessor&amp;lt;GraphCentricQuery, JanusGraphElement, JointIndexQuery&amp;gt;(query, tx.elementProcessor), JanusGraphVertex.class);

iterator
    return new ResultSetIterator(getUnfoldedIterator(),(query.hasLimit()) ? query.getLimit() : Query.NO_LIMIT);   
        1. QueryProcessor (org.janusgraph.graphdb.query).getUnfoldedIterator:107, 
            Iterator&amp;lt;R&amp;gt; subiter = new LimitAdjustingIterator(subq);
        2. this.next = nextInternal();
            hasNext:68, LimitAdjustingIterator (org.janusgraph.graphdb.query)
                getNewIterator:209, QueryProcessor$LimitAdjustingIterator (org.janusgraph.graphdb.query)
                    execute:1150, StandardJanusGraphTx$elementProcessorImpl (org.janusgraph.graphdb.transaction)
                        new SubqueryIterator
                            indexCache.getIfPresent(subQuery); // 这里的 schema 应该都是在启动的时候 cache 到了内存中，所以直接得到了，如果是 数据，应该要查询
                        
                    
                  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里我们就已经知道了，其实这里是构造了一个 GraphCentricQuery 封装所有的查询条件逻辑，然后通过 QueryProcessor 进行处理这个 query，调用 next 的时候会进行查询。&lt;/p&gt;

&lt;p&gt;上面我们已经得到了 ResultSetIterator ，接下来我们需要遍历这个 iterator。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;iterator.hasNext
    1. next = current
    2. tryToComputeNext()
    ...
        1. hasNext:49, ResultSetIterator (org.janusgraph.graphdb.query)
            return next != null;
            
        2. ResultSetIterator (org.janusgraph.graphdb.query).next:65, 
            1. LimitAdjustingIterator (org.janusgraph.graphdb.query).hasNext:68, 
                SubqueryIterator (org.janusgraph.graphdb.util).hasNext:79, 
            2. LimitAdjustingIterator (org.janusgraph.graphdb.query).next:94, 
                SubqueryIterator (org.janusgraph.graphdb.util).next:90, 

iterator.next()  
   return result.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里的代码比较杂乱，首先是 AbstractIterator 和 Iterators 类，然后是 ResultSetIterator LimitAdjustingIterator SubqueryIterator ，然后还有一个 Stream 类。&lt;/p&gt;

&lt;p&gt;AbstractIterator 和 Iterators  是 guava 提供的工具类，AbstractIterator 通过封装一个 Iterator，达到缓存和懒加载的效果。
例如 JDBC 的 ResultSet 如果做成一个 Iterator ，每次调用 next 的时候都会移动一次游标，这样就不能多次判断 hasNext。所以可以用 guava 进行封装。&lt;/p&gt;

&lt;p&gt;ResultSetIterator 和 guava 达到的效果类似，通过内部装饰一个 ResultSetIterator 。&lt;/p&gt;

&lt;p&gt;LimitAdjustingIterator 通过一个 getNewIterator 得到一个 懒加载 Iterator，其实也是和 guava 类似，只不过你可以认为它只能查看 limit 个元素，当遍历完这 limit 个元素，会重新从 0 开始 next limit次，然后再开始。
说的简单一点，如果一个数组有一千个元素，你的迭代器 limit 是 500，那么你只能得到 500 个元素，想要得到500 - 1000 的元素，要重新查询。类似 mysql 的分页&lt;/p&gt;

&lt;p&gt;SubqueryIterator 是代表依次查询的结果。先从 indexCache 查，没有就调用查询，查询结果是一个 List ，得到对应的 iterator 后放在 elementIterator 中。&lt;/p&gt;

&lt;p&gt;到这里我们就大概明白了整个查询过程，&lt;/p&gt;

&lt;h3 id=&#34;containsvertexlabel&#34;&gt;containsVertexLabel&lt;/h3&gt;

&lt;p&gt;containsVertexLabel 方法判断是否存在，直观的方法是直接调用上面的 getVertexLabels 然后判断一下，但是实际上不是这样。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mgmt.containsVertexLabel(vType.toString())
    transaction.containsVertexLabel(name);
        return getSchemaVertex(JanusGraphSchemaCategory.VERTEXLABEL.getSchemaName(name))!=null;
        1. JanusGraphSchemaCategory.VERTEXLABEL.getSchemaName(name) // 这一步就是在 name 前面加上标识，例如 vl rt
        2. JanusGraphSchemaVertex getSchemaVertex(String schemaName)
            graph.getSchemaCache().getSchemaId(schemaName)
            1. getSchemaCache 
            2. StandardSchemaCache.getSchemaId
                id = retriever.retrieveSchemaByName(schemaName); // 这个 retriever 是 StandardJanusGraph 中的变量 typeCacheRetrieval ，
                    typeCacheRetrieval.retrieveSchemaByName
                        StandardJanusGraph.this.newTransaction
                            QueryUtil.getVertices(consistentTx, BaseKey.SchemaName, typeName)
                            return v!=null?v.longId():null;
        
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;containsVertexLabel 会启动一个 transation 通过 name 查询这个 schema 的 typeName 对应的 vertexId。和 getVertexLabels 不太一样。&lt;/p&gt;

&lt;h3 id=&#34;makevertexlabel&#34;&gt;makeVertexLabel&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;mgmt.makeVertexLabel(vType.toString()).make();
    1. makeVertexLabel
        transaction.makeVertexLabel(name);
            StandardVertexLabelMaker maker = new StandardVertexLabelMaker(this);
            maker.name(name);
    2. make
        TypeDefinitionMap def = new TypeDefinitionMap();
        tx.makeSchemaVertex(JanusGraphSchemaCategory.VERTEXLABEL,name,def);
            1. schemaVertex = new VertexLabelVertex(this, IDManager.getTemporaryVertexID(IDManager.VertexIDType.GenericSchemaType,temporaryIds.nextID()), ElementLifeCycle.New);
            2. graph.assignID(schemaVertex, BaseVertexLabel.DEFAULT_VERTEXLABEL);
                ....
                element.setId(elementId);
            3. addProperty(schemaVertex, BaseKey.SchemaName, schemaCategory.getSchemaName(name));
                1. StandardVertexProperty prop = new StandardVertexProperty(IDManager.getTemporaryRelationID(temporaryIds.nextID()), key, (InternalVertex) vertex, normalizedValue, ElementLifeCycle.New);
                2. connectRelation(prop);
                    addedRelations.add(r); 
            4. vertexCache.add(schemaVertex, schemaVertex.longId());
            
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;makeVertexLabel 最关键的一步就是 addedRelations.add&amp;reg;, 添加关系，这样就能在 commit 的时候写到数据库了。&lt;/p&gt;

&lt;h3 id=&#34;commit&#34;&gt;commit()&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;commit
    transactionalConfig.commit();
    transaction.commit();
         1. graph.commit(addedRelations.getAll(), deletedRelations.values(), this); // 这里的两个集合分别是改变的 schema 
             
             
             1. final BackendTransaction schemaMutator = openBackendTransaction(tx); // 打开一个 transaction
             2. commitSummary = prepareCommit(addedRelations,deletedRelations, SCHEMA_FILTER, schemaMutator, tx, acquireLocks);
             3. schemaMutator.commit();
             
             4. commitSummary = prepareCommit(addedRelations,deletedRelations, hasTxIsolation? NO_FILTER : NO_SCHEMA_FILTER, mutator, tx, acquireLocks);
             5. mutator.commit();
                 1. storeTx.commit();
                     1. flushInternal();
                     2. tx.commit();
                 2. itx.commit();
                     1. flushInternal();
                     2. indexTx.commit();
          2. releaseTransaction();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其实我这里的注释和官方的不太一样，官方将 graph.commit 分为三部分：&lt;/p&gt;

&lt;p&gt;//1. Finalize transaction
//2. Assign JanusGraphVertex IDs
//3. Commit
//3.1 Log transaction (write-ahead log) if enabled
//3.2 Commit schema elements and their associated relations in a separate transaction if backend does not support transactional isolation
//[FAILURE] Exceptions during preparation here cause the entire transaction to fail on transactional systems
//or just the non-system part on others. Nothing has been persisted unless batch-loading&lt;/p&gt;

&lt;p&gt;经过我的分析，其实这里分两次 prepareCommit + commit ,是根据底层是否支持事务隔离，如果不支持，先 commit 和 schema 相关的变化，否则 schema 和 data 两边一起提交。&lt;/p&gt;

&lt;p&gt;当然这个 wal-log 和 prepareCommit 就大有文章。后续在分析。&lt;/p&gt;

&lt;h3 id=&#34;createcompositeindex-1&#34;&gt;createCompositeIndex&lt;/h3&gt;

&lt;p&gt;建索引，索引类型是 createCompositeIndex&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;buildCompositeIndex:650, ManagementSystem$IndexBuilder (org.janusgraph.graphdb.database.management)
    1.checkIndexName:489, ManagementSystem (org.janusgraph.graphdb.database.management)
        getGraphIndex:424, ManagementSystem (org.janusgraph.graphdb.database.management)
            getSchemaVertex:878, StandardJanusGraphTx (org.janusgraph.graphdb.transaction) 
                 // 这里 getSchemaVertex 的内容之前已经讨论过。
    updatedTypes.add((PropertyKeyVertex) key);
    2. transaction.makeSchemaVertex(JanusGraphSchemaCategory.GRAPHINDEX, indexName, def);
        // 这个之前已经说过，我们在简单过一遍
        1. graph.assignID(schemaVertex, BaseVertexLabel.DEFAULT_VERTEXLABEL);
        2. addProperty(schemaVertex, BaseKey.SchemaName, schemaCategory.getSchemaName(name));
    3. addSchemaEdge(indexVertex, keys[i], TypeDefinitionCategory.INDEX_FIELD, paras);
    4. updateIndex(index, SchemaAction.REGISTER_INDEX);
        
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;updatedTypes.add((PropertyKeyVertex) key);
schema 分析主要就是这些，我们还有一些地方没细看，接下来我们把几个复杂的过程分析一下。主要包括查询数据库和 update 索引&lt;/p&gt;

&lt;h1 id=&#34;局部调试&#34;&gt;局部调试&lt;/h1&gt;

&lt;p&gt;上面的调试过程让我们大概明白了每一步的过程，大概都在做什么，接下来我们要深入一些局部，看一下每一步具体都在做什么。&lt;/p&gt;

&lt;h2 id=&#34;1-makepropertykey&#34;&gt;1. makePropertyKey&lt;/h2&gt;

&lt;p&gt;我们从简单到复杂，首先看 makePropertyKey，看之前我们大概了解几个相关类。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;JanusGraphSchemaCategory
这个是 JanusGraph 的所有 schema 的种类，有 EDGELABEL, PROPERTYKEY, VERTEXLABEL, GRAPHINDEX, TYPE_MODIFIER 五种。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;PropertyKeyVertex
我们所有的 schema 都是以顶点的形式存在数据库中，所以我们 makePropertyKey 也会创建一个顶点，这个顶点的类型是 PropertyKeyVertex 。
他继承自 RelationTypeVertex，PropertyKey， JanusGraphSchemaVertex，InternalRelationType，RelationType，InternalVertex 等类。
他有 getBaseType getRelationIndexes getKeyIndexes 等方法，&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;BaseKey
BaseKey 和 PropertyKeyVertex 类似，PropertyKeyVertex 是我们定义的 schema，而 BaseKey 则是最基本的key，是 schema 的 ProperyKey, 他们是直接放在内存中的。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;JanusGraphVertexProperty&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;JanusGraphVertexProperty 代表一个顶点的 Property，和 JanusGraphEdge 一样继承自 JanusGraphRelation。
当我们给一个 JanusGraph 添加 Property，实际上会创建一条关系，同时返回一个 JanusGraphVertexProperty。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;InternalRelation&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;InternalRelation 代表一个关系，实际上就是一条边，在 janus 中，分为 VertexProperty 和 Edge 两种，无论是 Edge 还是 VertexProperty ，都是连接两个顶点。
其中 VertexProperty 是连接一个用户创建的顶点和一个 PropertyKey 顶点，而 Edge 是连接两个 PropertyKey 顶点。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;ElementCategory&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;元素种类，有 VERTEX, EDGE, PROPERTY 三种，可以用来判断 index 的种类。&lt;/p&gt;

&lt;h3 id=&#34;进入断点&#34;&gt;进入断点&lt;/h3&gt;

&lt;p&gt;我们进入断点到： makeSchemaVertex:830, StandardJanusGraphTx (org.janusgraph.graphdb.transaction)&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;code&gt;schemaVertex = new PropertyKeyVertex(this, IDManager.getTemporaryVertexID(IDManager.VertexIDType.UserPropertyKey, temporaryIds.nextID()), ElementLifeCycle.New);&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;新建一个代表 PropertyKey 的 Vertex。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;addProperty(schemaVertex, BaseKey.SchemaName, schemaCategory.getSchemaName(name));&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;
1. vertex = ((InternalVertex) vertex).it();

// 新建一个 VertexProperty 的对象
2. StandardVertexProperty prop = new StandardVertexProperty(IDManager.getTemporaryRelationID(temporaryIds.nextID()), key, (InternalVertex) vertex, normalizedValue, ElementLifeCycle.New);

3.connectRelation(InternalRelation r) 
    
    success = r.getVertex(i).addRelation(r);
        r.getVertex(i) 返回的是前面创建的 PropertyKeyVertex
        addRelation 是在这个 Vertex 内部调用 addedRelations.add(r)
    
    addedRelations.add(r); // 这个 addedRelations 是 StandardJanusGraph 的全局变量

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以看出，addProperty 实际上就是给 顶点和另一个 PropertyKey 建立一条边。&lt;/p&gt;

&lt;p&gt;到这里似乎就完成了，整个过程实际上就是修改了 addedRelations 。&lt;/p&gt;

&lt;h2 id=&#34;makevertexlabel-makeedgelabel&#34;&gt;makeVertexLabel makeEdgeLabel&lt;/h2&gt;

&lt;p&gt;这两个与 PropertyKey 类似，首先 new JanusGraphSchemaVertex ，分别是 PropertyKeyVertex EdgeLabelVertex VertexLabelVertex 。 然后调用 addProperty 。
addProperty 会 new 一个 StandardVertexProperty ，然后调用 connectRelation(prop) 。将 prop 中的 Relation 都建立连接，添加到 addedRelations。&lt;/p&gt;

&lt;h2 id=&#34;commit-preparecommit&#34;&gt;commit prepareCommit&lt;/h2&gt;

&lt;p&gt;//1) Collect deleted edges and their index updates and acquire edge locks
略
//2) Collect added edges and their index updates and acquire edge locks&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;
// 前面所有的关系 关系类型是 InternalRelation ，实现有 StandardVertexProperty 和 StandardEdge 两种
for (InternalRelation add : Iterables.filter(addedRelations,filter)) {
    
    // 每个 Relation 联系多个顶点，如果是 StandardVertexProperty 顶点就是 JanusGraphVertex，如果是 StandardEdge，顶点就是连接的两个 JanusGraphVertex
    for (int pos = 0; pos &amp;lt; add.getLen(); pos++) {
    	// 得到对应的顶点，可能有一个或者两个
    	InternalVertex vertex = add.getVertex(pos);
    	if (pos == 0 || !add.isLoop()) {
    	    
    	    // mutatedProperties: key 是关系连接的 vertex，value 是关系
    	    if (add.isProperty()) mutatedProperties.put(vertex,add);
    	    // mutations: key 是 vertex id, 关系是 add
    	    mutations.put(vertex.longId(), add);
    	}
    	if (!vertex.isNew() &amp;amp;&amp;amp; acquireLock(add,pos,acquireLocks)) {
    	    Entry entry = edgeSerializer.writeRelation(add, pos, tx);
    	    mutator.acquireEdgeLock(idManager.getKey(vertex.longId()), entry.getColumn());
        }
    }
    // indexUpdates : IndexSerializer.IndexUpdate
    indexUpdates.addAll(indexSerializer.getIndexUpdates(add));
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;//3) Collect all index update for vertices&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;for (InternalVertex v : mutatedProperties.keySet()) {
    indexUpdates.addAll(indexSerializer.getIndexUpdates(v,mutatedProperties.get(v)));
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;//4) Acquire index locks (deletions first)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;for (IndexSerializer.IndexUpdate update : indexUpdates) {
    if (!update.isCompositeIndex() || !update.isDeletion()) continue;
    CompositeIndexType iIndex = (CompositeIndexType) update.getIndex();
    if (acquireLock(iIndex,acquireLocks)) {
        mutator.acquireIndexLock((StaticBuffer)update.getKey(), (Entry)update.getEntry());
    }
}
for (IndexSerializer.IndexUpdate update : indexUpdates) {
    if (!update.isCompositeIndex() || !update.isAddition()) continue;
    CompositeIndexType iIndex = (CompositeIndexType) update.getIndex();
    if (acquireLock(iIndex,acquireLocks)) {
        mutator.acquireIndexLock((StaticBuffer)update.getKey(), ((Entry)update.getEntry()).getColumn());
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;//5) Add relation mutations&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;// 遍历 mutations，
for (Long vertexid : mutations.keySet()) {
    Preconditions.checkArgument(vertexid &amp;gt; 0, &amp;quot;Vertex has no id: %s&amp;quot;, vertexid);
    List&amp;lt;InternalRelation&amp;gt; edges = mutations.get(vertexid);
    List&amp;lt;Entry&amp;gt; additions = new ArrayList&amp;lt;Entry&amp;gt;(edges.size());
    List&amp;lt;Entry&amp;gt; deletions = new ArrayList&amp;lt;Entry&amp;gt;(Math.max(10, edges.size() / 10));
    
    // 这个顶点所有的 edges
    for (InternalRelation edge : edges) {
        InternalRelationType baseType = (InternalRelationType) edge.getType();
        assert baseType.getBaseType()==null;
        // 这个 InternalRelationType 的所有 type ，这里有点不太懂
        for (InternalRelationType type : baseType.getRelationIndexes()) {
            if (type.getStatus()== SchemaStatus.DISABLED) continue;
            
            // Arity 应该是数据的量，代表的意义应该是 LIST SINGLE 等
            for (int pos = 0; pos &amp;lt; edge.getArity(); pos++) {
                if (!type.isUnidirected(Direction.BOTH) &amp;amp;&amp;amp; !type.isUnidirected(EdgeDirection.fromPosition(pos)))
                    continue; //Directionality is not covered
                if (edge.getVertex(pos).longId()==vertexid) {
                    // 序列化数据
                    StaticArrayEntry entry = edgeSerializer.writeRelation(edge, type, pos, tx);
                    if (edge.isRemoved()) {
                        deletions.add(entry);
                    } else {
                        Preconditions.checkArgument(edge.isNew());
                        int ttl = getTTL(edge);
                        if (ttl &amp;gt; 0) {
                            entry.setMetaData(EntryMetaData.TTL, ttl);
                        }
                        // 添加到 additions
                        additions.add(entry);
                    }
                }
            }
        }
    }

    StaticBuffer vertexKey = idManager.getKey(vertexid);
    // 写出数据
    mutator.mutateEdges(vertexKey, additions, deletions);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;//6) Add index updates&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;for (IndexSerializer.IndexUpdate indexUpdate : indexUpdates) {
    assert indexUpdate.isAddition() || indexUpdate.isDeletion();
    if (indexUpdate.isCompositeIndex()) {
        IndexSerializer.IndexUpdate&amp;lt;StaticBuffer,Entry&amp;gt; update = indexUpdate;
        if (update.isAddition())
            // 直接调用 update 的方法
            mutator.mutateIndex(update.getKey(), Lists.newArrayList(update.getEntry()), KCVSCache.NO_DELETIONS);
        else
            mutator.mutateIndex(update.getKey(), KeyColumnValueStore.NO_ADDITIONS, Lists.newArrayList(update.getEntry()));
    } else {
        IndexSerializer.IndexUpdate&amp;lt;String,IndexEntry&amp;gt; update = indexUpdate;
        has2iMods = true;
        IndexTransaction itx = mutator.getIndexTransaction(update.getIndex().getBackingIndexName());
        String indexStore = ((MixedIndexType)update.getIndex()).getStoreName();
        if (update.isAddition())
            itx.add(indexStore, update.getKey(), update.getEntry(), update.getElement().isNew());
        else
            itx.delete(indexStore,update.getKey(),update.getEntry().field,update.getEntry().value,update.getElement().isRemoved());
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到这里我们可能比较迷惑的就是 IndexUpdate 是怎么获得的。&lt;/p&gt;

&lt;p&gt;获得 IndexUpdate 的思路大概是这样：以 CompositeIndex 为例，假如一个顶点，USER，有 name 和 sex 两个 PropertyKey，并且基于 name 和 sex 做了一个 CompositeIndex。
现在有一个顶点，假设 id 为 007，我设置了他的 name 为 &amp;ldquo;deng&amp;rdquo;，然后我们需要获得这个用户的 sex ，假设为 &amp;ldquo;male&amp;rdquo;，这时候我们需要在 index 插入一条记录 (deng,male) =&amp;gt; 007。&lt;/p&gt;

&lt;p&gt;所以我们可以看 getIndexUpdates 的源代码，首先是  IndexField[] fields = index.getFieldKeys() 得到这个 index 所有的 filedKey， 然后 new RecordEntry[fields.length]，得到一个
和 fields 长度一样的 RecordEntry 数组，然后从 pos=0 开始给 IndexField 数组赋值，直到 pos &amp;gt;= fields.length。这样就得到了所以和这个属性更新相关的索引更新。
已上面的例子为例，那么得到的 RecordEntry[] 就是 [deng,male]。&lt;/p&gt;

&lt;p&gt;然后我们得到了 indexUpdate additions 就是分别将他们写到数据库了。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>neo4j源码分析1-编译打包启动</title>
      <link>https://dengziming.github.io/post/neo4j/neo4j%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%901-%E7%BC%96%E8%AF%91%E6%89%93%E5%8C%85%E5%90%AF%E5%8A%A8/</link>
      <pubDate>Thu, 22 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/neo4j/neo4j%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%901-%E7%BC%96%E8%AF%91%E6%89%93%E5%8C%85%E5%90%AF%E5%8A%A8/</guid>
      
        <description>

&lt;h2 id=&#34;1-打包&#34;&gt;1.打包&lt;/h2&gt;

&lt;h3 id=&#34;1-打包community&#34;&gt;1.打包community&lt;/h3&gt;

&lt;p&gt;进入community,neo4j-graphdb-api，
注释掉common的：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;plugin&amp;gt;
  &amp;lt;groupId&amp;gt;org.revapi&amp;lt;/groupId&amp;gt;
  &amp;lt;artifactId&amp;gt;revapi-maven-plugin&amp;lt;/artifactId&amp;gt;
&amp;lt;/plugin&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;里面好像涉及到了版本检查，如果某个类的最新发布版本已经没有这个方法，打包会失败，反正对打包有影响，不删除可能会失败。&lt;/p&gt;

&lt;p&gt;还可能要在主项目的pom里面注释掉：&lt;code&gt;maven-checkstyle-plugin&lt;/code&gt;，代码风格检查可能会通不过。
然后用maven命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mvn -settings ~/opt/soft/apache-maven-3.5.0/conf/settings.xml -Dlicense.skip=true -DskipTests package install
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;2-打包企业版&#34;&gt;2.打包企业版&lt;/h3&gt;

&lt;p&gt;进入enterprise,ha目录
进入management,注释掉 &lt;groupId&gt;org.revapi&lt;/groupId&gt;
还有其他问题，比如java文件没有license，这里不一一列举。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mvn -settings ~/opt/soft/apache-maven-3.5.0/conf/settings.xml -Dlicense.skip=true -DskipTests package install
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;3-打包完整的tar包&#34;&gt;3. 打包完整的tar包&lt;/h3&gt;

&lt;p&gt;进入项目路径&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mvn clean install -Dmaven.test.skip=true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;要注意两个参数的异同点：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
-DskipTests，不执行测试用例，但编译测试用例类生成相应的class文件至target/test-classes下。

-Dmaven.test.skip=true，不执行测试用例，也不编译测试用例类。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;打包的输出文件：packaging/standalone/target/neo4j-community-3.4.0-SNAPSHOT-unix.tar.gz，这个就是我们的neo4j包。解压后，放到一个目录。一方面你可以选择执行 bin/neo4j start 启动neo4j，我们要分析源码，自然会是在本地启动。&lt;/p&gt;

&lt;h2 id=&#34;二-运行&#34;&gt;二、运行&lt;/h2&gt;

&lt;h3 id=&#34;1-启动&#34;&gt;1.启动&lt;/h3&gt;

&lt;p&gt;我们在IDEA中，找到入口类：org.neo4j.server.CommunityEntryPoint，点击运行，然后会报错，我们需要添加运行参数：&lt;/p&gt;

&lt;p&gt;-server &amp;ndash;home-dir=~/neo4j-community-3.2.6 &amp;ndash;config-dir=~/neo4j-community-3.2.6/conf&lt;/p&gt;

&lt;p&gt;这里的参数是刚刚解压的neo4j目录和配置文件。然后运行成功，访问 &lt;a href=&#34;http://localhost:7474/browser/，会发现有问题。&#34;&gt;http://localhost:7474/browser/，会发现有问题。&lt;/a&gt;
通过调试前端的js代码，我们发现版本有问题，这里我们稍作修改，找到 org.neo4j.kernel.internal.Version。最后的代码注释掉，换成我们的版本，也就是将Version.class.getPackage().getImplementationVersion() 换成 3.4，然后就可以运行成功了。
打开7474端口，写cypher语言，查看。&lt;/p&gt;

&lt;h3 id=&#34;2-打断点调试&#34;&gt;2.打断点调试&lt;/h3&gt;

&lt;p&gt;既然是源码分析，我们的办法就是先看，然后打断点调试，查看调用栈，但是由于是多线程，其实还是很有难度的，容易跟丢，后续我们慢慢来吧。&lt;/p&gt;

&lt;h3 id=&#34;3-代码结构查看&#34;&gt;3.代码结构查看&lt;/h3&gt;

&lt;p&gt;看源码之前我们先大概过一下代码结构。我们主要看 community 模块的结构，里面有很多子模块。&lt;/p&gt;

&lt;p&gt;我们可以大概根据名字猜测 ：io模块是用来处理读写数据的，kernel模块是我们需要着重查看的。bolt是处理bolt连接的，server是整个项目启动的。codegen是动态生成代码的。我们要从内核部分开始看。&lt;/p&gt;

&lt;h3 id=&#34;4-架构了解&#34;&gt;4.架构了解&lt;/h3&gt;

&lt;p&gt;The node records contain only a pointer to their first property and their first relationship (in what is oftentermed the _relationship chain). From here, we can follow the (doubly) linked-list of relationships until we find the one we’re interested in, the LIKES relationship from Node 1 to Node 2 in this case. Once we’ve found the relationship record of interest, we can simply read its properties if there are any via the same singly-linked list structure as node properties, or we can examine the node records that it relates via its start node and end node IDs. These IDs, multiplied by the node record size, of course give the immediate offset of both nodes in the node store file.&lt;/p&gt;

&lt;p&gt;这段话来自&lt;Graph Databases&gt;(作者：IanRobinson) 一书。描述了neo4j的存储方式。详情可以查阅其他资料。&lt;/p&gt;

&lt;h3 id=&#34;5-源码查看&#34;&gt;5.源码查看&lt;/h3&gt;

&lt;p&gt;参考下一篇&lt;/p&gt;
</description>
      
    </item>
    
  </channel>
</rss>