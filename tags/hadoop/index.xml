<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hadoop on 数据分析师之旅</title>
    <link>https://dengziming.github.io/tags/hadoop/</link>
    <description>Recent content in Hadoop on 数据分析师之旅</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Wed, 23 May 2018 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://dengziming.github.io/tags/hadoop/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>resourcemanager</title>
      <link>https://dengziming.github.io/post/hadoop/hadoopha/</link>
      <pubDate>Wed, 23 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/hadoop/hadoopha/</guid>
      
        <description>

&lt;p&gt;参考资料：&lt;/p&gt;

&lt;h1 id=&#34;hadoopha&#34;&gt;HadoopHa&lt;/h1&gt;

&lt;p&gt;hadoop 有两个NameNode，Active NameNode和Standby NameNode，通过 DFSZKFailoverController extends ZKFailoverController 进行切换。
ZKFailoverController通过HealthMonitor线程能及时检测到NameNode的健康状况，在主NameNode故障时借助Zookeeper实现自动的主备选举和切换。
DataNode 会同时向主NameNode和备NameNode上报数据块的位置信息，但只接收来自active namenode的读写命令。&lt;/p&gt;

&lt;p&gt;为啥把监控分开?&lt;/p&gt;

&lt;p&gt;显然，我们不能在NN进程内进行心跳等信息同步，最简单的原因，一次FullGC就可以让NN挂起十几分钟，所以，必须要有一个独立的短小精悍的watchdog来专门负责监控。这也是一个松耦合的设计，便于扩展或更改。&lt;/p&gt;

&lt;p&gt;通过隔离和Quorum Journal Manager(QJM)共享存储空间实现HDFS HA&lt;/p&gt;

&lt;h1 id=&#34;dfszkfailovercontroller&#34;&gt;DFSZKFailoverController&lt;/h1&gt;

&lt;p&gt;启动代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;  public static void main(String args[])
      throws Exception {
    if (DFSUtil.parseHelpArgument(args, 
        ZKFailoverController.USAGE, System.out, true)) {
      System.exit(0);
    }
    
    GenericOptionsParser parser = new GenericOptionsParser(
        new HdfsConfiguration(), args);
    DFSZKFailoverController zkfc = DFSZKFailoverController.create(
        parser.getConfiguration());
    {
        NNHAServiceTarget localTarget = new NNHAServiceTarget(
        localNNConf, nsId, nnId);
        return new DFSZKFailoverController(localNNConf, localTarget);
    }
    
    System.exit(zkfc.run(parser.getRemainingArgs()));
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;run 的步骤：
initZK();
formatZK(force, interactive);
initRPC();
initHM();
startRPC();
mainLoop();&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;initZK();
{
    elector = new ActiveStandbyElector(zkQuorum,
        zkTimeout, getParentZnode(), zkAcls, zkAuths,
        new ElectorCallbacks(), maxRetryNum);
    {
    	new ElectorCallbacks()
    	  // 临时节点ActiveStandbyElectorLock，用于标识锁
    	zkLockFilePath = znodeWorkingDir + &amp;quot;/&amp;quot; + LOCK_FILENAME;
    	// 永久节点ActiveBreadCrumb，用于存放active信息
    	zkBreadCrumbPath = znodeWorkingDir + &amp;quot;/&amp;quot; + BREADCRUMB_FILENAME;
    	this.maxRetryNum = maxRetryNum;
    	// createConnection for future API calls
    	// 创建zk连接
    	createConnection();
    	{
    	      // 不幸的是，zk的构造方法连接上zk之后，可能马上触发连接事件。
  			  // 因此如果构造zk之后注册watcher，可能不会捕获到连接事件。
  			  // 取而代之的方法是，先构造Watcher，在设置了zk的引用之前，使它阻塞所有的事件
  			  
  			  watcher = new WatcherWithClientRef();
  			  ZooKeeper zk = new ZooKeeper(zkHostPort, zkSessionTimeout, watcher);
  			  // 在watcher中设置zk的引用
  			  watcher.setZooKeeperRef(zk);
  			  // Wait for the asynchronous success/failure. This may throw an exception
  			  // if we don&#39;t connect within the session timeout.
  			  watcher.waitForZKConnectionEvent(zkSessionTimeout);
  			  
  			  for (ZKAuthInfo auth : zkAuthInfo) {
  			    zk.addAuthInfo(auth.getScheme(), auth.getAuth());
  			  }
  			  return zk;
    	      }
    	  }
    	}
}

formatZK(force, interactive);
initRPC();
{
    new ZKFCRpcServer(conf, bindAddr, this, getPolicyProvider());
    {
          this.zkfc = zkfc;
  			// 使用protocol buffer序列化
  			RPC.setProtocolEngine(conf, ZKFCProtocolPB.class,
  			    ProtobufRpcEngine.class);
  			ZKFCProtocolServerSideTranslatorPB translator =
  			    new ZKFCProtocolServerSideTranslatorPB(this);
  			BlockingService service = ZKFCProtocolService
  			    .newReflectiveBlockingService(translator);
  			// 使用hadoop rpc接口得到rpc server
  			// ZKFCProtocol是rpc协议，service是rpc协议的实现类
  			// ZKFCProtocolPB是protobuf rpc接口的一个过渡类
  			this.server = new RPC.Builder(conf).setProtocol(ZKFCProtocolPB.class)
  			    .setInstance(service).setBindAddress(bindAddr.getHostName())
  			    .setPort(bindAddr.getPort()).setNumHandlers(HANDLER_COUNT)
  			    .setVerbose(false).build();
    }
}
initHM();
startRPC();
mainLoop();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;WatcherWithClientRef 在构造zk时被注册为默认watcher，主要监听连接或者断开事件。当调用initZk之后，watcher.process会对事件进行处理，连接、断开、过期的状态类型都是EventType.None。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>yarn-api使用</title>
      <link>https://dengziming.github.io/post/hadoop/yarn-api%E4%BD%BF%E7%94%A8/</link>
      <pubDate>Wed, 23 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/hadoop/yarn-api%E4%BD%BF%E7%94%A8/</guid>
      
        <description>

&lt;p&gt;参考： &lt;a href=&#34;https://ieevee.com/tech/2015/05/05/yarn-dist-shell.html&#34;&gt;https://ieevee.com/tech/2015/05/05/yarn-dist-shell.html&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;distributeshell&#34;&gt;distributeshell&lt;/h1&gt;

&lt;h2 id=&#34;client解析&#34;&gt;Client解析&lt;/h2&gt;

&lt;p&gt;distShell主要有2个类组成，Client和ApplicationMaster。两个类都带有main入口。Client的主要工作是启动AM，真正要做的任务由AM来调度。 Client的简化框架如下。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public static void main(String[] args) {
    boolean result = false;
    try {
      Client client = new Client();  //1 创建Client对象
      try {
        boolean doRun = client.init(args);  //2 初始化
        if (!doRun) {
          System.exit(0);
        }
      }
      result = client.run();   //3 运行
    }
    if (result) {
      System.exit(0);
    }
    System.exit(2);
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;1-创建client对象&#34;&gt;1 创建Client对象&lt;/h3&gt;

&lt;p&gt;创建时会指定本Client要用到的AM。 创建yarnClient。yarn将client与RM的交互抽象出了编程库YarnClient，用以应用程序提交、状态查询和控制等，简化应用程序。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;  public Client(Configuration conf) throws Exception  {
    this(		//指定AM
      &amp;quot;org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster&amp;quot;,
      conf);
  Client(String appMasterMainClass, Configuration conf) {
    this.conf = conf;
    this.appMasterMainClass = appMasterMainClass;
    yarnClient = YarnClient.createYarnClient();		//创建yarnClient
    yarnClient.init(conf);
    opts = new Options();	//创建opts，后面解析参数的时候用
    opts.addOption(&amp;quot;appname&amp;quot;, true, &amp;quot;Application Name. Default value - DistributedShell&amp;quot;);
    opts.addOption(&amp;quot;priority&amp;quot;, true, &amp;quot;Application Priority. Default 0&amp;quot;);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;2-初始化&#34;&gt;2 初始化&lt;/h3&gt;

&lt;p&gt;init会解析命令行传入的参数，例如使用的jar包、内存大小、cpu个数等。 代码里使用GnuParser解析：init时定义所有的参数opts（可以认为是一个模板），
然后将opts和实际的args传入解析后得到一个CommnadLine对象，后面查询选项直接操作该CommnadLine对象即可，如cliParser.hasOption(&amp;ldquo;help&amp;rdquo;)和cliParser.getOptionValue(&amp;ldquo;jar&amp;rdquo;)。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt; public boolean init(String[] args) throws ParseException {
    CommandLine cliParser = new GnuParser().parse(opts, args);
    amMemory = Integer.parseInt(cliParser.getOptionValue(&amp;quot;master_memory&amp;quot;, &amp;quot;10&amp;quot;));
    amVCores = Integer.parseInt(cliParser.getOptionValue(&amp;quot;master_vcores&amp;quot;, &amp;quot;1&amp;quot;));
    shellCommand = cliParser.getOptionValue(&amp;quot;shell_command&amp;quot;);
    appMasterJar = cliParser.getOptionValue(&amp;quot;jar&amp;quot;);
    ...
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;3-运行&#34;&gt;3 运行&lt;/h3&gt;

&lt;p&gt;先启动yarnClient，会建立跟RM的RPC连接，之后就跟调用本地方法一样。通过此yarnClient查询NM个数、NM详细信息（ID/地址/Container个数等）、Queue info（其实没用到，示例里只是打印了下调试用）。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;
public class Client {
  public boolean run() throws IOException, YarnException {
    yarnClient.start();
    YarnClusterMetrics clusterMetrics = yarnClient.getYarnClusterMetrics();
    List&amp;lt;NodeReport&amp;gt; clusterNodeReports = yarnClient.getNodeReports(
收集提交AM所需的信息。
    YarnClientApplication app = yarnClient.createApplication();	//创建app
    GetNewApplicationResponse appResponse = app.getNewApplicationResponse();
...
    ApplicationSubmissionContext appContext = app.getApplicationSubmissionContext();
    //AM需要的本地资源，如jar包、log文件
    Map&amp;lt;String, LocalResource&amp;gt; localResources = new HashMap&amp;lt;String, LocalResource&amp;gt;();

    FileSystem fs = FileSystem.get(conf);
    addToLocalResources(fs, appMasterJar, appMasterJarPath, appId.toString(),
        localResources, null);
    ...	//添加localResource

    vargs.add(Environment.JAVA_HOME.$$() + &amp;quot;/bin/java&amp;quot;);
    vargs.add(&amp;quot;-Xmx&amp;quot; + amMemory + &amp;quot;m&amp;quot;);
    vargs.add(appMasterMainClass);
...
    for (CharSequence str : vargs) {
      command.append(str).append(&amp;quot; &amp;quot;);	//重新组织命令行
    }
	//创建Container加载上下文，包含本地资源，环境变量，实际命令。
    ContainerLaunchContext amContainer = ContainerLaunchContext.newInstance(
      localResources, env, commands, null, null, null);

    Resource capability = Resource.newInstance(amMemory, amVCores);
    appContext.setResource(capability);		//请求使用的内存、cpu

    appContext.setAMContainerSpec(amContainer);
    appContext.setQueue(amQueue);

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;重新组织出来的commands如下：&lt;/p&gt;

&lt;p&gt;$JAVA_HOME/bin/java -Xmx10m org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster &amp;ndash;container_memory 10
提交AM（即appContext），并启动监控。 Client只关心自己提交到RM的AM是否正常运行，而AM内部的多个task，由AM管理。如果Client要查询应用程序的任务信息，需要自己设计与AM的交互。
    yarnClient.submitApplication(appContext);   //客户端提交AM到RM
    return monitorApplication(appId);
总的来说，Client做的事情比较简单，即建立与RM的连接，提交AM，监控AM运行状态。&lt;/p&gt;

&lt;p&gt;有个疑问，走读代码没有看到jar包是怎么送到NM上去的。&lt;/p&gt;

&lt;h2 id=&#34;application-master解析&#34;&gt;Application Master解析&lt;/h2&gt;

&lt;p&gt;AM简化框架如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;

      boolean doRun = appMaster.init(args);
      if (!doRun) {
        System.exit(0);
      }
      appMaster.run();
      result = appMaster.finish();
// yarn抽象了两个编程库，AMRMClient和NMClient(AM和RM都可以用)，简化AM编程。

// 1 设置RM、NM消息的异步处理方法
    AMRMClientAsync.CallbackHandler allocListener = new RMCallbackHandler();
    amRMClient = AMRMClientAsync.createAMRMClientAsync(1000, allocListener);
    amRMClient.init(conf);
    amRMClient.start();

    containerListener = createNMCallbackHandler();
    nmClientAsync = new NMClientAsyncImpl(containerListener);
    nmClientAsync.init(conf);
    nmClientAsync.start();
// 2 向RM注册
    RegisterApplicationMasterResponse response = amRMClient.registerApplicationMaster(appMasterHostname,
        appMasterRpcPort, appMasterTrackingUrl);
// 3 计算需要的Container，向RM发起请求
    // Setup ask for containers from RM
    // Send request for containers to RM
    // Until we get our fully allocated quota, we keep on polling RM for
    // containers
    // Keep looping until all the containers are launched and shell script
    // executed on them ( regardless of success/failure).
    for (int i = 0; i &amp;lt; numTotalContainersToRequest; ++i) {
      ContainerRequest containerAsk = setupContainerAskForRM();
      amRMClient.addContainerRequest(containerAsk);		//请求指定个数的Container
    }

  private ContainerRequest setupContainerAskForRM() {
    Resource capability = Resource.newInstance(containerMemory,
      containerVirtualCores);		//指定需要的memory/cpu能力
    ContainerRequest request = new ContainerRequest(capability, null, null,
        pri);


4 // RM分配Container给AM，AM启动任务RMCallbackHandler RM消息的响应，由RMCallbackHandler处理。示例中主要对前两种消息进行了处理。

  private class RMCallbackHandler implements AMRMClientAsync.CallbackHandler {
    //处理消息：Container执行完毕。在RM返回的心跳应答中携带。如果心跳应答中有已完成和新分配两种Container，先处理已完成
    public void onContainersCompleted(List&amp;lt;ContainerStatus&amp;gt; completedContainers) {
...
    //处理消息：RM新分配Container。在RM返回的心跳应答中携带
    public void onContainersAllocated(List&amp;lt;Container&amp;gt; allocatedContainers) {

    public void onShutdownRequest() {done = true;}

    //节点状态变化
    public void onNodesUpdated(List&amp;lt;NodeReport&amp;gt; updatedNodes) {}

    public float getProgress() {
onContainersAllocated收到分配的Container之后，会提交任务到NM。

    public void onContainersAllocated(List&amp;lt;Container&amp;gt; allocatedContainers) {
        LaunchContainerRunnable runnableLaunchContainer =   //创建runnable容器
            new LaunchContainerRunnable(allocatedContainer, containerListener);
        Thread launchThread = new Thread(runnableLaunchContainer);	//新建线程

        // launch and start the container on a separate thread to keep
        // the main thread unblocked
        // as all containers may not be allocated at one go.
        launchThreads.add(launchThread);
        launchThread.start();	//线程中提交Container到NM，不影响主流程

//简单分析下LaunchContainerRunnable。该类实现自Runnable，其run方法准备任务命令（本例即为date）。

  private class LaunchContainerRunnable implements Runnable {
    public LaunchContainerRunnable(
        Container lcontainer, NMCallbackHandler containerListener) {
      this.container = lcontainer;		//创建时记录待使用的Container
      this.containerListener = containerListener;
    }
    public void run() {
      vargs.add(shellCommand);		//待执行的shell命令
      vargs.add(shellArgs);			//shell命令参数
      List&amp;lt;String&amp;gt; commands = new ArrayList&amp;lt;String&amp;gt;();
      commands.add(command.toString());	//转为commands

      //根据命令、环境变量、本地资源等创建Container加载上下文
      ContainerLaunchContext ctx = ContainerLaunchContext.newInstance(
              localResources, shellEnv, commands, null, allTokens.duplicate(), null);
      containerListener.addContainer(container.getId(), container);
      //异步启动Container
      nmClientAsync.startContainerAsync(container, ctx);
// onContainersCompleted的功能比较简单，收到Container执行完毕的消息，检查其执行结果，如果执行失败，则重新发起请求，直到全部完成。

// NMCallbackHandler NM消息的响应，由NMCallbackHandler处理。

//在distShell示例里，回调句柄对NM通知过来的各种事件的处理比较简单，只是修改AM维护的Container执行完成、失败的个数。这样等到有Container执行完毕后，可以重启发起请求。失败处理和上面Container执行完毕消息的处理类似，达到了上面问题里所说的loopback效果。

  static class NMCallbackHandler
    implements NMClientAsync.CallbackHandler {

    @Override
    public void onContainerStopped(ContainerId containerId) {

    @Override
    public void onContainerStatusReceived(ContainerId containerId,

    @Override
    public void onContainerStarted(ContainerId containerId,
...
总的来说，AM做的事就是向RM/NM注册回调函数，然后请求Container；得到Container后提交任务，并跟踪这些任务的执行情况，如果失败了则重新提交，直到全部任务完成。
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;unmanagedam&#34;&gt;UnmanagedAM&lt;/h1&gt;

&lt;p&gt;distShell的Client提交AM到RM后，由RM将AM分配到某一个NM上的Container，这样给AM调试带来了困难。yarn提供了一个参数，Client可以设置为Unmanaged，提交AM后，会在客户端本地起一个单独的进程来运行AM。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;
public class UnmanagedAMLauncher {
  public void launchAM(ApplicationAttemptId attemptId)
    //创建新进程
    Process amProc = Runtime.getRuntime().exec(amCmd, envAMList.toArray(envAM));
    try {
      int exitCode = amProc.waitFor();  //等待AM进程结束
    } finally {
      amCompleted = true;
    }

  public boolean run() throws IOException, YarnException {
      appContext.setUnmanagedAM(true);		//设置为Unmanaged
      rmClient.submitApplication(appContext);	//提交AM

      ApplicationReport appReport =		//监控AM状态，如果状态变为ACCEPTED，则跳出循环，launchAM。
          monitorApplication(appId, EnumSet.of(YarnApplicationState.ACCEPTED,
            YarnApplicationState.KILLED, YarnApplicationState.FAILED,
            YarnApplicationState.FINISHED));

      if (appReport.getYarnApplicationState() == YarnApplicationState.ACCEPTED) {
        launchAM(attemptId);
&lt;/code&gt;&lt;/pre&gt;
</description>
      
    </item>
    
    <item>
      <title>hdfs-client</title>
      <link>https://dengziming.github.io/post/hadoop/hdfs-client/</link>
      <pubDate>Tue, 22 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/hadoop/hdfs-client/</guid>
      
        <description>

&lt;p&gt;参考资料：&lt;/p&gt;

&lt;p&gt;hadoop 技术内幕丛书&lt;/p&gt;

&lt;h1 id=&#34;写&#34;&gt;写&lt;/h1&gt;

&lt;h1 id=&#34;创建流&#34;&gt;创建流&lt;/h1&gt;

&lt;p&gt;简单写一个 demo 进行测试，通过打断点方法，另外发现一个问题， 在 dfsClient.namenode.create 打断点调试会报错，可能是因为动态代理卡主了 ：&lt;/p&gt;

&lt;p&gt;注意我们上传的文件 nio-data.txt 内容可以进行控制，例如我们写 600 个 a(97)，转换为 DataOutputStream 后的 byte[] 就是 600个97 ，这样调试就知道是哪个数据，600 a 是因为每个chunk的默认大小是 512&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public static void main(String[] args) throws IOException {

    FileSystem fs = FileSystem.get(new Configuration());
    
    Path src = new Path(&amp;quot;ideaspace/learn-jvm/src/main/resources/data/nio-data.txt&amp;quot;); //文件里面是一个 java 代码
    Path desc = new Path(&amp;quot;/tmp/&amp;quot;);
    if (fs.exists(desc)){
            fs.delete(desc,true);
        }

    fs.copyFromLocalFile(src,desc);

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;经过一系列的调用后进入的第一个关键方法是 ：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;@Override
  public FSDataOutputStream create(final Path f, final FsPermission permission,
    final EnumSet&amp;lt;CreateFlag&amp;gt; cflags, final int bufferSize,
    final short replication, final long blockSize, final Progressable progress,
    final ChecksumOpt checksumOpt) throws IOException {
    statistics.incrementWriteOps(1);
    Path absF = fixRelativePart(f);
    return new FileSystemLinkResolver&amp;lt;FSDataOutputStream&amp;gt;() {
      @Override
      public FSDataOutputStream doCall(final Path p)
          throws IOException, UnresolvedLinkException {
        final DFSOutputStream dfsos = dfs.create(getPathName(p), permission,
                cflags, replication, blockSize, progress, bufferSize,
                checksumOpt);
        return dfs.createWrappedOutputStream(dfsos, statistics);
      }
      @Override
      public FSDataOutputStream next(final FileSystem fs, final Path p)
          throws IOException {
        return fs.create(p, permission, cflags, bufferSize,
            replication, blockSize, progress, checksumOpt);
      }
    }.resolve(this, absF);
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;1-create&#34;&gt;1.create&lt;/h1&gt;

&lt;p&gt;首先是 create，然后是 dfs.createWrappedOutputStream(out, statistics);&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public DFSOutputStream create(String src, 
                           FsPermission permission,
                           EnumSet&amp;lt;CreateFlag&amp;gt; flag, 
                           boolean createParent,
                           short replication,
                           long blockSize,
                           Progressable progress,
                           int buffersize,
                           ChecksumOpt checksumOpt,
                           InetSocketAddress[] favoredNodes) throws IOException {
  checkOpen();
  if (permission == null) {
    permission = FsPermission.getFileDefault();
  }
  FsPermission masked = permission.applyUMask(dfsClientConf.uMask);
  if(LOG.isDebugEnabled()) {
    LOG.debug(src + &amp;quot;: masked=&amp;quot; + masked);
  }
  String[] favoredNodeStrs = null;
  if (favoredNodes != null) {
    favoredNodeStrs = new String[favoredNodes.length];
    for (int i = 0; i &amp;lt; favoredNodes.length; i++) {
      favoredNodeStrs[i] = 
          favoredNodes[i].getHostName() + &amp;quot;:&amp;quot; 
                       + favoredNodes[i].getPort();
    }
  }
  final DFSOutputStream result = DFSOutputStream.newStreamForCreate(this,
      src, masked, flag, createParent, replication, blockSize, progress,
      buffersize, dfsClientConf.createChecksum(checksumOpt),
      favoredNodeStrs);
  beginFileLease(result.getFileId(), result);
  return result;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;两个方法比较关键：&lt;/p&gt;

&lt;p&gt;DFSOutputStream.newStreamForCreate 和 beginFileLease(result.getFileId(), result)&lt;/p&gt;

&lt;h2 id=&#34;1-newstreamforcreate-方法是第一次创建真正的-流-类是-dfsoutputstream&#34;&gt;1. newStreamForCreate 方法是第一次创建真正的 流，类是 DFSOutputStream&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;static DFSOutputStream newStreamForCreate(DFSClient dfsClient, String src,
      FsPermission masked, EnumSet&amp;lt;CreateFlag&amp;gt; flag, boolean createParent,
      short replication, long blockSize, Progressable progress, int buffersize,
      DataChecksum checksum, String[] favoredNodes) throws IOException {
    ...
    while (shouldRetry) {
      shouldRetry = false;
      try {
        stat = dfsClient.namenode.create(src, masked, dfsClient.clientName,
            new EnumSetWritable&amp;lt;CreateFlag&amp;gt;(flag), createParent, replication,
            blockSize, SUPPORTED_CRYPTO_VERSIONS);
        break;
      } catch (RemoteException re) {...}
    Preconditions.checkNotNull(stat, &amp;quot;HdfsFileStatus should not be null!&amp;quot;);
    final DFSOutputStream out = new DFSOutputStream(dfsClient, src, stat,
        flag, progress, checksum, favoredNodes);
    out.start();
    return out;
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;rpc
这部分没法调试，因为在远程。只能自己观看
通过 RPC 调用 NameNodeRpcServer.create -&amp;gt; namesystem.startFile -&amp;gt; startFileInt -&amp;gt; startFileInternal ，先跳过，&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;new DFSOutputStream(dfsClient, src, stat,flag, progress, checksum, favoredNodes);&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;/** Construct a new output stream for creating a file. */
  private DFSOutputStream(DFSClient dfsClient, String src, HdfsFileStatus stat,
      EnumSet&amp;lt;CreateFlag&amp;gt; flag, Progressable progress,
      DataChecksum checksum, String[] favoredNodes) throws IOException {
    this(dfsClient, src, progress, stat, checksum);
    this.shouldSyncBlock = flag.contains(CreateFlag.SYNC_BLOCK);

    computePacketChunkSize(dfsClient.getConf().writePacketSize, bytesPerChecksum);

    Span traceSpan = null;
    if (Trace.isTracing()) {
      traceSpan = Trace.startSpan(this.getClass().getSimpleName()).detach();
    }
    streamer = new DataStreamer(stat, traceSpan);
    if (favoredNodes != null &amp;amp;&amp;amp; favoredNodes.length != 0) {
      streamer.setFavoredNodes(favoredNodes);
    }
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;新建 DFSOutputStream 中有个重要的线程 DataStreamer，功能后续研究。
DFSOutputStream 中的成员变量我们可以好好看看，什么是 checksum，chunk，packet。另外它的父类 FSOutputSummer 也很重要。&lt;/p&gt;

&lt;h2 id=&#34;2-beginfilelease-这个可以暂时忽略-后面专门研究-lease&#34;&gt;2. beginFileLease 这个可以暂时忽略，后面专门研究 lease&lt;/h2&gt;

&lt;p&gt;dfs.createWrappedOutputStream(dfsos, statistics) 对上面创建的流就行一个 包装&lt;/p&gt;

&lt;p&gt;返回 return new HdfsDataOutputStream(dfsos, statistics, startPos);&lt;/p&gt;

&lt;p&gt;至此创建流的过程就完成了。我们大概回顾一下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;DistributedFileSystem.create(final Path f, final FsPermission permission,
final EnumSet&amp;lt;CreateFlag&amp;gt; cflags, final int bufferSize,
    final short replication, final long blockSize, final Progressable progress,
    final ChecksumOpt checksumOpt)
{
    // 1. 
    DFSOutputStream dfsos = DfsClient.create(getPathName(p), permission,
                cflags, replication, blockSize, progress, bufferSize,
                checksumOpt)
    {
        // 1. 
        DFSOutputStream.newStreamForCreate(this,
        	src, masked, flag, createParent, replication, blockSize, progress,
        	buffersize, dfsClientConf.createChecksum(checksumOpt),
        	favoredNodeStrs);
        {
            // 1.
            stat = dfsClient.namenode.create(src, masked, dfsClient.clientName,
                new EnumSetWritable&amp;lt;CreateFlag&amp;gt;(flag), createParent, replication,
                blockSize, SUPPORTED_CRYPTO_VERSIONS);
            {
                // RPC
            }
            // 2.
            final DFSOutputStream out = new DFSOutputStream(dfsClient, src, stat,
            flag, progress, checksum, favoredNodes);
            {
                // 
                class DataStreamer
                class Patket
                streamer = new DataStreamer(stat, traceSpan);
            }
            
        }
        // 2.
        beginFileLease(result.getFileId(), result);
    }
                
    // 2. 
    return dfs.createWrappedOutputStream(dfsos, statistics);
    {
        return new HdfsDataOutputStream(dfsos, statistics, startPos);
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;首先是 DistributedFileSystem 创建，然后调用 DfsClient 的 create ，DfsClient需要创建流和lease，创建流 由 DFSOutputStream 完成，
DFSOutputStream 需要分别和namenode、datanode通信。DFSOutputStream 内部有 Packet 和 DataStreamer，继承自 FSOutputSummer ，FSOutputSummer 完成了write的真正逻辑&lt;/p&gt;

&lt;h1 id=&#34;2-out-write-buf-0-bytesread&#34;&gt;2. out.write(buf, 0, bytesRead)&lt;/h1&gt;

&lt;p&gt;创建完成后就是写数据，HdfsDataOutputStream 写数据比较复杂，先写到缓存，然后发送。需要做 checksum 检验，然后做成一个 chuck，然后将多个 chuck 合成一个 Packet，然后发送 Packet。&lt;/p&gt;

&lt;p&gt;FSDataOutputStream.out.write(byte[])
调用过程：
out.write(bytes) -&amp;gt; FilterOutputStream.write -&amp;gt; DataOutputStream.write -&amp;gt; out.write(byte[], off, len) -&amp;gt; FSOutputSummer.write(byte b[], int off, int len)&lt;/p&gt;

&lt;p&gt;FSOutputSummer.write&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public synchronized void write(byte b[], int off, int len)
      throws IOException {
    
    checkClosed();
    
    if (off &amp;lt; 0 || len &amp;lt; 0 || off &amp;gt; b.length - len) {
      throw new ArrayIndexOutOfBoundsException();
    }
    // 循环调用 write ，每次写入 #write1() 长度
    for (int n=0;n&amp;lt;len;n+=write1(b, off+n, len-n)) {
    }
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个 byte b[] 就是我们的数据流，通过断点我们可以看到是600个97，也就是600个a。&lt;/p&gt;

&lt;p&gt;write1，这里有几个比较核心的内容，如果写入长度比较大，直接写入流，如果写入比较少，先写到 Buffer，到达一定长度再统一进行写到流。这么做是为了减少拷贝&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;private int write1(byte b[], int off, int len) throws IOException {
  
  // 写入长度大于本地buf的长度时，直接写入本地buf的长度。
  if(count==0 &amp;amp;&amp;amp; len&amp;gt;=buf.length) {
    // local buffer is empty and user buffer size &amp;gt;= local buffer size, so
    // simply checksum the user buffer and send it directly to the underlying
    // stream
    final int length = buf.length;
    writeChecksumChunks(b, off, length);
    return length;
  }
  // 当len小于本地buf的长度时，先写入buf，当buf写满之后，flushBuffer
  // copy user data to local buffer
  
  int bytesToCopy = buf.length-count; // 这个 count 代表以及复制的数据长度，第一次是 0
  bytesToCopy = (len&amp;lt;bytesToCopy) ? len : bytesToCopy;  // 这时候就是要复制的数据长度，600
  System.arraycopy(b, off, buf, count, bytesToCopy);
  count += bytesToCopy;
  if (count == buf.length) {
    // local buffer is full
    flushBuffer();
  } 
  return bytesToCopy;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，写入数据大的话，直接调用 writeChecksumChunks 将buf长度大小的数据生成 chunksum ，
（chunksum 是检查数据完整性的，相关知识可以查看计算机网络。）并写入 packet中。如果写入数据比较少，直接放进 buffer，等待buffer比较大，再统一flush&lt;/p&gt;

&lt;p&gt;数据写完了关闭流的时候会再调用一次 fulshBuffer ,会调用 writeChecksumChunks&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;private void writeChecksumChunks(byte b[], int off, int len)
throws IOException {
  // 计算checksum
  sum.calculateChunkedSums(b, off, len, checksum, 0);
  for (int i = 0; i &amp;lt; len; i += sum.getBytesPerChecksum()) {
    int chunkLen = Math.min(sum.getBytesPerChecksum(), len - i);
    int ckOffset = i / sum.getBytesPerChecksum() * getChecksumSize();
    // 一个chunk一个chunk的写入packet
    writeChunk(b, off + i, chunkLen, checksum, ckOffset, getChecksumSize());
  }

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;sum.calculateChunkedSums 计算校验值，计算完以后b还是 600个97，off和len分别是 0和600，checksum是一个36位的数组，
但是只有前八位有值：0 = 111,1 = 50,2 = -90,3 = 31,4 = -99,5 = 97,6 = -69,7 = 102。因为每512位生成4个校验码。现在是600位，需要8个。
然后写出 chunk，chunk的长度为： 512，所以这里会调用两次 writeChunk。&lt;/p&gt;

&lt;p&gt;writeChunk 先将 chunk 写入 currentPacket 中，当currentPacket写满之后调用 waitAndQueueCurrentPacket，
将packet放入dataQueue队列，等待DataStreamer线程将packet写入pipeline中，整个block发送完毕之后将发送一个空的packet。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;writeChunk{}
currentPacket.writeChecksum(checksum, ckoff, cklen);
currentPacket.writeData(b, offset, len);
currentPacket.numChunks++;
bytesCurBlock += len;
waitAndQueueCurrentPacket()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们把这段代码跑两遍以后，去看看生成的Packet，里面有4个属性：checksumStart = 33,checksumPos = 41,dataStart = 541,dataPos = 1141&lt;/p&gt;

&lt;p&gt;可以看出这几个的意思分别是 check 的开始位置和结束为止，data的开始位置和结束为止，之所以留了 33个位置，是 Packet 的 header。然后还有一个 buffer数组，里面就是按照检查数据和数据。&lt;/p&gt;

&lt;p&gt;这时候 Packet 的结构我们已经一清二楚了。&lt;/p&gt;

&lt;p&gt;相关详细内容可以继续深入查看源码。waitAndQueueCurrentPacket() 可以看具体代码，这时候已经报连接超时异常了。接下来我们调试发送数据到 DataNode。&lt;/p&gt;

&lt;h1 id=&#34;三-dfsoutputstream-datastreamer-发送-packet&#34;&gt;三.DFSOutputStream.DataStreamer 发送 packet&lt;/h1&gt;

&lt;p&gt;上面已经调试到：waitAndQueueCurrentPacket() ,会将 Packet 发送给 DFSOutputStream.DataStreamer&lt;/p&gt;

&lt;p&gt;DFSOutputStream.DataStreamer 是一个线程，里面有个 stage 标识到了哪一步。还有一个 response 用来回复消息。&lt;/p&gt;

&lt;p&gt;DataStreamer 会从 dataQueue 中拿出 packet 发送到 pipeline , 相关代码很长。取出部分：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;synchronized (dataQueue) {

	 // 发送packet，dataQueue为null，则发送一个心跳
	 if (dataQueue.isEmpty()) {
	   one = createHeartbeatPacket();
	 } else {
	   one = dataQueue.getFirst(); // regular data packet
	 }
	}
	
        // 建立pipeline
        setPipeline(nextBlockOutputStream());
        // 启动ResponseProcessor线程，更新DataStreamer的状态为DATA_STREAMING

// 当前packet是block的最后一个packet，等待接收之前所有packet的ack
      if (one.lastPacketInBlock) {
        // wait for all data packets have been successfully acked
        synchronized (dataQueue) {
          while (!streamerClosed &amp;amp;&amp;amp; !hasError &amp;amp;&amp;amp; 
              ackQueue.size() != 0 &amp;amp;&amp;amp; dfsClient.clientRunning) {
            try {
              // wait for acks to arrive from datanodes
              dataQueue.wait(1000);
            } catch (InterruptedException  e) {
              DFSClient.LOG.warn(&amp;quot;Caught exception &amp;quot;, e);
            }
          }
        }
        if (streamerClosed || hasError || !dfsClient.clientRunning) {
          continue;
        }
        stage = BlockConstructionStage.PIPELINE_CLOSE;
      }
       
     
     // 将 packet 从 dataQueue 移到 ackQueue，准备发送packet
      synchronized (dataQueue) {
        // move packet from dataQueue to ackQueue
        if (!one.isHeartbeatPacket()) {
          dataQueue.removeFirst();
          ackQueue.addLast(one);
          dataQueue.notifyAll();
        }
      }
     // 将packet写入pipeline
        one.writeTo(blockStream);
        blockStream.flush();   
    
      // 如果当前packet是最后一个，则继续等待此packet的ack，
      // 然后endBlock
    if (one.lastPacketInBlock) {
        // wait for the close packet has been acked
        synchronized (dataQueue) {
          while (!streamerClosed &amp;amp;&amp;amp; !hasError &amp;amp;&amp;amp; 
              ackQueue.size() != 0 &amp;amp;&amp;amp; dfsClient.clientRunning) {
            dataQueue.wait(1000);// wait for acks to arrive from datanodes
          }
        }
        if (streamerClosed || hasError || !dfsClient.clientRunning) {
          continue;
        }
        endBlock();
      }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;代码很长也比较杂乱，我们主要看一下 setPipeline(nextBlockOutputStream()); 和 one.writeTo(blockStream);blockStream.flush();  等&lt;/p&gt;

&lt;p&gt;第一步是：one = dataQueue.getFirst();&lt;br /&gt;
我们看看取到的 Packet 是什么样子：和我们上面发送的一样，buf=[0,0,0(33个0),11,50..(6个检测码),0,0,(很多0)，97,97,97(600个97)，],lastPacketInBlock=false &amp;hellip;&lt;/p&gt;

&lt;p&gt;nextBlockOutputStream 是通过 namenode 得到一个 LocatedBlock ，而 setPipeline 是和 Datanode 的通道。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;nextBlockOutputStream(){
lb = locateFollowingBlock(startTime,excluded.length &amp;gt; 0 ? excluded : null); 
-- dfsClient.namenode.addBlock(src, dfsClient.clientName,block, excludedNodes, fileId, favoredNodes);

success = createBlockOutputStream(nodes, storageTypes, 0L, false);

blockStream = out; //给写出流赋值。
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;createBlockOutputStream 对于我们第一次调试来说，关注三个就够了。
创建连接： /到 datanode 的 socket连接 Socket[addr=/127.0.0.1,port=50010,localport=58006]，
new Sender(out).writeBlock 建立 block&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;createBlockOutputStream{}
// 到 datanode 的 socket连接 Socket[addr=/127.0.0.1,port=50010,localport=58006]
s = createSocketForPipeline(nodes[0], nodes.length, dfsClient);
long writeTimeout = dfsClient.getDatanodeWriteTimeout(nodes.length);

OutputStream unbufOut = NetUtils.getOutputStream(s, writeTimeout);
InputStream unbufIn = NetUtils.getInputStream(s);
IOStreamPair saslStreams = dfsClient.saslClient.socketSend(s,
  unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);
unbufOut = saslStreams.out;
unbufIn = saslStreams.in;
out = new DataOutputStream(new BufferedOutputStream(unbufOut,
    HdfsConstants.SMALL_BUFFER_SIZE));
    
-- 发送写block请求，
new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,
dfsClient.clientName, nodes, nodeStorageTypes, null, bcs, 
nodes.length, block.getNumBytes(), bytesSent, newGS,
checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile);

blockStream = out; //给写出流赋值。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;回到 run 方法，通过 initDataStreaming ，然后调用 one.writeTo(blockStream); 这个 blockStream 上面已经赋值了，就是通过 socket 建立 的连接。这里只是将packet写入pipeline中的第一个dn。&lt;/p&gt;

&lt;p&gt;看看 Packet 的 writeTo 方法，Packet 的成员变量上面写了，是检查点位置，检查长度，数据位置，数据长度，等。&lt;/p&gt;

&lt;p&gt;主要步骤：
- 计算 pktLen 头长度+检查长度+数据长度
- 新建Header，包括 packet 是否是最后一个packet的信息等。
- 判断 checksumPos != dataStart 不等于需要删掉中间的空缺数据，&lt;code&gt;System.arraycopy(buf, checksumStart, buf, dataStart - checksumLen , checksumLen)&lt;/code&gt;
这个是因为 packet 容量是好几万，我们假设 10000，能容纳的 chunk 是 20个左右，所以应该有 20 * 4 = 80 的检查数据。所以前80个位置留给了检查位置，数据从81开始写。
但是如果我们没写满一个 Packet，检查数据就不需要80个，数据也是从 81开始写，这就空缺了一些数据。
- 将 header.getBytes() [0 = 0,1 = 0,2 = 2,3 = 100,4 = 0,5 = 25 &amp;hellip;.(一共33个值)] 的值放到 Packet 的头部。
- stm.write(buf, headerStart, header.getSerializedSize() + checksumLen + dataLen);&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;void writeTo(DataOutputStream stm) throws IOException {
  
  final int dataLen = dataPos - dataStart;  1411 - 541
  final int checksumLen = checksumPos - checksumStart; 
  final int pktLen = HdfsConstants.BYTES_IN_INTEGER + dataLen + checksumLen;
  PacketHeader header = new PacketHeader(
    pktLen, offsetInBlock, seqno, lastPacketInBlock, dataLen, syncBlock);
    
  // checksumPos不等于dataStart时，将checksum移动到data前面，
  // 紧挨着data，为header空出足够的空间
  if (checksumPos != dataStart) {
    // Move the checksum to cover the gap. This can happen for the last
    // packet or during an hflush/hsync call.
    System.arraycopy(buf, checksumStart, buf, 
                     dataStart - checksumLen , checksumLen); 
    checksumPos = dataStart;
    checksumStart = checksumPos - checksumLen;
  }
  
  final int headerStart = checksumStart - header.getSerializedSize();
  assert checksumStart + 1 &amp;gt;= header.getSerializedSize();
  assert checksumPos == dataStart;
  assert headerStart &amp;gt;= 0;
  assert headerStart + header.getSerializedSize() == checksumStart;
  
  // Copy the header data into the buffer immediately preceding the checksum
  // data.
  
  // 将header复制到packet的buf中，组成一个完整的packet
  System.arraycopy(header.getBytes(), 0, buf, headerStart,
      header.getSerializedSize());
  
  // corrupt the data for testing.
  if (DFSClientFaultInjector.get().corruptPacket()) {
    buf[headerStart+header.getSerializedSize() + checksumLen + dataLen-1] ^= 0xff;
  }
  // Write the now contiguous full packet to the output stream.
  // 将buf写入输出流中
  stm.write(buf, headerStart, header.getSerializedSize() + checksumLen + dataLen);
  
  // undo corruption.
  if (DFSClientFaultInjector.get().uncorruptPacket()) {
    buf[headerStart+header.getSerializedSize() + checksumLen + dataLen-1] ^= 0xff;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以看一下 最后的 stm.write， 通过跟踪我们发现最终调用： channel.write(buf);  是通过 NIO实现的。&lt;/p&gt;

&lt;p&gt;最后我们可以看到打印了错误信息： Exception in thread &amp;ldquo;main&amp;rdquo; java.io.IOException: All datanodes 127.0.0.1:50010 are bad. Aborting&amp;hellip;
这是因为我们调试时间长，连接已经断了。这个错误是哪一步打印的我们后续可以研究研究。&lt;/p&gt;

&lt;p&gt;其实这里有个疑问，这个 Packet 是最后一个 Packet，但是 它的 lastPacketInBlock=false？，其实这个确实不是最后一个，后面还要发送一个空的 Packet，只有 37个字节的头信息。&lt;/p&gt;

&lt;h1 id=&#34;四-dataxceiver-线程写入-datanode&#34;&gt;四、DataXceiver 线程写入 DataNode&lt;/h1&gt;

&lt;p&gt;注意：&lt;/p&gt;

&lt;p&gt;服务端的调试和客户端调试是反过来的，
需要先 stop-dfs.sh, 然后 &lt;code&gt;hadoop-daemons.sh start namenode&lt;/code&gt; &lt;code&gt;hadoop-daemons.sh start secondarynamenode&lt;/code&gt;  启动 namenode，然后我们在 IDEA 中运行 datanode
然后我们通过客户端命令: hadoop fs -copyFromLocal data.txt /tmp/data.txt
另外多线程调试不太方便，每次只能在一个线程打赏断点，否则会跳来跳去很麻烦，我们就在 DataXCerver 的 writeBlock 方法打上断点&lt;/p&gt;

&lt;p&gt;以上的流程可以看做是client端，client端将数据发送到dn上，由dn负责将packet写入本地磁盘，并向下一个dn发送。
DataXceiverServer 是在 DataNode 启动的线程，run 方法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public void run() {
  Peer peer = null;
  while (datanode.shouldRun &amp;amp;&amp;amp; !datanode.shutdownForUpgrade) {
    try {
      // 接收client的socket请求
      peer = peerServer.accept();
     
     // 一个 Daemon线程
      new Daemon(datanode.threadGroup,
          DataXceiver.create(peer, datanode, this))
          .start();
    } catch 
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里先循环接受请求，每次接受到一个就新建一个线程，加入线程组中。每个 DataXceiver 线程的 run 方法就是处理请求的：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;dataXceiverServer.addPeer(peer, Thread.currentThread(), this);
peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);
InputStream input = socketIn;
try {
  IOStreamPair saslStreams = datanode.saslServer.receive(peer, socketOut,
    socketIn, datanode.getXferAddress().getPort(),
    datanode.getDatanodeId());
  input = new BufferedInputStream(saslStreams.in,
    HdfsConstants.SMALL_BUFFER_SIZE);
  socketOut = saslStreams.out;
} catch 

super.initialize(new DataInputStream(input));

op = readOp();
processOp(op);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;processOp():&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;switch(op) {
    case READ_BLOCK:
      opReadBlock();
      break;
    case WRITE_BLOCK:
      opWriteBlock(in);
      break;

    default:
      throw new IOException(&amp;quot;Unknown op &amp;quot; + op + &amp;quot; in data stream&amp;quot;);
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;private void opWriteBlock(DataInputStream in) throws IOException {
    final OpWriteBlockProto proto = OpWriteBlockProto.parseFrom(vintPrefixed(in));
    final DatanodeInfo[] targets = PBHelper.convert(proto.getTargetsList());
    TraceScope traceScope = continueTraceSpan(proto.getHeader(),
        proto.getClass().getSimpleName());
    try {
      writeBlock(PBHelper.convert(proto.getHeader().getBaseHeader().getBlock()),
          PBHelper.convertStorageType(proto.getStorageType()),
          PBHelper.convert(proto.getHeader().getBaseHeader().getToken()),
          proto.getHeader().getClientName(),
          targets,
          PBHelper.convertStorageTypes(proto.getTargetStorageTypesList(), targets.length),
          PBHelper.convert(proto.getSource()),
          fromProto(proto.getStage()),
          proto.getPipelineSize(),
          proto.getMinBytesRcvd(), proto.getMaxBytesRcvd(),
          proto.getLatestGenerationStamp(),
          fromProto(proto.getRequestedChecksum()),
          (proto.hasCachingStrategy() ?
              getCachingStrategy(proto.getCachingStrategy()) :
            CachingStrategy.newDefaultStrategy()),
            (proto.hasAllowLazyPersist() ? proto.getAllowLazyPersist() : false));
     } finally {
      if (traceScope != null) traceScope.close();
     }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个方法有我们的断点，我们会在里面进行调试 writeBlock ，大致步骤为：&lt;/p&gt;

&lt;p&gt;新建 给客户端发确认消息的 replyOut 流，
新建 给其他DataNode 发消息的 mirror 流
新建 BlockReceiver  接收数据，接收到 Packet 后进行判断，如果不是最后一个，写到输出流，如果是最后一个或者长度为0，不做处理。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;
public void writeBlock(final ExtendedBlock block,
    final StorageType storageType, 
    final Token&amp;lt;BlockTokenIdentifier&amp;gt; blockToken,
    final String clientname,
    final DatanodeInfo[] targets,
    final StorageType[] targetStorageTypes, 
    final DatanodeInfo srcDataNode,
    final BlockConstructionStage stage,
    final int pipelineSize,
    final long minBytesRcvd,
    final long maxBytesRcvd,
    final long latestGenerationStamp,
    DataChecksum requestedChecksum,
    CachingStrategy cachingStrategy,
    final boolean allowLazyPersist) throws IOException {
    
    // 我们简单取一部分代码
    
    // 输出流，现在在 DataXCerver 中，输入流就是客户端写，输出流自然即时发送到客户端的
    final DataOutputStream replyOut = new DataOutputStream(
        new BufferedOutputStream(
            getOutputStream(),
            HdfsConstants.SMALL_BUFFER_SIZE));
    
    }
    
    // 这里一大堆的 mirrorOut 开头的流都是 数据写到 DataNode 后复制副本用的，由于在本地只有一个副本，所以都是 null 
    DataOutputStream mirrorOut = null;  // stream to next target
    DataInputStream mirrorIn = null;    // reply from next target
    Socket mirrorSock = null;           // socket to next target
    String mirrorNode = null;           // the name:port of next target
    String firstBadLink = &amp;quot;&amp;quot;;           // first datanode that failed in connection setup
    Status mirrorInStatus = SUCCESS;
    final String storageUuid;
    
    // 新建 BlockReceiver， BlockReceiver 的作用可以好好看注释，另外它的构造方法也有很大的信息量，其中有个 in 输入流，
    // 是在 DataXCerver 的 run 方法中：super.initialize(new DataInputStream(input)); 也就是对输入的 socket 流的二层包装
    blockReceiver = new BlockReceiver(block, storageType, in,
    peer.getRemoteAddressString(),
    peer.getLocalAddressString(),
    stage, latestGenerationStamp, minBytesRcvd, maxBytesRcvd,
    clientname, srcDataNode, datanode, requestedChecksum,
    cachingStrategy, allowLazyPersist)
    
    // 然后是一大堆的副本拷贝，我们本地调试线跳过。
    if (targets.length &amp;gt; 0) {// 跳过}
    
      // 给客户端发一个应答消息
      if (isClient &amp;amp;&amp;amp; !isTransfer) {
        if (LOG.isDebugEnabled() || mirrorInStatus != SUCCESS) {
          LOG.info(&amp;quot;Datanode &amp;quot; + targets.length +
                   &amp;quot; forwarding connect ack to upstream firstbadlink is &amp;quot; +
                   firstBadLink);
        }
        BlockOpResponseProto.newBuilder()
          .setStatus(mirrorInStatus)
          .setFirstBadLink(firstBadLink)
          .build()
          .writeDelimitedTo(replyOut);
        replyOut.flush();
      }
      
      // 这里开始写数据，
    blockReceiver.receiveBlock(mirrorOut, mirrorIn, replyOut,mirrorAddr, null, targets, false);
    
    // 后续处理
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们看一下 blockReceiver.receiveBlock&lt;/p&gt;

&lt;p&gt;new PacketResponder
receivePacket()&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;// 如果是来自客户端而且传输，新建回复的线程
if (isClient &amp;amp;&amp;amp; !isTransfer) {
        responder = new Daemon(datanode.threadGroup, 
            new PacketResponder(replyOut, mirrIn, downstreams));
        responder.start(); // start thread to processes responses
      }

// 循环写
while (receivePacket() &amp;gt;= 0) { /* R} 这里就是写所在逻辑了

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;receivePacket 方法：&lt;/p&gt;

&lt;p&gt;packetReceiver.receiveNextPacket(in); 接受一个 Packet&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;receivePacket{
packetReceiver.receiveNextPacket(in);

PacketHeader header = packetReceiver.getHeader();

// 这里把数据写到 副本上面
//First write the packet to the mirror:
    if (mirrorOut != null &amp;amp;&amp;amp; !mirrorError) {
      try {
        long begin = Time.monotonicNow();
        packetReceiver.mirrorPacketTo(mirrorOut);
        mirrorOut.flush();
        long duration = Time.monotonicNow() - begin;
        if (duration &amp;gt; datanodeSlowLogThresholdMs) {
          LOG.warn(&amp;quot;Slow BlockReceiver write packet to mirror took &amp;quot; + duration
              + &amp;quot;ms (threshold=&amp;quot; + datanodeSlowLogThresholdMs + &amp;quot;ms)&amp;quot;);
        }
      } catch (IOException e) {
        handleMirrorOutError(e);
      }
    }
    
    // 得到 数据和 检查数据
    ByteBuffer dataBuf = packetReceiver.getDataSlice();
    ByteBuffer checksumBuf = packetReceiver.getChecksumSlice();
    
    // 如果是最后一个 packet
    if (lastPacketInBlock || len == 0) {
      if(LOG.isDebugEnabled()) {
        LOG.debug(&amp;quot;Receiving an empty packet or the end of the block &amp;quot; + block);
      }
      // sync block if requested
      if (syncBlock) {
        flushOrSync(true);
      }
    } else {
        。。。。。
        out.write(dataBuf.array(), startByteToDisk, numBytesToDisk);
    }
    // 这里就行 check 很复杂，部分数据的 check 需要重新计算。
    checksumOut.write(buf);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里的代码太复杂，而且调试的时候稍微时间长一点就出现 连接异常。可以通过配置参数解决。
然后得到了 dataBuf 和 checksumBuf，如果不是最后一个，我们需要写到 DataNode 的具体数据中。
到这里整个写的操作就完成。中间的 和客户端心跳应答、失败处理。都没有涉及。简单梳理一下：&lt;/p&gt;

&lt;h1 id=&#34;五-pipelineack-和-packetresponder-信息处理&#34;&gt;五、PipeLineAck 和 PacketResponder 信息处理&lt;/h1&gt;

&lt;p&gt;上面的 one.writeTo(blockStream); 代码是将 Packet 写入到输出流，写出之前有一段代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt; // send the packet
 synchronized (dataQueue) {
   // move packet from dataQueue to ackQueue
   if (!one.isHeartbeatPacket()) {
     dataQueue.removeFirst();
     ackQueue.addLast(one);
     dataQueue.notifyAll();
   }
 }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就是将 Packet 放进 ackQueue 中，很明显这又是一个 消费者生产者。&lt;/p&gt;

&lt;p&gt;新建 DataStreamer 的run中，得到输出流，
建立 pipeLine 之后，会有 initDataStreaming 操作，还要启一个新的线程 ResponseProcessor 接收packet的ack，这个线程在initDataStreaming中启动，并更新DataStreamer线程的状态为DATA_STREAMING
ResponseProcessor，在 ResponseProcessor 的run方法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;synchronized (dataQueue) {
  one = ackQueue.getFirst();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意每次锁住的都是 dataQueue 对象。
与他对应的是 BlockReceiver 中的 PacketResponder 。 PacketResponder 的 run 方法比较长，主要是按照 packet 的写入顺序发送 ack。&lt;/p&gt;

&lt;p&gt;由于调试器的问题，这里一直跳不进去，能看到大概逻辑： 当数据节点顺利处理完数据，而且当前节点处在数据节点中间（收到下游DataNode的消息）
如果 ackQueue 中有数据，获取一个记录，接下来如果当前节点位于数据管道的中间，就在mirror流读取下游的确认，我们在本地调试是没有这一步的。&lt;/p&gt;

&lt;p&gt;如果当前节点位于数据管道的最后， 调用 pkt = waitForAckHead(seqno) 从 ackQueue 取出对应的 pkt ，然后调用 lastPacketInBlock = pkt.lastPacketInBlock;
然后是  if (lastPacketInBlock) {finalizeBlock(startTime);} ，然后是 sendAckUpstream ，这个就是给客户端发送 ack&lt;/p&gt;

&lt;p&gt;然后 ResponseProcessor 中收到了 ack，进行处理。逻辑比较简单，ack.readFields(blockReplyStream) 读取 ask，从输入流中读取的ack的seqno与ackQueue中取得的seqno不一样则抛出异常&lt;/p&gt;

&lt;p&gt;// 接收到ack后，从ackQueue中移除packet
      synchronized (dataQueue) {
        lastAckedSeqno = seqno;
        ackQueue.removeFirst();
        dataQueue.notifyAll();
        one.releaseBuffer(byteArrayManager);
      }
。&lt;/p&gt;

&lt;h1 id=&#34;6-namenode-处理&#34;&gt;6.NameNode 处理&lt;/h1&gt;

&lt;p&gt;上面我们说了 NameNode 的处理我们没法调试所以我们跳过了NameNode，原因是客户端通过远程的RPC调用执行，这次我们在IDEA启动NameNode，然后查看NameNode是如何处理写文件的。&lt;/p&gt;

&lt;p&gt;通过 RPC 调用 NameNodeRpcServer.create -&amp;gt; namesystem.startFile -&amp;gt; startFileInt -&amp;gt; startFileInternal&lt;/p&gt;

&lt;p&gt;中间有上锁的步骤，还有 BlockManager 的验证，namesystem 中有一个 dir:FSDirectory 的属性，a pure in-memory data structure ，
里面又有一个 rootDir：INodeDirectory 代表根节点，rootDir 中有一个 List&lt;INode&gt; children 代表所以的文件 ，还有一个 INodeMap 对象，保存。
注意还有一个 INodesInPath 类，从他的方法我们看出，它代表一个文件递归得到所有父目录的 InpPath 文件。&lt;/p&gt;

&lt;p&gt;startFileInternal 方法主要步骤如下：
- 根据 src 得到 INodesInPath，再得到 INodeFile 进行检验。
- newNode = dir.addFile(src, permissions, replication, blockSize,holder, clientMachine);
- getEditLog().logOpenFile(src, newNode, overwrite, logRetryEntry); 记录日志
- leaseManager.addLease(newNode.getFileUnderConstructionFeature()&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt; private BlocksMapUpdateInfo startFileInternal(FSPermissionChecker pc, 
      String src, PermissionStatus permissions, String holder, 
      String clientMachine, boolean create, boolean overwrite, 
      boolean createParent, short replication, long blockSize, 
      boolean isLazyPersist, CipherSuite suite, CryptoProtocolVersion version,
      EncryptedKeyVersion edek, boolean logRetryEntry)

//得到文件
final INodeFile myFile = INodeFile.valueOf(inode, src, true);

try {
      BlocksMapUpdateInfo toRemoveBlocks = null;
      if (myFile == null) {
      // 判断是否创建
        if (!create) {
          throw new FileNotFoundException(&amp;quot;Can&#39;t overwrite non-existent &amp;quot; +
              src + &amp;quot; for client &amp;quot; + clientMachine);
        }
      } else {
      // 是否覆盖
        if (overwrite) {
          toRemoveBlocks = new BlocksMapUpdateInfo();
          List&amp;lt;INode&amp;gt; toRemoveINodes = new ChunkedArrayList&amp;lt;INode&amp;gt;();
          long ret = dir.delete(src, toRemoveBlocks, toRemoveINodes, now());
          if (ret &amp;gt;= 0) {
            incrDeletedFileCount(ret);
            removePathAndBlocks(src, null, toRemoveINodes, true);
          }
        } else {
        // 存在而且不覆盖的情况？操作 lease
          // If lease soft limit time is expired, recover the lease
          recoverLeaseInternal(myFile, src, holder, clientMachine, false);
          throw new FileAlreadyExistsException(src + &amp;quot; for client &amp;quot; +
              clientMachine + &amp;quot; already exists&amp;quot;);
        }
      }

// Always do an implicit mkdirs for parent directory tree.
// 父目录
      Path parent = new Path(src).getParent();
      if (parent != null &amp;amp;&amp;amp; mkdirsRecursively(parent.toString(),
              permissions, true, now())) {
              // 添加到 namespace
        newNode = dir.addFile(src, permissions, replication, blockSize,
                              holder, clientMachine);
      }
      // 操作 lease
      leaseManager.addLease(newNode.getFileUnderConstructionFeature()
          .getClientName(), src);

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;INodesInPath：&lt;/p&gt;

&lt;p&gt;newNode = dir.addFile(src, permissions, replication, blockSize,holder, clientMachine);&lt;/p&gt;

&lt;p&gt;这个 dir 是 FSDirectory 类型的变量，最终调用了 INodeDirectory.addChild(INode) 方法，时间上就是给他的 children add 一个 INode&lt;/p&gt;

&lt;p&gt;和 addlease :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;synchronized Lease addLease(String holder, String src) {
    Lease lease = getLease(holder);
    if (lease == null) {
      lease = new Lease(holder);
      leases.put(holder, lease);
      sortedLeases.add(lease);
    } else {
      renewLease(lease);
    }
    sortedLeasesByPath.put(src, lease);
    lease.paths.add(src);
    return lease;
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;  /** Get a lease and start automatic renewal */
  private void beginFileLease(final long inodeId, final DFSOutputStream out)
      throws IOException {
    getLeaseRenewer().put(inodeId, out, this);
  }
  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;调用完 startFileInternal 后，会调用
stat = dir.getFileInfo(src, false, FSDirectory.isReservedRawName(srcArg), true);
实际上就是得到一个 HdfsFileStatus 的对象，返回给客户端。NameNode 穿件文件元数据信息的过程大致如此。&lt;/p&gt;

&lt;h1 id=&#34;七-datanode-创建文件&#34;&gt;七、DataNode 创建文件&lt;/h1&gt;

&lt;p&gt;上面讲了在 DataNode 的 DataXCeiver 的写数据过程，但是我们忽略了一些和流式接口无关的部分，包括创建数据块等。&lt;/p&gt;

&lt;p&gt;实际上客户端 DFSClient 发送创建元数据给 NameNode 以后，就要根据 HdfsFileStatus 创建到 DataNode 的输出流。
在 DataStreamer 的 createBlockOutputStream 方法中创建了 blockStream = out，实际上创建之前有个步骤：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;// send the request
new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,
    dfsClient.clientName, nodes, nodeStorageTypes, null, bcs, 
    nodes.length, block.getNumBytes(), bytesSent, newGS,
    checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个 Sender 和 DataXCerver 一样，继承自 DataTransferProtocol ，我们可以理解为 DataTransferProtocol 是客户端和 DataNode 通信协议，通信实现包含了创建 数据块和发送数据，
而 Sender 和 DataXCerver 就分别是创建数据块和发送数据的实现，分别是 TCP 和 RPC 通信方式实现。&lt;/p&gt;

&lt;p&gt;new Sender(out).writeBlock 只是简单的将信息以 ProtoBuf 的格式发送出去。实际上就是发送了一个 op ，这个 op 服务端会收到并解析，然后根据 op 的值调用不同方法，最后调用了：&lt;/p&gt;

&lt;p&gt;前面分析过： BlockReceiver.receiveBlock 。里面就有构建输出流写出，但是我们没有相关这个输出流是怎么来的，实际上就是一个文件流。我们看一下 BlockReceiver 的构造方法。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;this.block = block;

switch (stage) {
        case PIPELINE_SETUP_CREATE:
          replicaInfo = datanode.data.createRbw(storageType, block, allowLazyPersist);
          datanode.notifyNamenodeReceivingBlock(
              block, replicaInfo.getStorageUuid());
          break;


streams = replicaInfo.createStreams(isCreate, requestedChecksum);
this.out = streams.getDataOut();
this.checksumOut = new DataOutputStream(new BufferedOutputStream(
          streams.getChecksumOut(), HdfsConstants.SMALL_BUFFER_SIZE));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里有个 switch (stage)，实际上写文件都是一个 PIPELINE ，包括输入某个 client - DataNode - DataNode - DataNode ，由于我是本地模式调试，所以感觉不到这种 PIPELINE 的模式而已。&lt;/p&gt;

&lt;p&gt;datanode.data 是一个 FSDataSetImpl 类型的变量，控制着整个 DataNode 的文件信息。类型结构大致如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FSDataSetImpl 
    DataStorage
        bpStorageMap&amp;lt;bp,BlockPoolSliceStorage&amp;gt;
    FSVolumnList
        List&amp;lt;FSVolumnImpl&amp;gt;
             bpSlices&amp;lt;bp, BlockPoolSlice&amp;gt;
    volumeMap:ReplicaMap&amp;lt;bp, Map&amp;lt;blockid, ReplicaInfo&amp;gt;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;FSDataSetImpl 代表整个 DataNode 的文件存储，FSVolumnList 是因为我们配置的 data 存储路径，可以用逗号隔开，一般情况下配置1个路径，FSVolumnList 里面就一个 FsVolumeImpl。
FsVolumeImpl 里面有分为 多个 blockpool 存储，一个 blockpool 对应一个文件夹而已，这在一开始版本是没有的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Storage

  List&amp;lt;StorageDirectory&amp;gt; storageDirs
  public int   layoutVersion;   // layout version of the storage data
  public int   namespaceID;     // id of the file system
  public String clusterID;      // id of the cluster
  public long  cTime;   
  static class StorageDirectory
      File root

DataStorage
   private String datanodeUuid = null;
   Map&amp;lt;String, BlockPoolSliceStorage&amp;gt; bpStorageMap
   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;FsVolumeImpl&lt;/p&gt;

&lt;p&gt;replicaInfo 保存了文件的位置信息，所以可以用来创建输出流。&lt;/p&gt;

&lt;h1 id=&#34;八-租约处理&#34;&gt;八、租约处理&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;synchronized void put(final long inodeId, final DFSOutputStream out,
      final DFSClient dfsc) {
    if (dfsc.isClientRunning()) {
      if (!isRunning() || isRenewerExpired()) {
        //start a new deamon with a new id.
        final int id = ++currentId;
        daemon = new Daemon(new Runnable() {
          @Override
          public void run() {
            try {
              if (LOG.isDebugEnabled()) {
                LOG.debug(&amp;quot;Lease renewer daemon for &amp;quot; + clientsString()
                    + &amp;quot; with renew id &amp;quot; + id + &amp;quot; started&amp;quot;);
              }
              LeaseRenewer.this.run(id);
            } catch(InterruptedException e) {
              if (LOG.isDebugEnabled()) {
                LOG.debug(LeaseRenewer.this.getClass().getSimpleName()
                    + &amp;quot; is interrupted.&amp;quot;, e);
              }
            } finally {
              synchronized(LeaseRenewer.this) {
                Factory.INSTANCE.remove(LeaseRenewer.this);
              }
              if (LOG.isDebugEnabled()) {
                LOG.debug(&amp;quot;Lease renewer daemon for &amp;quot; + clientsString()
                    + &amp;quot; with renew id &amp;quot; + id + &amp;quot; exited&amp;quot;);
              }
            }
          }
          
          @Override
          public String toString() {
            return String.valueOf(LeaseRenewer.this);
          }
        });
        daemon.start();
      }
      dfsc.putFileBeingWritten(inodeId, out);
      emptyTime = Long.MAX_VALUE;
    }
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最重要的就是  LeaseRenewer.this.run(id)， 在run中调用renew对租约续约。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;for(long lastRenewed = Time.now(); !Thread.interrupted();
        Thread.sleep(getSleepPeriod())) {
      final long elapsed = Time.now() - lastRenewed;
      if (elapsed &amp;gt;= getRenewalTime()) {
        try {
          renew();
          if (LOG.isDebugEnabled()) {
            LOG.debug(&amp;quot;Lease renewer daemon for &amp;quot; + clientsString()
                + &amp;quot; with renew id &amp;quot; + id + &amp;quot; executed&amp;quot;);
          }
          lastRenewed = Time.now();
        } catch (SocketTimeoutException ie) {
          LOG.warn(&amp;quot;Failed to renew lease for &amp;quot; + clientsString() + &amp;quot; for &amp;quot;
              + (elapsed/1000) + &amp;quot; seconds.  Aborting ...&amp;quot;, ie);
          synchronized (this) {
            while (!dfsclients.isEmpty()) {
              dfsclients.get(0).abort();
            }
          }
          break;
        } catch (IOException ie) {
          LOG.warn(&amp;quot;Failed to renew lease for &amp;quot; + clientsString() + &amp;quot; for &amp;quot;
              + (elapsed/1000) + &amp;quot; seconds.  Will retry shortly ...&amp;quot;, ie);
        }
      }

&lt;/code&gt;&lt;/pre&gt;
</description>
      
    </item>
    
    <item>
      <title>resourcemanager</title>
      <link>https://dengziming.github.io/post/hadoop/first/</link>
      <pubDate>Tue, 22 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/hadoop/first/</guid>
      
        <description>&lt;p&gt;参考资料：&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://hortonworks.com/blog/apache-hadoop-yarn-resourcemanager/&#34;&gt;https://hortonworks.com/blog/apache-hadoop-yarn-resourcemanager/&lt;/a&gt;&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>yarn-nodemanager-剖析</title>
      <link>https://dengziming.github.io/post/hadoop/yarn-nodemanager-1/</link>
      <pubDate>Tue, 22 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/hadoop/yarn-nodemanager-1/</guid>
      
        <description>

&lt;h1 id=&#34;架构&#34;&gt;架构&lt;/h1&gt;

&lt;p&gt;ContainerManagementImpl&lt;/p&gt;

&lt;h1 id=&#34;container-生命周期&#34;&gt;Container 生命周期&lt;/h1&gt;

&lt;p&gt;第一步是 RM 的 applicationMasterLauncher ，创建 ApplicationMasterLauncher 后，遇到 launch 时间 ，
case LAUNCH: launch(application); =&amp;gt; new AMLauncher(context, application, event, getConfig());&lt;/p&gt;

&lt;p&gt;这个任务放进 队列里面等待执行，一旦执行会调用 launch() 方法，然后调用 containerMgrProxy.startContainers(allRequests); 这是 RPC 调用&lt;/p&gt;

&lt;p&gt;实际上就是 ContainerManagementImpl ，然后会调用 startContainerInternal ，然后就是 new ContainerImpl 。&lt;/p&gt;

&lt;p&gt;这是 APPMaster 启动需要的 container ，实际上还有 APPMaster 调度任务需要更多的 Container ，继续向 ContainerManagementImpl 请求&lt;/p&gt;

&lt;h2 id=&#34;1-资源本地化&#34;&gt;1. 资源本地化&lt;/h2&gt;

&lt;p&gt;实际上就是 ContainerManagementImpl ，然后会调用 startContainerInternal ，然后就是 new ContainerImpl 。
然后通过 if (null == context.getApplications().putIfAbsent(applicationID,application)) 判断是否是该 NodeManager 第一个 Container ，如果是的话，new ApplicationImpl
向 ApplicationImpl 发送 ApplicationInitEvent 事件，同时发送 ApplicationContainerInitEvent 事件。&lt;/p&gt;

&lt;p&gt;这些事件会触发 ACL、log等相关的事件， 收到 ApplicationContainerInitEvent 后将 Container 加入 ApplicationImpl 的维护列表。&lt;/p&gt;

&lt;p&gt;logHandle 处理完成之后会发送一个 log 事件，applicationImpl 收到后向 ResourceLocalizeService 发送 事件，
为 private 和 application 级别的资源创建 LocalResourceTrackerImp ，为下载资源作准备。&lt;/p&gt;

&lt;p&gt;private 的资源用户可见，如果该用户已经提交过了，无需创建。同理，如果 application 已经启动过 container 了，则同一个 application 的新 container 不必在创建。&lt;/p&gt;

&lt;p&gt;经过上面操作后，ResourceLocalizeService 向 ApplicationImpl 发送 Application_Init&lt;/p&gt;

&lt;p&gt;ApplicationImpl 收到 INIT 后，向所有的 ContainerImpl 发送 InitContainer ，ApplicationImpl 也从 ApplicationState.INITING 变为 ApplicationState.RUNNING,&lt;/p&gt;

&lt;p&gt;InitContainer 命令后，和 AuxService 交互，然后从 ContainerLaunchContext 得到各类可见性资源并保存到相应数据结构，然后发送给 ResourceLocalizeService 。&lt;/p&gt;

&lt;p&gt;ResourceLocalizeService 调用 handleInitContainerResources((ContainerLocalizationRequestEvent) event); 实际是 是发送给 LocalResourcesTrackerImpl 。&lt;/p&gt;

&lt;p&gt;LocalResourcesTrackerImpl 会 判断是否需要下载等，为对应的资源创建 LocalizedResource 状态机，将 Request 发送给 LocalizedResource。&lt;/p&gt;

&lt;p&gt;后续还是这样的时间驱动，总之可以概括为 ： NodeManager 上同一个 App 所有的 ContainerImpl 异步并发向向资源下载服务 ResourceLocalizeService 发送待下载的资源，
ResourceLocalizeService下载完成后会通知依赖资源的所以 Container ，当一个 Container 依赖的资源全部下载完毕，Container 将会进入 运行阶段&lt;/p&gt;

&lt;h2 id=&#34;2-container-运行&#34;&gt;2. Container 运行&lt;/h2&gt;

&lt;p&gt;运行是 ContainerLauncher 服务实现的，主要过程为： 将待运行 Container 所需要的环境变量和运行命令写到 &lt;code&gt;launch_container.sh&lt;/code&gt; 中，
将启动该脚本的命令写入：&lt;code&gt;default_container_executor.sh&lt;/code&gt; 中。&lt;/p&gt;

&lt;p&gt;通过运行该脚本启动 Container 。主要有四步：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;ContainerImpl 向 ContainersLauncher 发送 Launch_container ，请求启动 container。
dispatcher.getEventHandler().handle(new ContainersLauncherEvent(this, launcherEvent));&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ContainersLauncher 收到后，&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;Application app =context.getApplications().get(containerId.getApplicationAttemptId().getApplicationId());
ContainerLaunch launch = new ContainerLaunch(context, getConfig(), dispatcher, exec, app,event.getContainer(), dirsHandler, containerManager);
containerLauncher.submit(launch);
running.put(containerId, launch);
break;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ContainerLaunch 放到线程池执行，对应的 call 方法为：&lt;/p&gt;

&lt;p&gt;为 Container 创建 token 文件 和 &lt;code&gt;launch_container.sh&lt;/code&gt; ，将他们保存到 NodeManager 私有目录 nmPrivate 下面， &lt;code&gt;launch_container.sh&lt;/code&gt;包含了运行所以的命令。
一般都是前面 export 环境变量，最后有个 exec 命令 。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;准备好了 命令，
&lt;code&gt;Container_Launcher&lt;/code&gt; 首先向 ContainerImpl 发送 &lt;code&gt;Container_LANUCHED&lt;/code&gt; 命令，然他启动监控等。然后调用 ContainerExector launchContainer 启动 Container 。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;然后是启动监控，汇报信息等。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>yarn-resourcemanager-1</title>
      <link>https://dengziming.github.io/post/hadoop/yarn-resourcemanager-1/</link>
      <pubDate>Tue, 22 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/hadoop/yarn-resourcemanager-1/</guid>
      
        <description>

&lt;p&gt;参考资料：&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://hortonworks.com/blog/apache-hadoop-yarn-resourcemanager/&#34;&gt;https://hortonworks.com/blog/apache-hadoop-yarn-resourcemanager/&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;提交应用程序的过程&#34;&gt;提交应用程序的过程&lt;/h1&gt;

&lt;h2 id=&#34;1-yarnclient-submitapplication-appcontext&#34;&gt;1. yarnClient.submitApplication(appContext);&lt;/h2&gt;

&lt;p&gt;新建请求，最终调用： rmClient.submitApplication(request);&lt;/p&gt;

&lt;p&gt;实际上会通过RPC调用 ClientRMService.submitApplication(SubmitApplicationRequest request)&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;得到APPID：ApplicationId applicationId = submissionContext.getApplicationId();&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;rmAppManager.submitApplication(submissionContext, System.currentTimeMillis(), user);&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;放到 rmAppManager 中，rmAppManager 中存放了所有的 application。
跟进去，发现调用了：&lt;/p&gt;

&lt;p&gt;this.rmContext.getDispatcher().getEventHandler().handle(new RMAppEvent(applicationId, RMAppEventType.START));&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public void handle(RMAppEvent event) {
      ApplicationId appID = event.getApplicationId();
      RMApp rmApp = this.rmContext.getRMApps().get(appID);
      if (rmApp != null) {
        try {
          rmApp.handle(event);
        } catch (Throwable t) {
          LOG.error(&amp;quot;Error in handling event type &amp;quot; + event.getType()
              + &amp;quot; for application &amp;quot; + appID, t);
        }
      }
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后导致这个 applicationId 所在的 RMAppEvent 状态机发生变化。&lt;/p&gt;

&lt;h2 id=&#34;2-registerapplicationmasterresponse-response-amrmclient-registerapplicationmaster-appmasterhostname-appmasterrpcport-appmastertrackingurl&#34;&gt;2.RegisterApplicationMasterResponse response = amRMClient.registerApplicationMaster(appMasterHostname, appMasterRpcPort,appMasterTrackingUrl);&lt;/h2&gt;

&lt;p&gt;注册 ApplicationMaster，注意这段代码是在用户编写的 ApplicationMaster 类中，所以这段代码运行在yarn给APPMaster分配的Container中。&lt;/p&gt;

&lt;p&gt;RegisterApplicationMasterResponse response = client.registerApplicationMaster(appHostName, appHostPort, appTrackingUrl);&lt;/p&gt;

&lt;p&gt;会调用：RegisterApplicationMasterResponse response = rmClient.registerApplicationMaster(request);&lt;/p&gt;

&lt;p&gt;最终会通过RPC调用：ApplicationMasterServeice.registerApplicationMaster(RegisterApplicationMasterRequest request)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;this.rmContext
        .getDispatcher()
        .getEventHandler()
        .handle(
          new RMAppAttemptRegistrationEvent(applicationAttemptId, request
            .getHost(), request.getRpcPort(), request.getTrackingUrl()));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这种 RMAppAttemptEventType 类型的会 通过handle进行处理：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public void handle(RMAppAttemptEvent event) {
      ApplicationAttemptId appAttemptID = event.getApplicationAttemptId();
      ApplicationId appAttemptId = appAttemptID.getApplicationId();
      RMApp rmApp = this.rmContext.getRMApps().get(appAttemptId);
      if (rmApp != null) {
        RMAppAttempt rmAppAttempt = rmApp.getRMAppAttempt(appAttemptID);
        if (rmAppAttempt != null) {
          try {
            rmAppAttempt.handle(event);
          } catch (Throwable t) {
            LOG.error(&amp;quot;Error in handling event type &amp;quot; + event.getType()
                + &amp;quot; for applicationAttempt &amp;quot; + appAttemptId, t);
          }
        }
      }
    }
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;和上面的 RMAppEvent 一样，会进入一个状态机进行处理。&lt;/p&gt;

&lt;h3 id=&#34;1-状态机相互转换细节&#34;&gt;1.状态机相互转换细节&lt;/h3&gt;

&lt;p&gt;上面的过程细化一下：&lt;/p&gt;

&lt;p&gt;RMAppImpl 收到 RMAppEventType.START 事件后，会调用 RMStateStore#storeApplication，以日志记录 RMAppImpl 当前信息，&lt;/p&gt;

&lt;p&gt;至此，RMAppImpl 的运行状态由 NEW 转移为 NEW_SAVING。该步骤就较为复杂了，下面详细介绍下。&lt;/p&gt;

&lt;p&gt;其中 RMAppEventType 注册到中央异步调度器的地方在 ResourceManager.java 中，new ApplicationEventDispatcher(rmContext) 进行处理，
处理方式很简单：通过appid得到得到 RMAppImpl ，最终会给  RMAppImpl自己处理，进入他的状态机处理。状态机有这么一个事件：&lt;/p&gt;

&lt;p&gt;addTransition(RMAppState.NEW, RMAppState.NEW_SAVING, RMAppEventType.START, new RMAppNewlySavingTransition())&lt;/p&gt;

&lt;p&gt;RMAppNewlySavingTransition 的 transition 就是 app.rmContext.getStateStore().storeNewApplication(app);  实际上就是保存应用的相关信息。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public synchronized void storeNewApplication(RMApp app) {  
    //app=RMAppImpl  
    LOG.info(&amp;quot;begin to storeNewApplication,app=&amp;quot;+app.toString());  
    ApplicationSubmissionContext context = app.getApplicationSubmissionContext();  
    assert context instanceof ApplicationSubmissionContextPBImpl;  
    ApplicationState appState =  
        new ApplicationState(app.getSubmitTime(), app.getStartTime(), context,app.getUser());  
    dispatcher.getEventHandler().handle(new RMStateStoreAppEvent(appState));  
  }  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意： dispatcher.getEventHandler().handle(new RMStateStoreAppEvent(appState));  这里会调用 RMStateStore 状态机的 transition，实际上就是 store + notifyDoneStoringApplication&lt;/p&gt;

&lt;p&gt;&lt;code&gt;rmDispatcher.getEventHandler().handle(new RMAppNewSavedEvent(appId, storedException));&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;这个事件又会进入 RMAppImpl 的状态机，对应代码 addTransition(RMAppState.NEW_SAVING, RMAppState.SUBMITTED, RMAppEventType.APP_NEW_SAVED, new AddApplicationToSchedulerTransition())&lt;/p&gt;

&lt;p&gt;调用：app.handler.handle(new AppAddedSchedulerEvent(app.applicationId,app.submissionContext.getQueue(), app.user));&lt;/p&gt;

&lt;p&gt;会触发： RMAppImpl 处理 AppAddedSchedulerEvent&lt;/p&gt;

&lt;p&gt;然后这个事件会分配给：CapacityScheduler ，&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;case APP_ADDED:  
    {  
      AppAddedSchedulerEvent appAddedEvent = (AppAddedSchedulerEvent) event;  
      addApplication(appAddedEvent.getApplicationId(),  
        appAddedEvent.getQueue(), appAddedEvent.getUser());  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;addApplication 会调用 rmContext.getDispatcher().getEventHandler().handle(new RMAppEvent(applicationId, RMAppEventType.APP_ACCEPTED));&lt;/p&gt;

&lt;p&gt;RMAppImpl 会触发 ：addTransition(RMAppState.SUBMITTED, RMAppState.ACCEPTED,  RMAppEventType.APP_ACCEPTED, new StartAppAttemptTransition())&lt;/p&gt;

&lt;p&gt;对应的transition： createNewAttempt(); handler.handle(new RMAppStartAttemptEvent(currentAttempt.getAppAttemptId(),  transferStateFromPreviousAttempt));&lt;br /&gt;
实际上就是触发 RMAppAttemptImpl 状态机操作。&lt;/p&gt;

&lt;p&gt;RMAppAttemptImpl 接受 RMAppAttemptEventType.START 事件后，进行一系列初始化工作。将自身状态由NEW转换为SUBMITTED，并调用 AttemptStartedTransition。&lt;/p&gt;

&lt;p&gt;AttemptStartedTransition appAttempt.eventHandler.handle(new AppAttemptAddedSchedulerEvent(  appAttempt.applicationAttemptId, transferStateFromPreviousAttempt));&lt;/p&gt;

&lt;p&gt;AppAttemptAddedSchedulerEvent 会交给 CapacityScheduler 。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;case APP_ATTEMPT_ADDED:  
    {  
      AppAttemptAddedSchedulerEvent appAttemptAddedEvent =  
          (AppAttemptAddedSchedulerEvent) event;  
      addApplicationAttempt(appAttemptAddedEvent.getApplicationAttemptId(),  
        appAttemptAddedEvent.getTransferStateFromPreviousAttempt(),  
        appAttemptAddedEvent.getShouldNotifyAttemptAdded());  
    }  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实际上就是讲这个 attempt 放进队列，等待处理。并且：rmContext.getDispatcher().getEventHandler().handle( new RMAppAttemptEvent(applicationAttemptId, RMAppAttemptEventType.ATTEMPT_ADDED));&lt;/p&gt;

&lt;p&gt;RMAppAttemptImpl 接受到事件 RMAppAttemptEventType.ATTEMPT_ADDED 后，状态由SUBMITTED转换为SCHEDULED。进入内部类ScheduleTransition的transition函数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;private static final class ScheduleTransition  
      implements  
      MultipleArcTransition&amp;lt;RMAppAttemptImpl, RMAppAttemptEvent, RMAppAttemptState&amp;gt; {  
    @Override  
    public RMAppAttemptState transition(RMAppAttemptImpl appAttempt,  
        RMAppAttemptEvent event) {  
        LOG.info(&amp;quot;class::ScheduleTransition, func::transition, begin.&amp;quot;);  
      if (!appAttempt.submissionContext.getUnmanagedAM()) {  
        // Request a container for the AM.  
        ResourceRequest request =  
            BuilderUtils.newResourceRequest(  
                AM_CONTAINER_PRIORITY, ResourceRequest.ANY, appAttempt  
                    .getSubmissionContext().getResource(), 1);  
  
        // SchedulerUtils.validateResourceRequests is not necessary because  
        // AM resource has been checked when submission  
        Allocation amContainerAllocation = appAttempt.scheduler.allocate(  
            appAttempt.applicationAttemptId,  
            Collections.singletonList(request), EMPTY_CONTAINER_RELEASE_LIST, null, null);  
        if (amContainerAllocation != null  
            &amp;amp;&amp;amp; amContainerAllocation.getContainers() != null) {  
          assert (amContainerAllocation.getContainers().size() == 0);  
        }  
        return RMAppAttemptState.SCHEDULED;  
      } else {  
        // save state and then go to LAUNCHED state  
        appAttempt.storeAttempt();  
        return RMAppAttemptState.LAUNCHED_UNMANAGED_SAVING;  
      }  
    }  
  } 
   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里面就是：新建资源 ResourceRequest ，然后 appAttempt.scheduler.allocate&lt;/p&gt;

&lt;p&gt;&amp;mdash;&amp;mdash; 这里断层了,谁触发了 AMContainerImpl 启动和分配 Container，需要后续再看。&lt;/p&gt;

&lt;p&gt;这里有个疑问需要解答一下，之前一直好奇是哪里启动了 AMContainerImpl，上面的 schedule.allocate 将需要的资源提交给 schedule ，实际上 schedule 会分配。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;application.updateResourceRequests(ask);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这一句话，&lt;/p&gt;

&lt;p&gt;以  FairScheduler 为例，启动服务会调用 initScheduler(conf); 里面有三行代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;schedulingThread = new ContinuousSchedulingThread();
schedulingThread.setName(&amp;quot;FairSchedulerContinuousScheduling&amp;quot;);
schedulingThread.setDaemon(true);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;会有守护线程调用 continuousSchedulingAttempt(); 实际上会调用：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;    for (NodeId nodeId : nodeIdList) {
      FSSchedulerNode node = getFSSchedulerNode(nodeId);
      try {
        if (node != null &amp;amp;&amp;amp; Resources.fitsIn(minimumAllocation,
            node.getAvailableResource())) {
          attemptScheduling(node);
        }
      } catch (Throwable ex) {
        LOG.error(&amp;quot;Error while attempting scheduling for node &amp;quot; + node +
            &amp;quot;: &amp;quot; + ex.toString(), ex);
      }
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个 attemptScheduling(node); 就会创建 AMContainerImpl 实例，至于怎么创建，需要了解各个 Schedule 的内部细节。&lt;/p&gt;

&lt;p&gt;ResourceManager 为应用程序的 AM 分配资源后，创建一个 RMContainerImpl，并向它发送一个 RMContainerEventType.START 事件。&lt;/p&gt;

&lt;p&gt;RMContainerImpl 收到 RMContainerEventType.START 事件后，直接向 RMAppAttemptImpl 发送一个 RMAppAttemptEventType.CONTAINER_ALLOCATED&lt;/p&gt;

&lt;p&gt;RMAppAttemptImpl 收到 RMAppAttemptEventType.CONTAINER_ALLOCATED 事件后：调用 AMContainerAllocatedTransition：&lt;/p&gt;

&lt;p&gt;transition函数中，调用 scheduler.allocate 获取分配的资源，scheduler 返回资源之前，会向 RMContainerImpl 发送 RMContainerEventType.ALLOCATED事件。&lt;/p&gt;

&lt;p&gt;RMAppAttemptImpl 收到资源后，向 RMStateStore 发送 MStateStoreEventType.STORE_APP_ATTEMPT 事件请求记录日志。&lt;/p&gt;

&lt;p&gt;至此，RMAppAttemptImpl 状态从 SCHEDULED 转换为 ALLOCATED_SAVING。&lt;/p&gt;

&lt;p&gt;日志记录完成后，RMStateStore 向 RMAppAttemptImpl 发送 RMAppAttemptEventType.ATTEMPT_NEW_SAVED 事件。&lt;/p&gt;

&lt;p&gt;RMAppAttemptImpl 收到 RMAppAttemptEventType.ATTEMPT_NEW_SAVED 事件后，
向 ApplicationMasterLauncher 发送 AMLauncherEventType.LAUNCH 事件，
至此，RMAppAttemptImpl 状态从 ALLOCATED_SAVING 转换为 ALLOCATED。&lt;/p&gt;

&lt;p&gt;后面的和这里类似，不过涉及到了 RMContainer状态机，先跳过。&lt;/p&gt;

&lt;h2 id=&#34;3-总结&#34;&gt;3.总结&lt;/h2&gt;

&lt;p&gt;通过这个实例我们大概了解了yarn中的RPC、调度器、服务、状态机配合的过程。
一般是客户端（可以使用户的client、nodeManager进程或者它启动的container进程）发送请求，中间通过RPC调用了ResourceManager中的某个服务，这个服务会触发一定的事件，并且返回。&lt;/p&gt;

&lt;p&gt;例如客户端提交一个应用程序，首先有个 appid，每个appid对应的有一个 RMApp ，放在 rmAppManager 的一个map中。这个 RMApp 是一个状态机。&lt;/p&gt;

&lt;p&gt;然后会调用 this.rmContext.getDispatcher().getEventHandler().handle(new RMAppEvent(applicationId, RMAppEventType.START));&lt;/p&gt;

&lt;p&gt;调度器会启动对应的 EventHandle 去处理这个事件，而 对应的 EventHandle 会根据appid 通过 rmAppManager 得到对应的 RMApp，调用对应的状态转化函数就实现了状态转化。&lt;/p&gt;

&lt;p&gt;再例如某个container启动 APPMaster，也是调用
this.rmContext.getDispatcher().getEventHandler().handle
(new RMAppAttemptRegistrationEvent(applicationAttemptId, request.getHost(), request.getRpcPort(), request.getTrackingUrl()));&lt;/p&gt;

&lt;p&gt;然后调度器会启动对应的 EventHandle 去处理这个事件，而 对应的 EventHandle 会根据appid 通过 rmAppManager 得到对应的 RMApp，
这时候事件类似是 RMAppAttemptEvent，处理逻辑变了，会在另一个状态机进行操作。&lt;/p&gt;

&lt;h2 id=&#34;4-rmcontainer状态机&#34;&gt;4.RMContainer状态机&lt;/h2&gt;

&lt;p&gt;上面分析了 两个状态机，实际上还有一个 RMContainer ，这个和上面两个类似吧。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>yarn-基础库</title>
      <link>https://dengziming.github.io/post/hadoop/yarn-%E5%9F%BA%E7%A1%80%E5%BA%93/</link>
      <pubDate>Tue, 22 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/hadoop/yarn-%E5%9F%BA%E7%A1%80%E5%BA%93/</guid>
      
        <description>

&lt;h1 id=&#34;yarn-事件库和服务库&#34;&gt;yarn-事件库和服务库&lt;/h1&gt;

&lt;h2 id=&#34;使用&#34;&gt;使用&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;新建Event和EventType&lt;/li&gt;
&lt;li&gt;新建 AsyncDispatcher 并给 AsyncDispatcher 注册 Event 和对应的 EventHandler&lt;Event&gt;&lt;/li&gt;
&lt;li&gt;调用 AsyncDispatcher 的 getEventHandler 得到 EventHandler 然后调用 handler 的 handle 方法处理 Event&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;基本原理&#34;&gt;基本原理：&lt;/h2&gt;

&lt;p&gt;AsyncDispatcher 注册 EventHandler&lt;Event&gt; 的过程实际上生成了一个 map，保存了每个事件对应的handler。同时有一个 队列，用于放置 Event&lt;/p&gt;

&lt;p&gt;调用 handle 的时候 将Event放进queue中，内部启动一个线程不断处理 queue的任务。&lt;/p&gt;

&lt;h1 id=&#34;yarn-状态机&#34;&gt;yarn-状态机&lt;/h1&gt;

&lt;h2 id=&#34;使用-1&#34;&gt;使用&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;初始化&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;StateMachineFactory
.addTransition(JobStateInternal.NEW, JobStateInternal.INITED, JobEventType.JOB_INIT,new InitTransition())
.addTransition(JobStateInternal.INITED, JobStateInternal.SETUP, JobEventType.JOB_START,new StartTransition())
.installTopology()
.make()
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;新建对应的 Transition&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public static class InitTransition implements SingleArcTransition&amp;lt;JobStateMachine,JobEvent&amp;gt;{

        @Override
        public void transition(JobStateMachine job, JobEvent event) {
            System.out.println(&amp;quot;Receiving event &amp;quot; + event);
        }

    }
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;调用 StateMachine 的 doTransition(event.getType(), event)&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;原理&#34;&gt;原理&lt;/h2&gt;

&lt;p&gt;installTopology的时候创建一个拓扑图，记录每个 State 能接受的 Event，以及接受该 Event 后的操作，以及操作后的 State。&lt;/p&gt;

&lt;p&gt;每次有Event传入，调用对应的 Transition ，并且将 此时刻 的状态变为 操作后的状态。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>hadoop网站日志分析项目架构</title>
      <link>https://dengziming.github.io/post/project/hadoop/hadoop/</link>
      <pubDate>Fri, 30 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/project/hadoop/hadoop/</guid>
      
        <description>

&lt;p&gt;项目简介：大数据涉及到的业务很多很复杂，从一开始的项目架构，再到后台的网站搭建，以及数据的收集，数据的分析，数据的迁移，业务开发，后台运维，等等。我们没办法一个实验将所有的过程都学习到。本次试验我们将会将重点放在项目架构上，后面的项目我们将重点放在每一部分的实现上。通过本次实验，你将能了解到一个大数据架构师工作的基本步骤，虽然本次实验我们也有复杂的代码分析过程，但是大家没有必要将自己的重点放在代码上面，大家应该更加站在架构师的角度，专注于整个项目每一部分的连接，每个部分具体实现的细节，大家可以不必太深入，我们后期会有专门的实验放在这上面。
有关代码我们已经实现并且提供，大家直接打开，然后阅读熟悉每部分的意义即可。
本次项目我们是架构一个日志分析，我们的要完成的任务包括后台和前端的实现，网站的搭建，nginx反向代理的搭建，etl数据清洗程序，数据分析，数据报表的实现。&lt;/p&gt;

&lt;h2 id=&#34;一-业务分析和需求文档&#34;&gt;一、业务分析和需求文档&lt;/h2&gt;

&lt;h3 id=&#34;1-业务分析概述&#34;&gt;1.业务分析概述&lt;/h3&gt;

&lt;p&gt;本次试验我们主要是分析类似淘宝等购物网站上的点击流，从而进行展示分析。在本次项目中我们分别从七个大的角度来进行分析，分别为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;用户基本信息分析模块、浏览器信息分析模块、地域信息分析模块、用户浏览深度分析模块、外链数据分析模块、订单分析模块以及事件分析模块。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意几个概念:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;用户/访客：表示同一个浏览器代表的用户。唯一标示用户
会员：表示网站的一个正常的会员用户。
会话：一段时间内的连续操作，就是一个会话中的所有操作。
Pv：访问页面的数量
在本次项目中，所有的计数都是去重过的。比如：活跃用户/访客，计算uuid的去重后的个数。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们分析数据的需求文档和最终的展示结果大概如下。&lt;/p&gt;

&lt;h3 id=&#34;2-用户基本信息分析模块&#34;&gt;2.用户基本信息分析模块&lt;/h3&gt;

&lt;p&gt;用户基本信息分析模块主要是从用户/访客和会员两个主要角度分析浏览相关信息，包括但不限于新增用户，活跃用户，总用户，新增会员，活跃会员，总会员以及会话分析等。下面就各个不同的用户信息角度来进行分析：&lt;/p&gt;

&lt;h4 id=&#34;1-用户分析&#34;&gt;(1).用户分析&lt;/h4&gt;

&lt;p&gt;该分析主要分析新增用户、活跃用户以及总用户的相关信息。
新访客:老访客(活跃访客中) =  1:7~10
&lt;img src=&#34;../images/2017-06-20-21-21-54.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;2-会员分析&#34;&gt;(2).会员分析&lt;/h4&gt;

&lt;p&gt;该分析主要分析新增会员、活跃会员以及总会员的相关信息。
&lt;img src=&#34;../images/2017-06-20-21-22-26.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;3-会话分析&#34;&gt;(3).会话分析&lt;/h4&gt;

&lt;p&gt;该分析主要分析会话个数、会话长度和平均会话长度相关的信息。
&lt;img src=&#34;../images/2017-06-20-21-23-17.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;4-hourly分析&#34;&gt;(4).Hourly分析&lt;/h4&gt;

&lt;p&gt;该分析主要分析每天每小时的用户、会话个数以及会话长度的相关信息。
&lt;img src=&#34;../images/2017-06-20-21-24-07.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;3-其他模块分析&#34;&gt;3.其他模块分析&lt;/h3&gt;

&lt;p&gt;在用户模块的基础上，我们可以添加其他的六个模块分析，我们本次试验先不展示所有的模块，只是作简单介绍，例如地域分布模块：
&lt;img src=&#34;../images/2017-06-20-21-25-43.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;上面分析的业务需求大家可能不太懂，没关系，注意在下面的项目中，时不时回头看看我们的需求，就能明白了。&lt;/p&gt;

&lt;h2 id=&#34;二-开发环境搭建&#34;&gt;二、开发环境搭建&lt;/h2&gt;

&lt;p&gt;为了方便管理，我们以后按照管理，尽量使用maven构建java和scala项目。另外我们的软件安装在D盘的&lt;code&gt;soft&lt;/code&gt;目录下，我们的开发项目放在D盘的&lt;code&gt;workspace&lt;/code&gt;目录下。&lt;/p&gt;

&lt;h3 id=&#34;1-下载安装软件&#34;&gt;1.下载安装软件&lt;/h3&gt;

&lt;p&gt;分别在&lt;code&gt;http://tomcat.apache.org&lt;/code&gt;和&lt;code&gt;http://maven.apache.org&lt;/code&gt;下载tomcat和maven，解压后放在D盘的soft目录，然后配置环境变量，需要配置的环境变量包括 &lt;code&gt;MAVEN_HOME&lt;/code&gt;和&lt;code&gt;TOMCAT_HOME&lt;/code&gt;，并且将他们的bin目录添加到&lt;code&gt;PATH&lt;/code&gt;中。安装配置完成后，在命令行输入start-up和mvn命令，检查是否安装正确。
确保无误后，我们的开发环境使用IDEA，安装好IDEA，打开，配置maven的目录，如下图的方法，搜索maven，在Maven的配置填写maven的路径。
&lt;img src=&#34;../images/2017-06-20-19-07-00.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;2-搭建服务器&#34;&gt;2.搭建服务器&lt;/h3&gt;

&lt;h4 id=&#34;1-布置开发环境&#34;&gt;(1).布置开发环境&lt;/h4&gt;

&lt;p&gt;如果大家熟悉javaEE开发，这一段就比较简单。我们搭建服务器就是新建一个javaEE项目，然后启动，这个过程需要借助tomcat实现。首先打开IDEA，IDEA中已经配置好了maven的路径。
点击File -&amp;gt;New -&amp;gt; Project ，选择java的web application，然后下一步:
&lt;img src=&#34;../images/2017-06-20-19-09-55.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在下一步我们设置项目路径，我们的项目名为&lt;code&gt;taobaopayment&lt;/code&gt;，放在D盘下的workspace目录下。然后点击完成。这时候就新建了一个web项目，我们在项目的web文件下能看到一个index.jsp文件，这个文件你可以修改为自定义的内容，例如我修改为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;%@ page contentType=&amp;quot;text/html;charset=UTF-8&amp;quot; language=&amp;quot;java&amp;quot; %&amp;gt;
&amp;lt;html&amp;gt;
  &amp;lt;head&amp;gt;
    &amp;lt;title&amp;gt;taobaopayment&amp;lt;/title&amp;gt;
  &amp;lt;/head&amp;gt;
  &amp;lt;body&amp;gt;
  支付页面
  &amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;2-本地发布项目&#34;&gt;(2).本地发布项目&lt;/h4&gt;

&lt;p&gt;菜单栏选择Run -&amp;gt;Edit Configuration或者点击右上角按钮添加tomcat的发布参数，依次点击 加号 -&amp;gt;tomcat server -&amp;gt; local ，添加tomcat：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/2017-06-20-19-18-24.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在右边的配置页面配置好名字、地址、端口：
&lt;img src=&#34;../images/2017-06-20-19-20-30.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;然后在deployment选项下面点击加号添加发布选项，然后设置你content名字，我们设置为 &lt;code&gt;taobaopayment&lt;/code&gt;：
&lt;img src=&#34;../images/2017-06-20-19-21-52.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;点击确定后，我们可以看到右上方和下方都出现了可以启动的三角形按钮，点击启动：
&lt;img src=&#34;../images/2017-06-20-19-25-24.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;启动成功后打开浏览器，输入&lt;code&gt;http://localhost:8080/taobaopayment/&lt;/code&gt;，出现我们刚刚编辑的jsp页面，剩下的操作自己实验。
&lt;img src=&#34;../images/2017-06-20-19-24-42.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;三-web服务器开发&#34;&gt;三、Web服务器开发&lt;/h2&gt;

&lt;p&gt;根据我们的需求文档，我们需要实现支付成功和退款页面。这里又分为两部分，一是前端的页面传来的请求数据，这部分代码使用JavaScript编写，另一方面是后台的服务器发送过来的代码，通过Java语言编写。&lt;/p&gt;

&lt;h3 id=&#34;1-后端开发&#34;&gt;1.后端开发&lt;/h3&gt;

&lt;h4 id=&#34;1-程序后台事件分析&#34;&gt;(1).程序后台事件分析&lt;/h4&gt;

&lt;p&gt;本项目中在程序后台会有chargeSuccess事件，本事件的主要作用是发送订单成功的信息给nginx服务器。发送格式同pc端发送方式， 也是访问同一个url来进行数据的传输。格式为:
&lt;code&gt;http://hongyahuayu.com/index.jpg?query1=spark&lt;/code&gt;
当会员最终支付成功的时候触发chargeSuccess该事件，该事件需要程序主动调用，然后向后台发送数据：
&lt;code&gt;u_mid=maomao&amp;amp;c_time=1449142044528&amp;amp;oid=orderid_1&amp;amp;ver=1&amp;amp;en=e_cs&amp;amp;pl=jdk&amp;amp;sdk=java&lt;/code&gt;，其中 &lt;code&gt;u_mid&lt;/code&gt;和&lt;code&gt;oid&lt;/code&gt;代表用户id和订单id。
前面我们分析了后端的业务，如果你不太懂，我们尅简单地说，后端程序的工作流如下：
&lt;img src=&#34;../images/2017-06-20-21-52-16.jpg&#34; alt=&#34;&#34; /&gt;
简单说，后端就是要设计方法，当&lt;code&gt;chargeSuccess&lt;/code&gt;触发的时候，我们给后台发送数据。&lt;/p&gt;

&lt;h4 id=&#34;2-后端程序开发&#34;&gt;(2).后端程序开发&lt;/h4&gt;

&lt;p&gt;程序开发有一定难度，另外由于我们本次试验的重点是后面的数据分析，这一块不作太高的要求，大家能够理解即可，核心代码如下，其余代码可以在项目中查看：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;	public static boolean onChargeSuccess(String orderId, String memberId) {
		try {
			if (isEmpty(orderId) || isEmpty(memberId)) {
				// 订单id或者memberid为空
				log.log(Level.WARNING, &amp;quot;订单id和会员id不能为空&amp;quot;);
				return false;
			}
			// 代码执行到这儿，表示订单id和会员id都不为空。
			Map&amp;lt;String, String&amp;gt; data = new HashMap&amp;lt;String, String&amp;gt;();
			data.put(&amp;quot;u_mid&amp;quot;, memberId);
			data.put(&amp;quot;oid&amp;quot;, orderId);
			data.put(&amp;quot;c_time&amp;quot;, String.valueOf(System.currentTimeMillis()));
			data.put(&amp;quot;ver&amp;quot;, version);
			data.put(&amp;quot;en&amp;quot;, &amp;quot;e_cs&amp;quot;);
			data.put(&amp;quot;pl&amp;quot;, platformName);
			data.put(&amp;quot;sdk&amp;quot;, sdkName);
			// 创建url
			String url = buildUrl(data);
			// 发送url&amp;amp;将url加入到队列
			SendDataMonitor.addSendUrl(url);
			return true;
		} catch (Throwable e) {
			log.log(Level.WARNING, &amp;quot;发送数据异常&amp;quot;, e);
		}
		return false;
	}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;注意事项&lt;/em&gt;&lt;/strong&gt; 修改代码这里url地址为自己服务器的地址：
&lt;img src=&#34;../images/2017-06-20-23-42-54.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;2-前端开发&#34;&gt;2.前端开发&lt;/h3&gt;

&lt;h4 id=&#34;1-前端事件分析&#34;&gt;(1).前端事件分析&lt;/h4&gt;

&lt;p&gt;前面我们说后端的事件主要是chargeSuccess，前端的时间处理就更复杂了。针对我们最终的不同分析模块，我们需要不同的数据，接下来分别从各个模块分析，每个模块需要的数据。
1. 用户基本信息就是用户的浏览行为信息分析，也就是我们只需要pageview事件就可以了；
2. 浏览器信息分析以及地域信息分析其实就是在用户基本信息分析的基础上添加浏览器和地域这个维度信息，其中浏览器信息我们可以通过浏览器的window.navigator.userAgent来进行分析，地域信息可以通过nginx服务器来收集用户的ip地址来进行分析，也就是说pageview事件也可以满足这两个模块的分析。
3. 外链数据分析以及用户浏览深度分析我们可以在pageview事件中添加访问页面的当前url和前一个页面的url来进行处理分析，也就是说pageview事件也可以满足这两个模块的分析。
4. 订单信息分析要求pc端发送一个订单产生的事件，那么对应这个模块的分析，我们需要一个新的事件chargeRequest。对于事件分析我们也需要一个pc端发送一个新的事件数据，我们可以定义为event。
我们要分析的模块包括：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;用户基本信息分析
浏览器信息分析
地域信息分析
外链数据分析
用户浏览深度分析
订单信息分析
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们处理的事件包括：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pageview事件
chargeRequest事件
launch事件
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;第一，Launch事件。当用户第一次访问网站的时候触发该事件，不提供对外调用的接口，只实现该事件的数据收集。
第二，Pageview事件，当用户访问页面/刷新页面的时候触发该事件。该事件会自动调用，也可以让程序员手动调用。
第三，chargeRequest事件。当用户下订单的时候触发该事件，该事件需要程序主动调用。
每次都会发送对应的数据，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;u_sd=8E9559B3-DA35-44E1-AC98-85EB37D1F263&amp;amp;c_time=1449139048231&amp;amp;oid=orderid123&amp;amp;on=%E4%BA%A7%E5%93%81%E5%90%8D%E7%A7%B0&amp;amp;cua=1000&amp;amp;cut=%E4%BA%BA%E6%B0%91%E5%B8%81&amp;amp;pt=%E6%B7%98%E5%AE%9D&amp;amp;ver=1&amp;amp;en=e_cr&amp;amp;pl=website&amp;amp;sdk=js&amp;amp;b_rst=1920*1080&amp;amp;u_ud=12BF4079-223E-4A57-AC60-C1A04D8F7A2F&amp;amp;b_iev=Mozilla%2F5.0%20(Windows%20NT%206.1%3B%20WOW64)%20AppleWebKit%2F537.1%20(KHTML%2C%20like%20Gecko)%20Chrome%2F21.0.1180.77%20Safari%2F537.1&amp;amp;l=zh-CN
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个url的字段比较多，字段词典如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;参数名称	类型	描述
en	string	事件名称, eg: e_pv
ver	string	版本号, eg: 0.0.1
pl	string	平台, eg: website
sdk	string	Sdk类型, eg: js
b_rst	string	浏览器分辨率，eg: 1800*678
b_iev	string	浏览器信息useragent
u_ud	string	用户/访客唯一标识符
l	string	客户端语言
u_mid	string	会员id，和业务系统一致
u_sd	string	会话id
c_time	string	客户端时间
p_url	string	当前页面的url
p_ref	string	上一个页面的url
tt	string	当前页面的标题
ca	string	Event事件的Category名称
ac	string	Event事件的action名称
kv_*	string	Event事件的自定义属性
du	string	Event事件的持续时间
oid	string	订单id
on	string	订单名称
cua	string	支付金额
cut	string	支付货币类型
pt	string	支付方式
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;2-前端程序开发&#34;&gt;(2).前端程序开发&lt;/h4&gt;

&lt;p&gt;前面我们简单实现了后端开发，现在前端的JavaScript代码实现可能就更复杂了，对大家来说难度略大，但是还好这不是我们的重点，我大概展示几个函数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;onPageView: function() {
				// 触发page view事件
				if (this.preCallApi()) {
					var time = new Date().getTime();
					var pageviewEvent = {};
					pageviewEvent[this.columns.eventName] = this.keys.pageView;
					pageviewEvent[this.columns.currentUrl] = window.location.href; // 设置当前url
					pageviewEvent[this.columns.referrerUrl] = document.referrer; // 设置前一个页面的url
					pageviewEvent[this.columns.title] = document.title; // 设置title
					this.setCommonColumns(pageviewEvent); // 设置公用columns
					this.sendDataToServer(this.parseParam(pageviewEvent)); // 最终发送编码后的数据ss
					this.updatePreVisitTime(time);
				}
			},

			onChargeRequest: function(orderId, name, currencyAmount, currencyType, paymentType) {
				// 触发订单产生事件
				if (this.preCallApi()) {
					if (!orderId || !currencyType || !paymentType) {
						this.log(&amp;quot;订单id、货币类型以及支付方式不能为空&amp;quot;);
						return ;
					}

					if (typeof(currencyAmount) == &amp;quot;number&amp;quot;) {
						// 金额必须是数字
						var time = new Date().getTime();
						var chargeRequestEvent = {};
						chargeRequestEvent[this.columns.eventName] = this.keys.chargeRequestEvent;
						chargeRequestEvent[this.columns.orderId] = orderId;
						chargeRequestEvent[this.columns.orderName] = name;
						chargeRequestEvent[this.columns.currencyAmount] = currencyAmount;
						chargeRequestEvent[this.columns.currencyType] = currencyType;
						chargeRequestEvent[this.columns.paymentType] = paymentType;
						this.setCommonColumns(chargeRequestEvent); // 设置公用columns
						this.sendDataToServer(this.parseParam(chargeRequestEvent)); // 最终发送编码后的数据ss
						this.updatePreVisitTime(time);
					} else {
						this.log(&amp;quot;订单金额必须是数字&amp;quot;);
						return ;
					}	
				}
			},
			
			onEventDuration: function(category, action, map, duration) {
				// 触发event事件
				if (this.preCallApi()) {
					if (category &amp;amp;&amp;amp; action) {
						var time = new Date().getTime();
						var event = {};
						event[this.columns.eventName] = this.keys.eventDurationEvent;
						event[this.columns.category] = category;
						event[this.columns.action] = action;
						if (map) {
							for (var k in map){
								if (k &amp;amp;&amp;amp; map[k]) {
									event[this.columns.kv + k] = map[k];
								}
							}
						}
						if (duration) {
							event[this.columns.duration] = duration;
						}
						this.setCommonColumns(event); // 设置公用columns
						this.sendDataToServer(this.parseParam(event)); // 最终发送编码后的数据ss
						this.updatePreVisitTime(time);
					} else {
						this.log(&amp;quot;category和action不能为空&amp;quot;);
					}
				}
			}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;完整的代码，如果有兴趣自己可以详细研究，
*** 注意事项 ***，发布的时候一定要将代码中的url改为你的服务器的url：
&lt;img src=&#34;../images/2017-06-20-23-05-24.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;3-项目发布&#34;&gt;3.项目发布&lt;/h3&gt;

&lt;h4 id=&#34;1-本地项目发布&#34;&gt;(1).本地项目发布&lt;/h4&gt;

&lt;p&gt;前面我们已经简单的实现了后台和前端的代码，首先我们在本地启动服务，方法和前面一样，只是我们将自己的代码添加进去了，点击启动按钮。
&lt;img src=&#34;../images/2017-06-20-22-14-58.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;然后去浏览器访问 &lt;code&gt;http://localhost:8080/taobaopayment/demo4.jsp&lt;/code&gt;，然后点击跳转按钮测试
&lt;img src=&#34;../images/2017-06-20-22-16-22.jpg&#34; alt=&#34;&#34; /&gt;
截止现在我们的后台项目基本完成，这里面的代码比较难，大家可以根据情况查看，不用花太多的精力。&lt;/p&gt;

&lt;h4 id=&#34;2-发布到linux的tomcat上&#34;&gt;(2).发布到linux的tomcat上&lt;/h4&gt;

&lt;p&gt;我们的项目不可能放在本地运行，需要放到Linux的集群环境才能正常运行。首先打开Xshell，连上linux节点。
第一步，在Linux上安装tomcat。
安装的过程很简单，首先要安装java配置JAVA相关环境变量，然后在tomcat的官网下载tomcat的tar.gz包，解压，然后配置tomcat相关环境变量。启动tomcat的命令是tomcat安装目录下面的bin下面的&lt;code&gt;startup.sh&lt;/code&gt;，执行就能启动tomcat，然后访问节点的8080端口，如图：
&lt;img src=&#34;../images/2017-06-20-22-27-11.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;第二步，将项目打成war包。
类似以前打jar包，点开project structure -&amp;gt; artificts，添加一个artifict，名字为taobaopayment，type选择 Web Application:Archive，设置好对应的输出目录output idrectory，一般默认即可，然后点击确定&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/2017-06-20-22-28-57.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;设置好后，点击build -&amp;gt; build artifacts，构建war包
&lt;img src=&#34;../images/2017-06-20-22-32-33.jpg&#34; alt=&#34;&#34; /&gt;
构建结束后，进入刚刚设置的输出目录，你将会在刚刚设置的目录下看到一个war包。&lt;/p&gt;

&lt;p&gt;通过xshell的xftp工具，将打出来的war包拷贝放在linux的tomcat安装目录的webapps目录下：
&lt;img src=&#34;../images/2017-06-20-22-35-41.jpg&#34; alt=&#34;&#34; /&gt;
然后tomcat会自动解压war包，我们就可以在浏览器访问刚刚发布的项目了：
&lt;img src=&#34;../images/2017-06-20-22-39-45.jpg&#34; alt=&#34;&#34; /&gt;
到这里我们的整个后端项目就发布成功了。&lt;/p&gt;

&lt;h2 id=&#34;四-数据分析系统开发&#34;&gt;四、数据分析系统开发&lt;/h2&gt;

&lt;p&gt;本次我们的重心是整个系统的搭建，这部分的开发过程比较复杂，大家酌情进行学习，代码我们已经写好，大家只要稍作理解。细节和逻辑我们后续的实验还会讲解。&lt;/p&gt;

&lt;h3 id=&#34;1-需求回顾&#34;&gt;1.需求回顾&lt;/h3&gt;

&lt;p&gt;之前我们已经做过需求分析，这时候大家再回头看看我们一开始的需求设计，我们是要完成几个关键指标的设计分析。假设我们已经配置好了tomcat和nginx，那么我们知道每次用有浏览等行为时，我们的服务器就会给我们的设置的url发送数据，然后nginx就会收到我们发送的数据。接下来就是分析nginx收集到的数据。&lt;/p&gt;

&lt;p&gt;随便选取一条数据查看：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;192.168.126.1^A1458731952.690^A192.168.126.11^A/log.gif?en=e_pv&amp;amp;p_url=http%3A%2F%2Flocalhost%3A8080%2FBIG_DATA_LOG2%2Fdemo.jsp&amp;amp;p_ref=http%3A%2F%2Flocalhost%3A8080%2FBIG_DATA_LOG2%2Fdemo.jsp&amp;amp;tt=%E6%B5%8B%E8%AF%95%E9%A1%B5%E9%9D%A21&amp;amp;ver=1&amp;amp;pl=website&amp;amp;sdk=js&amp;amp;u_ud=EAB36BC9-0347-4D33-8579-AA8C331D001A&amp;amp;u_mid=laoxiao&amp;amp;u_sd=2D24B8A2-B2EF-450C-8C86-4F8B0F3E2785&amp;amp;c_time=1458731943823&amp;amp;l=zh-CN&amp;amp;b_iev=Mozilla%2F5.0%20(Windows%20NT%2010.0%3B%20WOW64%3B%20rv%3A45.0)%20Gecko%2F20100101%20Firefox%2F45.0&amp;amp;b_rst=1366*768
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这条数据提取了很多来自url的query信息，通过^A隔开，我们需要编写代码分析这种数据。&lt;/p&gt;

&lt;h3 id=&#34;2-etl处理&#34;&gt;2.ETL处理&lt;/h3&gt;

&lt;h4 id=&#34;1-定义工具类&#34;&gt;(1).定义工具类&lt;/h4&gt;

&lt;p&gt;本次试验的工具类主要是从一个url中抽取KPI信息，我们前面的业务需要的信息包括了地址、浏览器、操作系统等，根据我们的分析，所以我们需要使用IP地址解析等工具。解析url的工具类我们放在&lt;code&gt;com.hongya.etl.util&lt;/code&gt;下面，大家自己认真分析。&lt;/p&gt;

&lt;h4 id=&#34;2-定义相关常量类&#34;&gt;(2).定义相关常量类&lt;/h4&gt;

&lt;p&gt;我们的数据分析是有时间段的，我们分析的结果放进hbase中的表中，表名、字段名、时间范围等都是需要用到的常量，我们放在&lt;code&gt;com.hongya.common&lt;/code&gt;下面，大家可以查看。&lt;/p&gt;

&lt;h4 id=&#34;3-业务代码&#34;&gt;(3).业务代码&lt;/h4&gt;

&lt;p&gt;我们的数据字段含义在前面已经讲过了，现在我们需要将数据解析后放进hbase中。ETL过程就是简单的字符串处理，只需要一个Mapper程序即可完成。相应的代码在&lt;code&gt;com.hongya.etl.mr.ald&lt;/code&gt;中，大家可以查看。&lt;/p&gt;

&lt;h4 id=&#34;4-本地测试运行&#34;&gt;(4).本地测试运行&lt;/h4&gt;

&lt;p&gt;然后我们在本地新建一个文件，将上面哪一行测试数据放进去：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;192.168.126.1^A1458731952.690^A192.168.126.11^A/log.gif?en=e_pv&amp;amp;p_url=http%3A%2F%2Flocalhost%3A8080%2FBIG_DATA_LOG2%2Fdemo.jsp&amp;amp;p_ref=http%3A%2F%2Flocalhost%3A8080%2FBIG_DATA_LOG2%2Fdemo.jsp&amp;amp;tt=%E6%B5%8B%E8%AF%95%E9%A1%B5%E9%9D%A21&amp;amp;ver=1&amp;amp;pl=website&amp;amp;sdk=js&amp;amp;u_ud=EAB36BC9-0347-4D33-8579-AA8C331D001A&amp;amp;u_mid=laoxiao&amp;amp;u_sd=2D24B8A2-B2EF-450C-8C86-4F8B0F3E2785&amp;amp;c_time=1458731943823&amp;amp;l=zh-CN&amp;amp;b_iev=Mozilla%2F5.0%20(Windows%20NT%2010.0%3B%20WOW64%3B%20rv%3A45.0)%20Gecko%2F20100101%20Firefox%2F45.0&amp;amp;b_rst=1366*768
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后我们将&lt;code&gt;com.hongya.etl.mr.ald.AnalyserLogDataRunner&lt;/code&gt;中的&lt;code&gt;setJobInputPaths&lt;/code&gt;方法的路径改为刚刚添加的文件的路径，并且在&lt;code&gt;setConf&lt;/code&gt;方法设置一下zookeeper地址，并且启动zookeeper，就可以点击运行，在本地观察结果。
&lt;img src=&#34;../images/2017-06-21-01-16-19.jpg&#34; alt=&#34;&#34; /&gt;
如图我们可以看出ETL后将这一行数据转化为了Hbase的一条Put，这里一直运行不结束是因为我没有启动Hbase，所以一直没法写进去，发布项目的时候是需要启动的。
这个Mapper读取数据格式上面有，而写出的数据格式是Hbase的Put。不知道大家是否记得Hbase的javaAPI，我们介绍过Put的使用。最后我们每一条记录将会放进Hbase的表格中，例如上面的示例数据最后会解析为一条Put数据，我们可以看出它的rowKey是带着日期的：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rowKey 1458731952690_84973288
cf: info ,key:tt value:测试页面1
cf: info ,key:country value:unknown
cf: info ,key:ver value:1
cf: info ,key:u_mid value:laoxiao
cf: info ,key:os value:Windows
cf: info ,key:city value:unknown
cf: info ,key:ip value:192.168.126.1
cf: info ,key:b_rst value:1366*768
cf: info ,key:en value:e_pv
cf: info ,key:c_time value:1458731943823
cf: info ,key:l value:zh-CN
cf: info ,key:u_sd value:2D24B8A2-B2EF-450C-8C86-4F8B0F3E2785
cf: info ,key:u_ud value:EAB36BC9-0347-4D33-8579-AA8C331D001A
cf: info ,key:os_v value:Windows
cf: info ,key:p_ref value:http://localhost:8080/BIG_DATA_LOG2/demo.jsp
cf: info ,key:province value:unknown
cf: info ,key:s_time value:1458731952690
cf: info ,key:p_url value:http://localhost:8080/BIG_DATA_LOG2/demo.jsp
cf: info ,key:browser value:Firefox
cf: info ,key:sdk value:js
cf: info ,key:pl value:website
cf: info ,key:browser_v value:45.0
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;3-关键指标分析&#34;&gt;3.关键指标分析&lt;/h3&gt;

&lt;p&gt;上面的ETL完成后我们的结果数据都放在hbase的event_logs表格的info列族中，现在我们需要运行代码分析这些数据，对我们的数据进行我们前面的设计文档中的关键指标分析。&lt;/p&gt;

&lt;h4 id=&#34;1-代码结构规范&#34;&gt;(1).代码结构规范&lt;/h4&gt;

&lt;p&gt;我们的程序需要定期分析hbase的数据，我们分析的指标有很多，我们需要从hbase中提取的数据也有很多。最后分析完每个指标后我们放进mysql中，供echart做展示。
1. 首先我们分析的指标需要即KPI需要专门的类定义好，放在&lt;code&gt;com.hongya.common.KpiType&lt;/code&gt;下面。
2. 我们需要自定义Key和Value的类型，这些类型包含了需要统计的关键维度信息，作为mapreduce任务的输入输出key，我们定义好了放在&lt;code&gt;com.hongya.transformer.model.dim&lt;/code&gt;下面。
3. 我们的hadoop任务写mysql需要有专门的OutputFormat，我们放在&lt;code&gt;com.hongya.transformer.service&lt;/code&gt;下面，由于每个唯独统计任务写mysql都不一样，所以我们通过配置文件的方式传入，在&lt;code&gt;output-collector.xml&lt;/code&gt;中有相关配置。
4. 无论是读写mysql还是hbase都有配置，我们通过配置文件的方式传入，配置文件有&lt;code&gt;query-mapping.xml&lt;/code&gt;，&lt;code&gt;transfomer-env.xml&lt;/code&gt;。&lt;/p&gt;

&lt;h4 id=&#34;2-业务代码实现&#34;&gt;(2).业务代码实现&lt;/h4&gt;

&lt;p&gt;上面分析了业务，我们开始写mapreduce程序统计分析指标。
我们知道我们现在的mapreduce读取hbase的数据，然后写进mysql中，相关代码我们放在了&lt;code&gt;com.hongya.transformer.mr&lt;/code&gt;包下面。
由于时间关系，我们的业务只实现了new user指标的统计，大家可以查看&lt;code&gt;com.hongya.transformer.mr.nu&lt;/code&gt;包下的内容。
当我们的hbase中有数据时，运行&lt;code&gt;com.hongya.transformer.mr.nu.NewInstallUserRunner&lt;/code&gt;，就能看到下面的结果。
&lt;img src=&#34;../images/2017-06-21-13-10-29.jpg&#34; alt=&#34;&#34; /&gt;
如果你map完成后reduce就失败 ，没关系，是因为你的mysql还没有配置好，我们在后面会介绍的。&lt;/p&gt;

&lt;h2 id=&#34;四-数据分析系统架构&#34;&gt;四、数据分析系统架构&lt;/h2&gt;

&lt;p&gt;我们有了服务器后，接下来的任务就是对服务器的数据进行收集处理，处理后的数据进行展示。我们需要搭建一个完整的服务器，这时候首先需要一个集群，我们在云端直接使用一个节点的centos作为服务器。首先打开Xshell，连上centos节点，我们这里节点名为&lt;code&gt;node1&lt;/code&gt;。&lt;/p&gt;

&lt;h3 id=&#34;1-部署hadoop和hbase&#34;&gt;1.部署hadoop和Hbase&lt;/h3&gt;

&lt;h4 id=&#34;1-部署hadoop&#34;&gt;(1).部署hadoop&lt;/h4&gt;

&lt;p&gt;hadoop单节点安装很简单，以前讲过。直接解压后，配置&lt;code&gt;hadoop-env.sh、core-site.xml、hdfs-site.xml&lt;/code&gt;
1. hadoop-env.sh配置 JAVA_HOME
2. core-site.xml配置&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt; &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;fs.defaultFS&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;hdfs://localhost:8020&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hadoop.tmp.dir&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;/opt/data/hadoop&amp;lt;/value&amp;gt;
      &amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后就能够格式化，启动：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hdfs namenode -format
start-dfs.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;2-配置hbase&#34;&gt;(2).配置Hbase&lt;/h4&gt;

&lt;p&gt;首先安装单节点的zookeeper，安装好后启动，这个不细说：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;zkServer.sh start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后解压Hbase安装包.
1. 配置hbase-env.sh，配置JAVA_HOME，然后将 HBASE_MANAGES_ZK改为false
2. 配置hbase-site.xm&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hbase.rootdir&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;hdfs://localhost:8020/hbase&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hbase.cluster.distributed&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hbase.zookeeper.quorum&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;localhost&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;复制hadoop配置
复制hadoop的 core-site.xml和hdf-site.xml到hbase的conf目录下
然后就能启动Hbase了。
启动hbase后使用hbase shell进入交互窗口，执行建表语句：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;create &#39;event_logs&#39; ,&#39;info&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果执行成功了就可以开始下一步了。&lt;/p&gt;

&lt;h3 id=&#34;2-部署nginx服务&#34;&gt;2.部署nginx服务&lt;/h3&gt;

&lt;p&gt;前面我们将项目发布到tomcat上面了。正常情况下我们需要通过nginx进行负载均衡，同时收集url的请求日志。
我们可以安装nginx，也可以安装淘宝开源的tengine，比一般nginx多一些功能，而且淘宝上有中文文档。&lt;/p&gt;

&lt;h4 id=&#34;1-nginx安装&#34;&gt;(1).nginx安装&lt;/h4&gt;

&lt;p&gt;步骤如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;1.安装GCC编译器等工具：
yum install -y gcc gcc-c++ autoconf automake libtool make openssl openssl-devel pcre pcre-devel
2.下载安装Nginx:
wget http://nginx.org/download/nginx-1.6.3.tar.gz
注：这里也可以下载tengine压缩包，比一般nginx多一些功能
tar -zxvf nginx-1.6.3.tar.gz 
cd nginx-1.6.3/  
./configure --prefix=/usr/local/nginx
--sbin-path=/usr/local/nginx/sbin/nginx
--conf-path=/usr/local/nginx/conf/nginx.conf
--pid-path=/usr/local/nginx/logs/nginx.pid \
--with-http_ssl_module \
--with-http_stub_status_module \
--with-http_gzip_static_module \ 
make &amp;amp;&amp;amp; make install 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果正常的话，就安装好了，然后启动nginx服务即可。记住启动之前需要先关闭之前的tomcat，因为他们两个的端口冲突了。启动命令是就是nginx，启动以后，如果不修改配置，我们可以直接打开浏览器访问8080端口，出现这样就算是成功了。
&lt;img src=&#34;../images/2017-06-21-15-52-28.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&#34;2-nginx配置&#34;&gt;(2).nginx配置&lt;/h4&gt;

&lt;p&gt;现在我们已经安装了nginx，我们要配置nginx的反向代理，让他替我们监听我们的需要监听的端口。上面我们安装的时候已经配置了配置文件的路径：&lt;code&gt;/usr/local/nginx/conf/nginx.conf&lt;/code&gt;。现在我们修改他的内容。
我们让它监听80端口，这样我们给80端口发送的数据就可以被nginx收集然后我们后期处理：
我们只需要添加下面的内容：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;log_format my_format &#39;$remote_addr^A$msec^A$http_host^A$request_uri&#39;;

location = /log.gif {
   root html;
   ## 配置日志文件保存位置
   access_log /opt/data/access.log my_format;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改后的nginx.conf内容大概是这样的，其实就是添加了一行log_format，然后修改了端口信息：
&lt;img src=&#34;../images/2017-06-21-16-06-08.jpg&#34; alt=&#34;&#34; /&gt;
然后我们通过命令让配置文件生效： sudo nginx -s reload&lt;/p&gt;

&lt;h4 id=&#34;3-测试nginx收集日志&#34;&gt;(3).测试nginx收集日志&lt;/h4&gt;

&lt;p&gt;这时候我们可以启动我们之前的web项目，启动之前记得修改代码的url为刚刚配置的地址格式：
&lt;img src=&#34;../images/2017-06-21-16-11-05.jpg&#34; alt=&#34;&#34; /&gt;
然后我们发布运行项目，或者打war包放在tomcat里面，然后启动tomcat。在浏览器输入：&lt;code&gt;http://localhost:8080/taobaopayment/demo4.jsp&lt;/code&gt; 模拟用户点击。如果你发现浏览器一直处于刷新状态，可能你需要换一个浏览器：
&lt;img src=&#34;../images/2017-06-21-16-14-49.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;你还可以运行我们的Test类，来模拟后台的数据，报错没关系，只需要手动点击停止程序：
&lt;img src=&#34;../images/2017-06-21-16-16-11.jpg&#34; alt=&#34;&#34; /&gt;
然后我们查看刚刚配置的文件，已经有了几条记录，是刚刚我们发送的，而且都是我们配置的格式：
&lt;img src=&#34;../images/2017-06-21-16-25-15.jpg&#34; alt=&#34;&#34; /&gt;
到这里就恭喜，我们的gninx基本完成。&lt;/p&gt;

&lt;h3 id=&#34;3-日志收集系统&#34;&gt;3.日志收集系统&lt;/h3&gt;

&lt;p&gt;日志手机一般有两种方式，shell实现和flume实现。
shell命令前面大家都熟悉过，flume使用在前面的SparkStreaming实验也使用过，我们不介绍过多，简单回顾一下即可。
首先安装好flume，配置JAVA_HOME和HADOOP_HOME，然后新建或者复制一个配置文件，log.cfg ，添加下面的内容：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# 配置三个组件的名字
agent.sources = r1
agent.channels = c1
agent.sinks = k1

# For each one of the sources, the type is defined
agent.sources.r1.type = exec
## 这里配置你刚刚手机日志的文件
agent.sources.r1.command = tail -F /Users/dengziming/opt/data/hongya/taobaopayment/access.log
agent.sources.r1.port = 44444

# The channel can be defined as follows.
agent.channels.c1.type = memory
agent.channels.c1.capacity = 1000
agent.channels.c1.transactionCapacity = 1000

# Each sink&#39;s type must be defined
agent.sinks.k1.type = hdfs
agent.sinks.k1.hdfs.path = hdfs://localhost:8020/flume/events/%Y-%m-%d/%H%M/
agent.sinks.k1.hdfs.filePrefix = events-
agent.sinks.k1.hdfs.round = true
agent.sinks.k1.hdfs.roundValue = 10
agent.sinks.k1.hdfs.roundUnit = minute
agent.sinks.k1.hdfs.useLocalTimeStamp = true

#Specify the channel the sink should use
agent.sources.r1.channels = c1
agent.sinks.k1.channel = c1
agent.channels.memoryChannel.capacity = 100
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置好后适用命令启动：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/flume-ng agent --conf conf --conf-file conf/log.cfg --name agent -D flume.root.logger=INFO,console
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样就会收集我们刚刚nginx的日志到hadoop的 &lt;code&gt;/flume/events/%Y-%m-%d/%H%M/&lt;/code&gt; 路径下
恭喜你，马上就要进入数据分析部分&lt;/p&gt;

&lt;h3 id=&#34;4-提交数据分析任务&#34;&gt;4.提交数据分析任务&lt;/h3&gt;

&lt;p&gt;现在我们要开始运行程序，分析数据了。&lt;/p&gt;

&lt;h4 id=&#34;1-启动zookeeper-hadoop-hbase&#34;&gt;(1).启动zookeeper、hadoop、hbase&lt;/h4&gt;

&lt;p&gt;启动命令就不说了，然后记得之前我们已经在hbase中创建了表格：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;create &#39;event_logs&#39; ,&#39;info&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;../images/2017-06-21-16-45-52.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;2-运行etl任务&#34;&gt;(2).运行ETL任务&lt;/h4&gt;

&lt;p&gt;我们的etl的任务是 &lt;code&gt;com.hongya.etl.mr.ald.AnalyserLogDataRunner&lt;/code&gt;，打开这段代码，我们修改几个路径和配置，因为是测试，我们把数据放在本地运行。修改的主要是zookeeper地址和我们刚刚的日志路径：
&lt;img src=&#34;../images/2017-06-21-16-41-30.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;然后运行程序：
&lt;img src=&#34;../images/2017-06-21-16-46-55.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;然后我们可以根据日志看到打印的rowKey，我们可以在hbase中查看这些rowKey
&lt;img src=&#34;../images/2017-06-21-16-48-51.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;3-创建mysql表格&#34;&gt;(3).创建mysql表格&lt;/h4&gt;

&lt;p&gt;我们的etl完成后数据放在hbase中，然后我们需要进行统计分析，结果放在mysql，首先是建立mysql表格，我们的表格统一放在数据库report下面，首先建库，然后按照下面的语句依次建表：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;DROP TABLE IF EXISTS `stats_user`;
CREATE TABLE `stats_user` (
  `date_dimension_id` int(11) NOT NULL,
  `platform_dimension_id` int(11) NOT NULL,
  `active_users` int(11) DEFAULT &#39;0&#39; COMMENT &#39;活跃用户数&#39;,
  `new_install_users` int(11) DEFAULT &#39;0&#39; COMMENT &#39;新增用户数&#39;,
  `total_install_users` int(11) DEFAULT &#39;0&#39; COMMENT &#39;总用户数&#39;,
  `sessions` int(11) DEFAULT &#39;0&#39; COMMENT &#39;会话个数&#39;,
  `sessions_length` int(11) DEFAULT &#39;0&#39; COMMENT &#39;会话长度&#39;,
  `total_members` int(11) unsigned DEFAULT &#39;0&#39; COMMENT &#39;总会员数&#39;,
  `active_members` int(11) unsigned DEFAULT &#39;0&#39; COMMENT &#39;活跃会员数&#39;,
  `new_members` int(11) unsigned DEFAULT &#39;0&#39; COMMENT &#39;新增会员数&#39;,
  `created` date DEFAULT NULL,
  PRIMARY KEY (`platform_dimension_id`,`date_dimension_id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8 ROW_FORMAT=COMPACT COMMENT=&#39;统计用户基本信息的统计表&#39;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这只是一个表格，由于我们的表格太多，这里不展示，我们会将所有数据库的建表语句放在文件中，大家可以参考，最终如图：
&lt;img src=&#34;../images/2017-06-21-16-55-12.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;4-运行统计任务&#34;&gt;(4).运行统计任务&lt;/h4&gt;

&lt;p&gt;新用户点击分析任务放在&lt;code&gt;com.hongya.transformer.mr.nu.NewInstallUserRunner&lt;/code&gt;中，大家可以查看代码，然后我们修改一下配置，主要是mysql的用户名密码，在DimensionConverterImpl中，另外我们还有查看核对 src/transformer-env.xml 下的内容：
如图，修改用户名密码为你的mysql设置：
&lt;img src=&#34;../images/2017-06-21-16-58-32.jpg&#34; alt=&#34;&#34; /&gt;
修改zookeeper地址和运行的起始日期，你可以设置的小一点：
&lt;img src=&#34;../images/2017-06-21-16-57-33.jpg&#34; alt=&#34;&#34; /&gt;
然后我们点击运行，我们就可以根据日志看到map和reduce执行的过程:
&lt;img src=&#34;../images/2017-06-21-17-02-13.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;执行完成后，我们查看我们刚刚的mysql的report库的stats_device_browser和stats_user表格：
&lt;img src=&#34;../images/2017-06-21-17-03-45.jpg&#34; alt=&#34;&#34; /&gt;
当然我们还可以查看dimension_browser等其他表格。
程序执行成功。&lt;/p&gt;

&lt;h3 id=&#34;4-数据转移系统&#34;&gt;4.数据转移系统&lt;/h3&gt;

&lt;p&gt;数据转移系统我们使用sqoop，由于我们的部分mapreduce任务每次运行的结果都放在hadoop或者Hbase上面，我们可能需要手动将关键指标转移到关系型数据库，然后编写代码进行展示。但是在这里，我们都写进了mysql，就暂时不适用sqoop了，也是为了减轻大家的负担。&lt;/p&gt;

&lt;h3 id=&#34;5-数据展示&#34;&gt;5.数据展示&lt;/h3&gt;

&lt;p&gt;数据展示我们使用 jquery + Echart吧。真正项目的echart展示部分一般不需要我们管，会有专门的前端高手负责，所以我们就简单的做一下吧。以浏览器维度为例，我们直接写sql语句&lt;code&gt;select name,count(*) from dimension_browser group by browser_name&lt;/code&gt;，将结果文件写到echart的option属性中：
&lt;img src=&#34;../images/2017-06-21-20-20-50.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;然后重新再浏览器段查看吧：
&lt;img src=&#34;../images/2017-06-21-20-21-47.jpg&#34; alt=&#34;&#34; /&gt;
当然我这是一种不可取的做法，因为这种方式显然是不符合企业生产环境的。真正的生产环境肯定是通过后台和数据库交互，通过ajax将数据传给前端展示，我们做的很敷衍，是因为这不是我们的重点。&lt;/p&gt;

&lt;h3 id=&#34;6-项目发布&#34;&gt;6.项目发布&lt;/h3&gt;

&lt;p&gt;这上面的所有步骤都完成了，就可以发布项目了，我们需要一套任务调度系统。azkaban是目前来说用的比较多的任务调度系统，我们推荐大家课后了解一下azkaban的安装使用。这里我们没法演示了。&lt;/p&gt;

&lt;h2 id=&#34;项目上线&#34;&gt;项目上线&lt;/h2&gt;

&lt;h3 id=&#34;1-基本步骤&#34;&gt;1.基本步骤&lt;/h3&gt;

&lt;p&gt;将整个项目设计好以后，就可以上线了，这里我们总结一下真实项目上线的过程。
1. 软件的安装
本地安装开发环境需要的东西，以及相关的依赖。服务器需要安装tomcat、nginx、zookeeper、hadoop、Hbase
2. 项目开发
这里的项目开发有三部分，服务端程序，日志分析程序，前端展示程序，其中日志分析程序我们只完成了new user 开发，剩下的业务由大家自己开发。前端展示程序我们只是简单展示，没有开发，这不是重点。
3. 搭建nginx服务器，监听80端口
nginx的安装和部署需要注意很多，安装完成后修改配置文件。
4. 启动flume，收集来自nginx的数据
flume配置完成后会收集日志文件的日志，按照时间放到hadoop上面。
5. 发布web项目
将项目打成war包，放到tomcat上，然后浏览器就可以访问，有访问时会向80端口发送数据。
6. 建表
新建hbase的表格和mysql表格
7. 定时启动hadoop任务
我们通过以前说过的方法将程序打成jar包，上传到linux，在Linux上通过&lt;code&gt;hadoop jar&lt;/code&gt;命令提交。
8. 启动展示任务
这里我们简单处理一下忽略掉。
9. 将运行和展示任务添加到定时任务进行调度
这部分比较复杂，需要专门的时间学习。&lt;/p&gt;

&lt;h3 id=&#34;2-自己动手发布程序&#34;&gt;2.自己动手发布程序&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;安装相关软件，我这里已经安装完成，大家自己检查安装。&lt;/li&gt;

&lt;li&gt;&lt;p&gt;安装hadoop、zookeeper、hbase、flume并配置
这部分不详细讲解，配置方法上面都有，配置好以后启动相应集群。启动命令分别为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;start-dfs.sh
start-yarn.sh
start-hbase.sh
bin/flume-ng agent -c ./conf -f ./conf/log.cfg -n agent 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;至少有这些java进程：
&lt;img src=&#34;../images/2017-06-21-21-38-57.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;安装nginx，启动nginx服务，修改nginx的配置文件，监听80端口，并且收集格式为： /log.gif 的url。
可以在浏览器访问linux的80端口：
&lt;img src=&#34;../images/2017-06-21-21-40-36.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;项目打war包，放到tomcat中。
打包之前，修改代码的url为你的linux节点加上log.gif
&lt;img src=&#34;../images/2017-06-21-21-57-51.jpg&#34; alt=&#34;&#34; /&gt;
然后打成war包，名字为：&lt;code&gt;taobaopayment.war&lt;/code&gt;，放在tomcat的webapps目录下，使员工startup.sh 启动tomcat，访问8080端口，然后访&lt;code&gt;http://node1:8080/taobaopayment/demo4.jsp&lt;/code&gt;：
如果这里一直在刷新，那么需要换一个浏览器：
&lt;img src=&#34;../images/2017-06-21-22-06-20.jpg&#34; alt=&#34;&#34; /&gt;
比如我用Safari浏览器，不断点击，产生数据：
&lt;img src=&#34;../images/2017-06-21-22-07-45.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;创建hbase和mysql表格
hbase创建event_logs表格，info列族，
mysql建表语句文件里有。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;执行mapreduce的ETL任务
你可以选择打jar包，或者在本地执行，执行的主类是：&lt;code&gt;com.hongya.etl.mr.ald.AnalyserLogDataRunner&lt;/code&gt;
需要修改zookeeper配置和输入路径，如果在集群上运行，这个输入路径是前面配置的flume手机日志的路径。
&lt;img src=&#34;../images/2017-06-21-22-12-59.jpg&#34; alt=&#34;&#34; /&gt;
执行完成后，可以去hbase查看数据。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;运行分析程序
分析程序基于我们刚刚的结果，主类为：&lt;code&gt;com.hongya.transformer.mr.nu.NewInstallUserRunner&lt;/code&gt;，运行之前需要在&lt;code&gt;DimensionConverterImpl&lt;/code&gt;类中设置mysql的连接信息。
&lt;img src=&#34;../images/2017-06-21-22-16-00.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;查看mysql数据库的结果，并展示
查看mysql的结果：
&lt;img src=&#34;../images/2017-06-21-22-16-54.jpg&#34; alt=&#34;&#34; /&gt;
数据展示模块，需要使用Echart，脱离了我们的实验主题，我们简单模拟，访问浏览器的：&lt;code&gt;http://node1:8080/taobaopayment/showUser.jsp&lt;/code&gt;和&lt;code&gt;http://node1:8080/taobaopayment/showBrowser.jsp&lt;/code&gt;
&lt;img src=&#34;../images/2017-06-21-22-18-47.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
      
    </item>
    
  </channel>
</rss>