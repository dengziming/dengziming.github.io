<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>架构 on 数据分析师之旅</title>
    <link>https://dengziming.github.io/tags/%E6%9E%B6%E6%9E%84/</link>
    <description>Recent content in 架构 on 数据分析师之旅</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Mon, 15 Apr 2019 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://dengziming.github.io/tags/%E6%9E%B6%E6%9E%84/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>分布式存储-Consensus</title>
      <link>https://dengziming.github.io/post/data-intensive-architecture/consensus/</link>
      <pubDate>Mon, 15 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/data-intensive-architecture/consensus/</guid>
      
        <description>

&lt;h1 id=&#34;七-consensus&#34;&gt;七、Consensus&lt;/h1&gt;

&lt;p&gt;之前介绍了 replica、partition、transaction。本次我们开始分布式系统相关讨论。&lt;/p&gt;

&lt;h2 id=&#34;1-consistency-guarantees&#34;&gt;1. Consistency Guarantees&lt;/h2&gt;

&lt;p&gt;分布式系统会有一个比较模糊的概念，eventual consistency,这是一个 weak guarantee，还会学习一些 stronger consistency，
然后我们又回到之前谈论过的 order，最后讨论分布式事务。&lt;/p&gt;

&lt;h2 id=&#34;2-linearizability&#34;&gt;2. Linearizability&lt;/h2&gt;

&lt;p&gt;在 eventually consistent 中，我们保证数据最终一致，这很 confusion，能不能保证任何时候访问都是一样的，也就是数据好像只有一个 replica 一样？
这就是 linearizability（也叫“atomic consistency [7], strong consistency, immediate consistency, external consistency [8]”）&lt;/p&gt;

&lt;p&gt;在 linearizable 中，client 写成功以后，其他的 clients 都必须马上能看到这个数据，保证数据都是最新的、不过时的数据，
换句话说 linearizable 就是 recency guarantee，我们可以看一些 not linearizable 的 例子：&lt;/p&gt;

&lt;p&gt;两个人通过手机视频看世界杯，但是由于访问了不同的 replica，存在 replica-lag，导致数据不一致，
其中至少有一个看到的是 stale value，也就是 violation of linearizability。&lt;/p&gt;

&lt;h3 id=&#34;1-what-makes-a-system-linearizable&#34;&gt;(1) What Makes a System Linearizable?&lt;/h3&gt;

&lt;p&gt;上面已经说了，Linearizable 其实就是说多个 replica 表现的像分一个 replica 一样。
在 distributed systems 的书面语中，数据被称为 register，可以是一个 redis 的一个 key，mysql 的一行数据。
register 我们定义两种基本类型的 operation：
1. read(x) ⇒ v means the client requested to read the value of register x, and the database returned the value v.
2. write(x, v) ⇒ r means the client requested to set the register x to value v, and the database returned response r (which could be ok or error).”&lt;/p&gt;

&lt;p&gt;有这些还不能完全定义 linearizability：如果读写并发，可能读到 new value 或者 old value，会出现值 back and forth 的情况。
这显然不能满足需求，所以我们需要加更多 constraint，我们要求如果 A 某个时刻读到了 new value，B再次读不能出现 old value。
这样我们就可以在定义一种类型的 operation：
3. cas(x, vold, vnew) ⇒ r means the client requested an atomic compare-and-set operation (see “Compare-and-set”). If the current value of the register x equals vold, it should be atomically set to vnew. If x ≠ vold then the operation should leave the register unchanged and return an error. r is the database’s response (ok or error).&lt;/p&gt;

&lt;p&gt;linearizability 需要 所有的 operations 都 “move forward in time”，也就是前面的 recency guarantee，
一旦 new value 读或者写了，后续的都是 new value。&lt;/p&gt;

&lt;p&gt;一些有趣的要点：
1.&lt;/p&gt;

&lt;h3 id=&#34;2-relying-on-linearizability&#34;&gt;(2) Relying on Linearizability&lt;/h3&gt;

&lt;p&gt;通过 Linearizability 能实现很多功能，&lt;/p&gt;

&lt;h3 id=&#34;3-implementing-linearizable-systems&#34;&gt;(3) Implementing Linearizable Systems&lt;/h3&gt;

&lt;p&gt;实现 Linearizable 最简单的方式就是真的只保存一份数据，但是我们肯定希望是分布式的。常见几种方式：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Single-leader replication (potentially linearizable) 一个 leader 能基本保证，但是设计上或者并发的bug可能出现问题。另外单节点 failover 问题。&lt;/li&gt;
&lt;li&gt;Consensus algorithms (linearizable)&lt;/li&gt;
&lt;li&gt;Multi-leader replication (not linearizable)&lt;/li&gt;
&lt;li&gt;Leaderless replication (probably not linearizable)&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&#34;linearizability-and-quorums&#34;&gt;Linearizability and quorums&lt;/h4&gt;

&lt;p&gt;很多网络延迟导致 quorums 是没法保证 Linearizability 的。&lt;/p&gt;

&lt;h4 id=&#34;the-cost-of-linearizability&#34;&gt;The Cost of Linearizability&lt;/h4&gt;

&lt;h4 id=&#34;the-cap-theorem&#34;&gt;The CAP theorem&lt;/h4&gt;

&lt;p&gt;CAP 理论的 C 指的就是这个 Linearizability，如果要想线性一致，就没法满足可用性。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>分布式存储-Transactions</title>
      <link>https://dengziming.github.io/post/data-intensive-architecture/transactions/</link>
      <pubDate>Mon, 31 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/data-intensive-architecture/transactions/</guid>
      
        <description>

&lt;h1 id=&#34;三-transactions&#34;&gt;三、Transactions&lt;/h1&gt;

&lt;p&gt;在一个数据系统中，很多东西可能出错，有可能是网络问题，硬件问题，系统 bug。为了可靠，我们必须让系统可靠，事务就是一个选择。
事务是我们设计应用的时候遵循的的一套规定，但实际上他并不是理所当然的，二是要通过设计底层算法来满足的，这次我们就看看这些算法。
这一节我们主要将重点放在单机的事务，下一节将会讨论分布式事务。&lt;/p&gt;

&lt;h2 id=&#34;1-the-slippery-concept-of-a-transaction&#34;&gt;1. The Slippery Concept of a Transaction&lt;/h2&gt;

&lt;p&gt;现在几乎所有的 relational databases，以及一些 NoSql 都支持 transaction，但是 transaction 的定义到底是什么？
实际上一些 NoSql 摆脱了事务的约束，因为事务有时候严重影响了扩展性。都是一些  trade-offs&lt;/p&gt;

&lt;h2 id=&#34;2-the-meaning-of-acid&#34;&gt;2. The Meaning of ACID&lt;/h2&gt;

&lt;p&gt;不同数据库的 ACID 实现方式是不同的，例如 isolation 的定义有很多，除了 ACID 还有一种约束叫做 BASE，就更加模糊了，一个一个看吧。&lt;/p&gt;

&lt;h3 id=&#34;1-atomicity&#34;&gt;(1) Atomicity&lt;/h3&gt;

&lt;p&gt;原子性其实有很多种定义，一般代表一个事物不可分，例如同时如果是多线程，另一个线程不可能看到转账中间的状态。&lt;/p&gt;

&lt;p&gt;但是在事务中，并不是这个意思，上面的问题是 隔离性讨论范畴，Atomic 是说，例如一个转账的事务，分为两步，不可能只完成一步而另一步失败。&lt;/p&gt;

&lt;p&gt;没有原子性保证，如果一个事物中途失败，我们没法知道哪些改变了哪些没改变。&lt;/p&gt;

&lt;h3 id=&#34;2-consistency&#34;&gt;(2) Consistency&lt;/h3&gt;

&lt;p&gt;Consistency 是一个被严重重载的词，前面的 replica 中它是说主备不一致；CAP 中指的是 线性一致性，这里指的是一种 good state。&lt;/p&gt;

&lt;p&gt;这个定义很模糊，因为他很难描述清楚。一是转账的时候，事务前后两个人的总金额要一样，
注册用户名的时候保证用户名不重复，不能违背外键约束，这都是一致性的问题。
AID 都是 properties of databases，数据库可能依赖 AID 来实现 Consistency，所以严格来说 Consistency 并不属于 ACID&lt;/p&gt;

&lt;h3 id=&#34;3-isolation&#34;&gt;(3) Isolation&lt;/h3&gt;

&lt;p&gt;大部分数据都是多线程操作的，如果多个线程访问了同一条数据，就会出现类似前面讨论的 concurrency-write 问题，但是这里还有 read 的问题。
有的经典课本将它改为 serializability，意思是每个线程都可以假设只有他一个线程在操作数据库，数据库保证并发执行的结果和顺序执行的结果是一样的。
实际上真正的数据库很少会让操作只在一个线程里面，这样性能太低。&lt;/p&gt;

&lt;h3 id=&#34;4-durability&#34;&gt;(4) Durability&lt;/h3&gt;

&lt;p&gt;这个就是说事务一旦提交就持久化了，例如已经存入了 nonvolatile storage，例如 hard drive or SSD，结合 write-ahead log 等方式实现。
在 replicated databases 中，可能意味着已经复制到了好几个副本之中。
perfect durability 是不可能存在的，如果你的机房被恐怖分子炸掉了，硬盘都没了。&lt;/p&gt;

&lt;p&gt;自从分布式理论有了以后， REPLICATION AND DURABILITY 就出现了一些变化，相关的可以查看资料。&lt;/p&gt;

&lt;h2 id=&#34;single-object-and-multi-object-operations&#34;&gt;Single-Object and Multi-Object Operations&lt;/h2&gt;

&lt;p&gt;上面我们知道了，atomicity 和 isolation 描述了在客户端执行多个操作时候 ，数据库的做法。
前者表示如果事务进行到一半失败了必须删掉所有修改的数据，后者表示各个线程互不干扰。接下来看一个例子。&lt;/p&gt;

&lt;p&gt;一个邮件系统需要显示未读邮件数量，如果每次用户登录都执行 count 操作会比较耗时，所以添加了一个变量 counter 表示未读邮件。&lt;/p&gt;

&lt;p&gt;这时候每次收到一个邮件就 counter++ ，读了一份邮件就 counter &amp;ndash;。
atomicity 能够保证 不会出现异常（例如已经没用邮件但是 counter &amp;gt;0）,不会出现 消息标记为已读但是 counter&amp;ndash; 执行失败。
如果标记已读的时候，突然收到一份邮件，这时候 counter 的 ++ 和 &amp;ndash; 是并发的，Isolation 保证不会出现异常。&lt;/p&gt;

&lt;p&gt;Multi-object transactions 需要决定哪个读和写操作是同一个 事务。
对于 relational，就是同一个 tcp 连接的一个 start 和 commit 之间的操作属于同一个事务。
但是对于 Nosql，并没有相应的方法，尽管提供了对于的API，但是实际上不一定能保证事务。&lt;/p&gt;

&lt;h3 id=&#34;1-single-object-writes&#34;&gt;(1) Single-object writes&lt;/h3&gt;

&lt;p&gt;如果一个事务修改了一个 json 文件，但是由于网络问题，修改了一半。或者多个事务同时修改了一个字段，这时候也和上面一样需要进行单个对象事务控制。&lt;/p&gt;

&lt;p&gt;一些数据库提供了 更复杂的 atomic 操作，例如 increment 可以避免 read-modify-write，类似的有 compare-and-set ，思想类似了乐观锁。
这样的操作实际上和 ACID 类似，compare-and-set 实际上也是 lightweight transactions，或者是市场化的 ACID。&lt;/p&gt;

&lt;h3 id=&#34;2-the-need-for-multi-object-transactions&#34;&gt;(2) The need for multi-object transactions&lt;/h3&gt;

&lt;p&gt;许多 NoSql 放弃了 multi-object transactions， 因为需要在不同的 partitions 上面实现事务。
我们真的需要 multi-object transactions 么？我们的应用能否只通过 single-object operations 实现。考虑以下情形：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;外键约束 关系型数据库可能有外键，我们执行一个操作的时候，需要考虑满足这些约束&lt;/li&gt;
&lt;li&gt;document databases 很难 join，所以会有 denormalization，对应的修改必须是同时修改。&lt;/li&gt;
&lt;li&gt;考虑 secondary indexes，值修改了索引也要进行修改。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这些情形可以不用事务，但是容错就会相当复杂。&lt;/p&gt;

&lt;h3 id=&#34;3-handling-errors-and-aborts&#34;&gt;(3) Handling errors and aborts&lt;/h3&gt;

&lt;p&gt;ACID 的主要目的是如果发生了错误，我可以完全丢弃掉这些改变，但是也有例外。
leaderless replication 在你的数据出现错误以后并不会进行修改。
如果发生了错误就直接 abort 然后重试也不是很好的选择：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;如果事务已经 succeeded，但是网络问题导致没有返回成功消息，你可能会标记为失败，然后重试会执行两次？&lt;/li&gt;
&lt;li&gt;如果集群 load 过高你这时候再次重试反而会是问题严重。&lt;/li&gt;
&lt;li&gt;有时候重试再多次也是失败的&lt;/li&gt;
&lt;li&gt;如果事物内部有一些操作，例如发邮件？你总不能每次重试就发一次邮件吧。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;2-weak-isolation-levels&#34;&gt;2. Weak Isolation Levels&lt;/h2&gt;

&lt;p&gt;上面说过 serializable 并不是最好的隔离方式，还有一些其他的方式。&lt;/p&gt;

&lt;h3 id=&#34;1-read-committed&#34;&gt;(1) Read Committed&lt;/h3&gt;

&lt;p&gt;Read Committed 有两个保证，1 是读数据的时候只能看到提交成功的数据，2 是写数据的时候只会覆盖写成功的数据。也就是没有脏读和脏写。&lt;/p&gt;

&lt;h4 id=&#34;no-dirty-reads&#34;&gt;No dirty reads&lt;/h4&gt;

&lt;p&gt;如果你读了别的事务还没提交的数据就是脏读，如果覆盖了别的事务还没提交的数据就是脏写。
举个例子，如果你的账户余额为0，查看余额的时候，有人刚好在给你转账，
先给你加了 500，然后他的账户余额不够事务失败了，然后又 rollback，结果你刷新发现又变成了0。你看到 500 是脏数据，所以是脏读。
同理如果你给自己存钱的时候覆盖了这个 500，就是脏写。&lt;/p&gt;

&lt;p&gt;Read Committed 隔离级别数据库需要保证不会出现脏读，也就是看到的数据是已经 commit 的。&lt;/p&gt;

&lt;h4 id=&#34;no-dirty-writes&#34;&gt;No dirty writes&lt;/h4&gt;

&lt;p&gt;同样需要保证不会出现脏写，一般需要加锁，等待前一个事务 commit 或者 abort&lt;/p&gt;

&lt;h4 id=&#34;implementing-read-committed&#34;&gt;Implementing read committed&lt;/h4&gt;

&lt;p&gt;简单粗暴的方式就是加锁，这也是大部分数据库 保证 没有 No dirty writes 的方式，但是脏读也加锁就会降低性能。
所以脏读一般是通过保存两份数据，一个是未提交的数据，一个是已经提交的数据。读的时候返回已经提交的数据。&lt;/p&gt;

&lt;h3 id=&#34;2-snapshot-isolation-and-repeatable-read&#34;&gt;(2) Snapshot Isolation and Repeatable Read&lt;/h3&gt;

&lt;p&gt;貌似 上面已经很完美了，但是看看下面的例子：
还是两个人转账，A 把 500 块转给 B，首先把 A 设置为 0，然后 B 设置为 500。
此时B 查看了两个用户的账户，在还没开始的时候查看了A的，结束的时候查看了B的，发现两个人都是 0.&lt;/p&gt;

&lt;p&gt;上面的问题和之前的反了过来。脏读是读了修改到一半的数据，不可重复读是修改了读到一半的数据。
这貌似不是一个大问题，只要等一会儿再读，就发现是正确的。但是注意以下情况：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Backups 如果我再备份数据，类似这里的读。&lt;/li&gt;
&lt;li&gt;Analytic queries&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Snapshot isolation 就是解决这个问题的方法。实现 快照隔离需要用 MVCC 算法。
之前每条数据都有两个版本，未提交的和提交的，现在改成多个，也就是每一条保存多个版本的数据，算法如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;每个事务都有一个 txid。&lt;/li&gt;
&lt;li&gt;每一行数据有一个 created_by 段包含了 tx_id。&lt;/li&gt;
&lt;li&gt;每一行数据有一个 deleted_by 字段默认空值，如果数据删除了，实际上不会删的，二是 deleted_by 会有一个 txid 标志它被删掉。&lt;/li&gt;
&lt;li&gt;如果确定没有实物能访问到被删掉的数据，就真的删了被删掉的数据。&lt;/li&gt;
&lt;li&gt;update 等于 delete + create&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;读数据时候的 Visibility rules 如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;每个 transaction 开始的时候会将所有正在执行的 transactions 列出，这些事务的修改无论成功与否都 ignore。&lt;/li&gt;
&lt;li&gt;所有 abort 的事务做的修改 都 ignore。&lt;/li&gt;
&lt;li&gt;比它大的 txid 做出的修改都 ignore。&lt;/li&gt;
&lt;li&gt;其他的修改都是 可见的。&lt;/li&gt;
&lt;li&gt;这里的修改指的是 create 和 delete&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;换个方式，只有下面的情况才是 visible：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;transaction 开始的时候，已经 commit 的数据&lt;/li&gt;
&lt;li&gt;别的事物虽然已经删了数据，但是还没有提交的。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;3-indexes-and-snapshot-isolation&#34;&gt;(3) Indexes and snapshot isolation&lt;/h3&gt;

&lt;p&gt;在 multi-version 的 databases 里面 index 如何工作? 可以和数据一样，有多个索引，但也可以在有多个版本的时候避免索引更新。
还有的结构是 appendOnly 的结构。。。。&lt;/p&gt;

&lt;h3 id=&#34;4-repeatable-read-and-naming-confusion&#34;&gt;(4) Repeatable read and naming confusion&lt;/h3&gt;

&lt;p&gt;Repeatable read 其实一开始是没有的，也是慢慢被发现的，所以它的名字有很多，实现方式也不一样，提供的一致性保证也不一定一样。&lt;/p&gt;

&lt;h2 id=&#34;3-preventing-lost-updates&#34;&gt;3. Preventing Lost Updates&lt;/h2&gt;

&lt;p&gt;前面只讨论了 dirty writes，类似我们讨论的并发写，还有一些问题我们没有讨论。最有名的就是  Lost Updates，例如并发给某个数据 +1.
一般可能出现的情况：
1. 查询数据并增加，例如：转账的时候同时转账。
2. 同时编辑某个 wiki 文档的一个部分。
3. 修改一个 json 字符串&lt;/p&gt;

&lt;h3 id=&#34;1-atomic-write-operations&#34;&gt;(1) Atomic write operations&lt;/h3&gt;

&lt;p&gt;大部分 relational db 都支持 atomic 写，即 &lt;code&gt;UPDATE counters SET value = value + 1 WHERE key = &#39;foo&#39;;&lt;/code&gt; 而不需要 read-modify-write。&lt;/p&gt;

&lt;p&gt;一般情况下都是通过 exclusive lock 方式实现，这种方式也被称为  cursor stability，另一种就是强制要求所有的操作在一个线程里。&lt;/p&gt;

&lt;p&gt;但是有些 orm 框架让你很容易写出 unsafe 的并发更新bug而且很难查出来。&lt;/p&gt;

&lt;h3 id=&#34;2-explicit-locking&#34;&gt;(2) Explicit locking&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;SELECT * FROM figures
  WHERE name = &#39;robot&#39; AND game_id = 222
  FOR UPDATE;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样直接锁住记录&lt;/p&gt;

&lt;h3 id=&#34;3-automatically-detecting-lost-updates&#34;&gt;(3) Automatically detecting lost updates&lt;/h3&gt;

&lt;p&gt;加锁的方式是让操作序列化执行，同样可以让每个并发执行，如果发生了错误就 abort 重试。
Lost update detection 是一种很高级的特性。&lt;/p&gt;

&lt;h3 id=&#34;4-compare-and-set&#34;&gt;(4) Compare-and-set&lt;/h3&gt;

&lt;p&gt;这种方式就是写之前先读，如果数据没有变化再写。
但是如果写之前读的是还没提交的数据，可能还是有问题的。&lt;/p&gt;

&lt;h3 id=&#34;5-conflict-resolution-and-replication&#34;&gt;(5) Conflict resolution and replication&lt;/h3&gt;

&lt;p&gt;如果有多个副本，写数据都是并发，同步数据是异步的，这时候 locks 和 compare-and-set 的方式以及没法保证数据一致性了。&lt;/p&gt;

&lt;h2 id=&#34;4-write-skew-and-phantoms&#34;&gt;4. Write Skew and Phantoms&lt;/h2&gt;

&lt;p&gt;除了上面的问题，可能还有新的问题。看下嘛的例子：
我们有个医生门诊应用，每个医生可以请假，但是至少保证有一个医生没有请假。医生请假的时候会查询目前在岗的医生，发现有人就可以请假。
但是如果有一个时间所有的医生都同时查看，发现大家都在，然后大家都一起请假，就违背了约束。&lt;/p&gt;

&lt;h3 id=&#34;1-characterizing-write-skew&#34;&gt;(1) Characterizing write skew&lt;/h3&gt;

&lt;p&gt;这种问题叫做 write skew，既不是脏写也不是 lost update。对应的读问题叫做 phantoms read。
因为两个事务更新的是两个数据（每个医生设置自己的状态为请假），这里的冲突不太明显，但是确实违背了约束。&lt;/p&gt;

&lt;p&gt;write skew 可以看出广义的 lost update，write skew 是多个事务读了同一个数据然后更新了其中的部分数据。
如果更新的是同一条数据就会得到 dirty write 或者 lost update。&lt;/p&gt;

&lt;p&gt;lost update 我们有很多种方案可以解决，但是貌似这里就不行了：
1. Atomic single-object 没法解决，因为 这里有多个 obj
2. automatic detection 很难，因为这种问题需要 Serializable 的隔离级别
3. 通过添加唯一位数、主键约束都不一定可以，例如这里有多个医生的在岗状态，如果加约束需要每次都扫描全表。&lt;/p&gt;

&lt;p&gt;还有很多 write skew 的问题，
会议室预约系统如何保证一个会议室在同一时间不会被两个人预定？
Multiplayer game 怎么保证两个用户不会出现在同一个点，
用户名唯一性约束等&lt;/p&gt;

&lt;h3 id=&#34;2-phantoms-causing-write-skew&#34;&gt;(2) Phantoms causing write skew&lt;/h3&gt;

&lt;p&gt;上面的问题有一个共同点：
1. 查询是否满足多个条件
2. 根据上面的结果做出更新
3. 第二步的更新导致第一步的条件不成立&lt;/p&gt;

&lt;p&gt;注意一定是有更改，如果是只读的， snapshot isolation 就能保证不会出现异常。&lt;/p&gt;

&lt;h3 id=&#34;3-materializing-conflicts&#34;&gt;(3) Materializing conflicts&lt;/h3&gt;

&lt;p&gt;貌似有的问题是我们没法加锁，例如第一个 不知道给哪个医生加锁？但是如果能够物化冲突，找到加锁的地方就能够解决了。&lt;/p&gt;

&lt;p&gt;会议室预定的，我们可以给每个会议室每个时间段创建一条记录，然后加锁。问题在于有的我们很难物化，例如用户名唯一性，
不可能给每个用户名都创建一个索引。大多数还是使用 Serializability 的隔离级别&lt;/p&gt;

&lt;h2 id=&#34;4-serializability&#34;&gt;4. Serializability&lt;/h2&gt;

&lt;p&gt;这是目前 strongest isolation level，让数据库表现的类似所有的事务都 execute in parallel。&lt;/p&gt;

&lt;p&gt;如何实现  implementing 并且它的效率如何是需要考虑的问题。目前三种实现：顺序执行、2PL、SSI&lt;/p&gt;

&lt;h3 id=&#34;1-actual-serial-execution&#34;&gt;(1) Actual Serial Execution&lt;/h3&gt;

&lt;h4 id=&#34;encapsulating-transactions-in-stored-procedures&#34;&gt;Encapsulating transactions in stored procedures&lt;/h4&gt;

&lt;h3 id=&#34;2-two-phase-locking-2pl&#34;&gt;(2) Two-Phase Locking (2PL)&lt;/h3&gt;

&lt;p&gt;很久一段时间，实现 serial 只有一种方法，那就是 2PL，注意这里的 2pl 和 后面的 2pc 不一样。&lt;/p&gt;

&lt;p&gt;我们知道锁的作用就是更新时候保证一个是在另一个完成后进行的。2PL类似，很多个线程都可以同时读一条数据，
但是如果想要更改数据，就需要使用 exclusive access ：
1. A 如果读了数据 然后 B 想更改这条数据，B 需要等待 A 事务 commit 或者 abort
2. A 如果修改了数据 然后 B 想读这条数据，B 需要等待 A 事务 commit 或者 abort&lt;/p&gt;

&lt;p&gt;2PL 和 前面的锁不一样的地方在于，writers 不仅会锁住其他的 writers，还会锁住其他的 readers，反过来也一样。
这就是和 snapshot isolation 不一样的地方，所以他能够防止前面所有的错误。&lt;/p&gt;

&lt;h4 id=&#34;implementation-of-two-phase-locking&#34;&gt;Implementation of two-phase locking&lt;/h4&gt;

&lt;p&gt;实现 2PL 的方式就是给每个记录加锁，锁可以是 shared 或者 exclusive。使用方式如下：
1. 如果线程 A 想读 一个记录，需要获得 shared mode 的锁。所谓 shared mode 就是多个 shared mode 的线程可以共用锁
2. 如果线程 A 想写 一个记录，需要获得 exclusive mode 的锁。所谓 exclusive mode，就是只能一个人获得锁
3. 如果一个事务先 读然后写，需要将对应的 shared mode 的锁 升级为 exclusive mode 的锁。
4. 一个事务获得了锁，就需要一个 hold 住，知道结束（commit或者 abort）.&lt;/p&gt;

&lt;p&gt;这就是 two-phase 的由来，phase1 就是 acquired lock(execute transaction), phase2 就是 release locks(end of the transaction)
由于使用了很多 locks，很容易出现 A 等待 B释放锁，同时 B 等待 A，这就是 死锁，数据库一般可以自动发现死锁。&lt;/p&gt;

&lt;h4 id=&#34;performance-of-two-phase-locking&#34;&gt;Performance of two-phase locking&lt;/h4&gt;

&lt;p&gt;一直依赖不用 2PC 的原因就是性能太差了，一方面获得锁释放锁有性能开销，更重要的还是并发度太低。
而且一旦出现事务等待，不知道要等待多久。另外死锁出现的概率大了很多。&lt;/p&gt;

&lt;h4 id=&#34;predicate-locks&#34;&gt;Predicate locks&lt;/h4&gt;

&lt;p&gt;上面的预定会议室程序，如果一个用户预定了一个某个时间段的某个地点，另一个用户此时还可以预约，只要不是相同的时间和地点。&lt;/p&gt;

&lt;p&gt;这时候我们可以使用 Predicate locks，和 2PL 类似，但是不一样的是 2PL 对查询的某一条数据加锁，
Predicate locks 是加在所有的符合某个条件的所有数据。&lt;/p&gt;

&lt;p&gt;这里的关键在于 Predicate locks 不仅可以加在目前存在的记录上面，还可以加在未来会写进来的数据上。
如果 2PL 包含了 Predicate locks，就可以防止任何形式的 phantom，也就成了 Serializable。&lt;/p&gt;

&lt;h4 id=&#34;index-range-locks&#34;&gt;Index-range locks&lt;/h4&gt;

&lt;p&gt;Predicate locks 并不好用，如果有太多 lock，判断每个lock 很耗时，所以很多 2PL 使用的是 index-range locking。&lt;/p&gt;

&lt;p&gt;例如我们要锁住 某个会议室在 明天下午三点 ，这判断起来很麻烦，我们可以直接将这个会议室锁起来！或者把所有的会议室 下午三点都锁起来。
由于 room_id 或者 time 可能有索引，所以判断起来会比较快。&lt;/p&gt;

&lt;p&gt;index-range locks 并没有 Predicate locks 精确，但是可以降低查询量，也是一个好的折中。
如果没有索引，就锁住整个表格，这样就会降低性能，但是也是一个折中。&lt;/p&gt;

&lt;h3 id=&#34;serializable-snapshot-isolation-ssi&#34;&gt;Serializable Snapshot Isolation (SSI)&lt;/h3&gt;

&lt;p&gt;前面我们发现，提供的隔离性越好，性能就越差？貌似性能和隔离性一定是 odds？不一定，有个 SSI 算法比较新，提供了很好的性能。&lt;/p&gt;

&lt;h4 id=&#34;pessimistic-versus-optimistic-concurrency-control&#34;&gt;Pessimistic versus optimistic concurrency control&lt;/h4&gt;

&lt;p&gt;2PL 是 pessimistic 悲观锁，如果可能会出错那么就等待安全了再操作，类似 mutual exclusion。
Serial execution ，就是极端的 pessimistic。&lt;/p&gt;

&lt;p&gt;serializable snapshot isolation 是一种 optimistic concurrency control technique.
先假定不会出错，然后 commit 的时候判断有没有出错。&lt;/p&gt;

&lt;h3 id=&#34;decisions-based-on-an-outdated-premise&#34;&gt;Decisions based on an outdated premise&lt;/h3&gt;

&lt;p&gt;我们前面出现的 write skew 的时候，都有一个通用的模式：
一个 transaction 读一些数据，检查结果并且做一些动作，但是提交数据的时候，这些数据可能已经被另一个事务更改。&lt;/p&gt;

&lt;p&gt;transaction 会基于 premise 做出一些 action，等后面准备 commit 的时候，这个 premise 已经不正确了。
换句话说，读的数据和写的数据之间有一个 causal dependency。如何发现 outdated premise，主要有两种：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Detecting reads of a stale MVCC object version (uncommitted write occurred before the read)&lt;/li&gt;
&lt;li&gt;Detecting writes that affect prior reads (the write occurs after the read)&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&#34;detecting-stale-mvcc-reads&#34;&gt;Detecting stale MVCC reads&lt;/h4&gt;

&lt;p&gt;在 snapshot isolation 中使用了 multi-version 的数据，每个事务不会读到当前还没有提交的数据，
所以其他的线程可能修改了这个数据，为了防着这种情况发生，db 需要记录由于 MVCC visibility rules 而忽略的数据，
当这个事务 commit 的时候，检查这些数据是否提交，如果提交了就 abort。&lt;/p&gt;

&lt;p&gt;为何要等待 commit 的时候判断而不是直接放弃？因为 当前事务可能是只读，或者另一个事务会失败，或者另一个事务会持续很久。&lt;/p&gt;

&lt;h4 id=&#34;detecting-writes-that-affect-prior-reads&#34;&gt;Detecting writes that affect prior reads&lt;/h4&gt;

&lt;p&gt;当一个 transaction 写数据的时候，他需要查看最近读了这个数据的事务，通知这些事务他们的他们的数据已经过时了。&lt;/p&gt;

&lt;h4 id=&#34;performance-of-serializable-snapshot-isolation&#34;&gt;Performance of serializable snapshot isolation&lt;/h4&gt;

&lt;p&gt;和 2PL 相比，SSI 不用阻塞线程。读写之间是不会阻塞，导致延迟低。&lt;/p&gt;

&lt;p&gt;aborts rate 对性能影响很大。但是 SSI 对 长事务 的敏感程度 肯定比 2PL 和 real serial 低。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>分布式存储-replication</title>
      <link>https://dengziming.github.io/post/data-intensive-architecture/replication/</link>
      <pubDate>Mon, 31 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/data-intensive-architecture/replication/</guid>
      
        <description>

&lt;h1 id=&#34;一-replication&#34;&gt;一、Replication&lt;/h1&gt;

&lt;p&gt;Replication 就是将你的数据放在多个节点，这样有很多好处。数据从地理上可以隔用户更近，保持高可用，增加吞吐量。&lt;/p&gt;

&lt;p&gt;本章我们假设数据都是完整的进行副本存放，后面再讲分区。数据的副本解决难点就是数据变化的一致性，如果数据一致不变，那就太简单了，没什么可以讨论的。&lt;/p&gt;

&lt;p&gt;对于数据变化的副本存储，有很多需要注意的地方，例如每次写数据操作数据复制是同步的还是异步的，节点挂了怎么办，数据一致性问题，时效性问题。
我们的学习主要分为三种设计：single-leader, multi-leader, and leaderless&lt;/p&gt;

&lt;h2 id=&#34;1-leaders-and-followers&#34;&gt;1. Leaders and Followers&lt;/h2&gt;

&lt;p&gt;每个节点上面的副本叫做 replica ，当有 multi replica 的时候，一个问题出现了，怎么保证数据写到了每一个副本。
数据库的每一次写都必须保证被每个副本处理，否则就会出现不一致。
最常用的解决方案就是 leader-based-replication，也叫 active-passive、master-slave。&lt;/p&gt;

&lt;p&gt;在主从结构中，写只能发送给 leader，leader 写的时候会发送数据给 follower，而读操作可以读任何一个节点。&lt;/p&gt;

&lt;h3 id=&#34;1-synchronous-versus-asynchronous-replication&#34;&gt;(1) Synchronous Versus Asynchronous Replication&lt;/h3&gt;

&lt;p&gt;replicated 系统重要特点就是 同步写还是异步写，一般关系型数据库可以配置，其他的都是硬编码写死的。&lt;/p&gt;

&lt;p&gt;客户端发送数据给leader，然后leader转发给follower，最后通知client。如果 leader 收到 follower 的确认消息再回复client，这就是同步。
如果leader发送给了 follower后就直接回复client，就是异步。一般情况，系统都只有一个follower是同步，其余的都是异步。这叫 semi-synchronous。&lt;/p&gt;

&lt;p&gt;为了效率经常设置为 Asynchronous，这样如何 leader 失败了而且不恢复，那还没有被副本保存的数据就丢失了，
这样的好处就是可以提供持续服务，尽管follower很慢。即便是配置使用 Synchronous，实际上也是有一台是同步，其余的是异步。&lt;/p&gt;

&lt;p&gt;Weakening durability 可能是一个比较糟糕的 trade-off，但是 Asynchronous 还是很常用，尤其是地理上分布式的集群。&lt;/p&gt;

&lt;p&gt;当然也有很多其他的方案在研究中，例如 Azure 使用了 chain replication。&lt;/p&gt;

&lt;h3 id=&#34;2-setting-up-new-followers&#34;&gt;(2) Setting Up New Followers&lt;/h3&gt;

&lt;p&gt;系统有时候需要添加新的节点，如何保证新节点和leader的数据一致呢？直接复制数据过来肯定是不行的，leader 一直处在 flux 中，一般步骤有四步：
1. leader 在某个确定一致性的时间点拍一个 snapshot 而不用锁住系统，大部分数据库都提供了这个功能，有的能通过第三方工具提供这个功能。
2. follower 将数据复制过去。
3. follower 将从这个时间点以后的数据复制过去，这需要拍 snapshot 时候的日志位置，这个点的名字叫 Log sequence number 或者 binlog coordinates。
4. 当 follower 将所有的change都同步后，我们称之为 catch up，就可以和其他follower一样，处理 leader 发生的变化了。&lt;/p&gt;

&lt;p&gt;真正的步骤其实是有很多不同，有时候可以自动配置，有时候需要手动配置。&lt;/p&gt;

&lt;h3 id=&#34;3-handling-node-outages&#34;&gt;(3) Handling Node Outages&lt;/h3&gt;

&lt;p&gt;有时候节点会挂掉，可能是我们不知道的原因，也可能是为了安装系统模块而重启，这时候应该怎么保证集群的高可用&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;follower 挂掉&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;follower 挂点然后重启的步骤是很简单的，类似前面的添加新节点，启动以后，根据日志能找到最后一个处理的事务，然后从leader请求这个时间点以后的更改，然后进行更改，完成后就 catch up。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;leader failover&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;leader 挂掉后可能麻烦一点，必须马上选择一名新的leader，这个过程被称为 failover&lt;/p&gt;

&lt;p&gt;failover的具体步骤有4：
1. 确定leader失败了，失败的原因很多，磁盘、电源等等，目前用的最多的是 timeout 方法，节点相互 bounce message，如果突然在某个时间点收不到，就是失败了。
2. 选择新的 leader，需要在大多数的follwer 进行选举，最好是选一个数据最新的。所有的节点同意这个节点成为 consensus。
3. 告知系统使用新的leader，client 发送数据到新的 leader，旧的 leader 回复后也要认同新的 leader。&lt;/p&gt;

&lt;p&gt;failover 总是会有些不期而遇的问题：
1. 如果是 asynchronous replica ，可能遇到一些数据没有同步过来，导致数据少了，如果这时候 older leader起来了，就会发生错误，这时候一般选择 discard 这部分数据。
2. 如果系统和别的系统进行结合，discard 是很危险的，github 发生了一次 mysql leader挂掉，然后discard 了一部分，删掉了部分自增的primary key，可是这部分pk已经保存到 redis了，导致了错误
3. 有时候两个节点都认为自己是 leader，这个叫做 split brain，这时候一半需要去掉一个，但是有可能两个都被去掉了。
4. timeout 的具体值多少合适？如果太长了，可能导致发现的晚。如果太短了可能误判，尤其是系统压力大的时候，还重新换一次会造成更大的压力。&lt;/p&gt;

&lt;p&gt;这些问题解决起来还是很棘手的，所以有很多算法，我们后续会介绍。&lt;/p&gt;

&lt;h3 id=&#34;4-implementation-of-replication-logs&#34;&gt;(4) Implementation of Replication Logs&lt;/h3&gt;

&lt;p&gt;上面各种恢复，添加，都涉及到日志，日志记录了数据的更改，实际上我们学习 Hadoop 的时候，也知道里面有日志，现在我们来研究一下，日志怎么实现。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Statement-based replication&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;基于 statement 的 replication，这是最简单的方法，数据库记录每一次更新，也就是说，你的 UPDATE、INSERT、DELETE都会记录到日志中，然后发送给follower。
这样的问题是：
对于 current_timestamp、random 这样的方法返回值不一样，基于判断的例如 update &amp;hellip; where &amp;hellip; 必须按照顺序，一些触发器、存储过程等 side effects。&lt;/p&gt;

&lt;p&gt;对于这些问题，当然也有特定的解决方案，例如先计算一些可能导致不一致的函数。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;WAL-log&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;前面讲过 B-Tree 和 LSM-Tree，都是基于WAL-log，是一种只能 append 的二进制日志，leader 不仅将 wal-log 保存到磁盘，而且发送到 followers。&lt;/p&gt;

&lt;p&gt;WAL 的 disadvantage 是太底层，wal 包含了磁盘那个位置的 block 需要修改哪些 bytes，这样耦合性比较高，如果format 改变了，基本上很难让 leader和follower 使用不同版本。&lt;/p&gt;

&lt;p&gt;这个问题看起来更像是实现的问题，如果 replication 的协议支持follower 使用新的版本，就能完成 zore-downtime 的 upgrade。
首先更新 follower的版本，然后让leader failover，选择一个 follower 为 leader。如果replication 的协议不支持版本不一直，这就叫做 wal-shipping，会造成 upgrade-downtime。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Logical (row-based) log replication&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这种方式就是 replication log 和具体的 storage device 解耦，使用不同格式。 relational database 的 logical log 一般都是一系列表示行的recorders。一个改变很多行的 statement 会
产生很多行这样的日志，后面有一个transaction 完成的标识符。这种方式是解耦的，所以可以容忍不同的版本，甚至不同的存储引擎。这也让外部的系统容易获得数据，
例如数仓、二级索引，外部缓存，这个技术叫做 change data capture。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;trigger based replication&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;前面讲的都是数据库系统的，有时候我们需要更灵活的，例如你只想一部分数据，或者你想从一个数据库迁移到另一个数据库。这样你就应该将replication 提升到应用程序层面。
很多数据库提供了工具完成这个事情。许多关系型数据库提供了一项功能：triggers and stored procedures。
trigger 能让你注册用户代码，一旦数据发生改变，将会将数据放到另一给表格，外部系统可以访问。&lt;/p&gt;

&lt;h3 id=&#34;5-problems-with-replication-lag&#34;&gt;(5) Problems with Replication Lag&lt;/h3&gt;

&lt;p&gt;之前说了 Replication 是保证数据安全、忍受 node failures，实际上还可以提高 scalability。&lt;/p&gt;

&lt;p&gt;Leader-based 需要所有的 write 请求都通过 master，将 read 分发到 slave，增加节点就能增加扩展性，
节点太多了就只能使用 异步的数据同步方式，否则可用性就很差。&lt;/p&gt;

&lt;p&gt;然而一旦 slave 的数据 fallen behind 了，可能访问的数据过时了，这看起来问题不大，毕竟等等就行，这叫做  eventual consistency。&lt;/p&gt;

&lt;p&gt;这个 eventually 是一个很NB的词，并没有告诉你具体时间，只是说最终会一致。我们称时间差为 the replication lag。这个 lag导致的问题我们接下来会讨论。&lt;/p&gt;

&lt;h4 id=&#34;1-read-your-own-writes&#34;&gt;1. Read your own writes&lt;/h4&gt;

&lt;p&gt;之前的 qq 空间有这个问题，当你发表了评论后页面会刷新，但是刷新后看不到自己发表的评论。
这时候你以为评论失败了，所以重新评论一下，然后刷新发现了自己评论了两次。&lt;/p&gt;

&lt;p&gt;这种问题被叫做 “read-after-write consistency”，也叫做 “read-your-writes consistency”。有一些解决方案：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;如果是用户可以修改的信息，从 master 读。&lt;/li&gt;
&lt;li&gt;如果是一分钟之内修改过的数据，从 master 读，或者可以监控 lag 大于1分钟的follow 排除。&lt;/li&gt;
&lt;li&gt;客户端保存最近一次写的 timestamp，这样服务端保证读的时候先等 slave 的数据同步已经到了这个时间。这里的 timestamp 可以是逻辑时间。&lt;/li&gt;
&lt;li&gt;如果有多个数据中心，可能更复杂。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;上面的解决方案只是一个 client，如果用户同事有多个 client 访问，例如手机和pc，这时候要考虑更多问题。&lt;/p&gt;

&lt;h4 id=&#34;2-monotonic-reads&#34;&gt;2. Monotonic Reads&lt;/h4&gt;

&lt;p&gt;类似上面，如果连续两次访问来自于不同的 slave，可能出现时间倒退，例如看球赛的时候刚开始比分 1:1,刷新一下变成 1:0 了！&lt;/p&gt;

&lt;p&gt;Monotonic Reads 就是保证上面的异常不发生的一致性。他比 eventually consistency 更强，比 strong consistency 更弱。&lt;/p&gt;

&lt;p&gt;这种一致性的解决方案就是保证每个 userid 都从同一个 slave 读数据，例如根据 userid 的 hash。&lt;/p&gt;

&lt;h4 id=&#34;3-consistent-prefix-reads&#34;&gt;3. Consistent Prefix Reads&lt;/h4&gt;

&lt;p&gt;在一个群聊系统，如果你发现两个人的对话顺序反了，就是 Consistent Prefix Reads 没有得到保证。&lt;/p&gt;

&lt;p&gt;这个问题解决方案需要保证有相互依赖的数据都按顺序写入。后续再  causal dependencies 和 happens-before 会讨论。&lt;/p&gt;

&lt;h3 id=&#34;6-solutions-for-replication-lag&#34;&gt;(6). Solutions for Replication Lag&lt;/h3&gt;

&lt;p&gt;后续会讲解 事务 和 分布式事务。&lt;/p&gt;

&lt;h2 id=&#34;2-multi-leader-replication&#34;&gt;2. Multi-Leader Replication&lt;/h2&gt;

&lt;p&gt;一个 leader 可能会有一些问题，例如写太多了可能会有性能问题。也有一些其他的选择， 例如多个 leader 。&lt;/p&gt;

&lt;h3 id=&#34;1-use-cases-for-multi-leader-replication&#34;&gt;(1). “Use Cases for Multi-Leader Replication”&lt;/h3&gt;

&lt;p&gt;一般情况如果只有一个 datacenter 是不需要用多个 leader 的，所以在多个 datacenter 的时候会使用 multi-leader&lt;/p&gt;

&lt;h4 id=&#34;multi-datacenter-operation&#34;&gt;Multi-datacenter operation&lt;/h4&gt;

&lt;p&gt;如果有多个 datacenter ，那么每个 datacenter 一个 leader 是一个不错的选择。
我们对比一下在 multi-datacenter 中使用 single-leader 和    multi-leader 的fare&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Performance ，multi-datacenter 的性能可能好一些&lt;/li&gt;
&lt;li&gt;Tolerance of datacenter outages ，multi-datacenter 不用担心leader 没了，所以更简单一点&lt;/li&gt;
&lt;li&gt;Tolerance of network problems，由于 datacenter 之间的网络稳定性肯定不如 datacenter 内部，所以 multi-leader 更好点&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;总之，如果有多个 datacenter ，选择 multi-leader。尽管如此也会有缺点，
例如需要同时修改一份数据，可能发生冲突导致一边成功一边失败了，后面的 handle write conflict 会讨论。&lt;/p&gt;

&lt;p&gt;同时，multi-leader 可能会有一些陷阱、和数据库的其他特性放一起经常出问题，例如主键自增。所以 multi-leader 需要尽量避免。&lt;/p&gt;

&lt;h4 id=&#34;clients-with-offline-operation&#34;&gt;“Clients with offline operation”&lt;/h4&gt;

&lt;p&gt;以 印象笔记、icloud 为例，我们在手机、电脑 的笔记可以在线编辑，也可能离线编辑。
这个架构就和 Multi-Leader 类似，每个设备是一个 datacenter，服务端也是一个 datacenter，网络连接并不靠谱。&lt;/p&gt;

&lt;h4 id=&#34;collaborative-editing&#34;&gt;Collaborative editing&lt;/h4&gt;

&lt;p&gt;以 Google docs、wiki 为例，大家可以同事协同编辑，相关的算法就是 automatic conflict resolution。
你可能以为这个和 multi-leader 的 replication 不一样，实际上很类似。
一个人编辑的时候，change 马上就出现在 local replica ，然后是异步同步到服务器和其他的人。
如果你希望保证没有冲突，Application 需要在文档上面加锁，其余人获得锁之前不能编辑，这种就类似与 single-leader 的replica。
但是为了快速工作，你可能将锁分的很小，大家可以同时编辑，这就类似 multi-leader ，同时带来了 conflict resolution。&lt;/p&gt;

&lt;h4 id=&#34;handling-write-conflicts&#34;&gt;Handling Write Conflicts&lt;/h4&gt;

&lt;p&gt;如果两个人同时编辑，一个修改了标题一个修改了内容这不会有冲突，但是如果两个人都修改了标题该如何解决这个问题。
如果是 single-leader，并不会有这种问题，因为写都是有时间顺序的。后面的那个要么等待前面的修改事务成功或者失败，要么自己让让写任务失败。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;“Synchronous versus asynchronous conflict detection”&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如果两个任务修改同一个数据都成功了，然后异步发现了冲突，这时候解决已经晚了，因为不知道放弃哪个。
当然原则上也可以通过同步修改的方式避免冲突，这样就和 single-leader 没什么区别了。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Conflict avoidance&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;避免冲突是目前最好的方法了，例如用户可以修改自己信息的话，就让所有的修改都发送到同一个 datacenter 的 leader。&lt;/p&gt;

&lt;p&gt;有时候也可能某个 datacenter 挂了或者用户去了另一个地方需要 datacenter 换了，需要使用别的方法避免冲突。这时候还是需要处理冲突。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Converging toward a consistent state&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于 single-leader 的 writes 都有一个 sequential order，统一数据的最后一次写决定了数据。
对于 multi-leader 就不一样了，并不知道哪个 datacenter 的才是正确的，如果不进行修改就会出现各个 datacenter 不一致，
所以所有的更改必须 最终是 convergent ，也就是说 最终一致性。几种常见的 convergent 方法：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;简单粗暴，直接每个操作一个 uuid，例如 hash，或者时间戳，最大的那个胜出，如果使用时间戳，这种方法就叫做 Last-Write—wins（LLW）。&lt;/li&gt;
&lt;li&gt;给每个 replica 一个 id，id 最高的占主导地位，和上面的类似。&lt;/li&gt;
&lt;li&gt;合并数据，例如 一个标题是A，一个是B，最后就是 A|B&lt;/li&gt;
&lt;li&gt;用户自己实现冲突合并逻辑。&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
&lt;li&gt;Custom conflict resolution logic&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;自己实现冲突解决一般有两种方案：On Read 和 On Write ，相关细节比较简单&lt;/p&gt;

&lt;h4 id=&#34;what-is-a-conflict&#34;&gt;What is a conflict?&lt;/h4&gt;

&lt;p&gt;上面说的两个人同时修改标题的冲突很明显，所以可以通过一些程序控制，实际上也有一些不明显的的冲突。&lt;/p&gt;

&lt;p&gt;例如两个人预定会议室，同一个会议室同一时间只能有一个人预定成功，但如果是 multi-leader 可能两个人都预订成功了。
或者注册的时候要求 username 不重复，但是有可能两个人同时申请同一个 username。&lt;/p&gt;

&lt;h3 id=&#34;2-multi-leader-replication-topologies&#34;&gt;(2) Multi-Leader Replication Topologies&lt;/h3&gt;

&lt;p&gt;Replication Topologies 是用来描述数据在节点间传播的路径。例如 circular 、star、all-2-all。&lt;/p&gt;

&lt;p&gt;每一种都有优势和劣势。&lt;/p&gt;

&lt;h2 id=&#34;3-leaderless-replication&#34;&gt;3. Leaderless Replication&lt;/h2&gt;

&lt;p&gt;实际上 cassandra 采用了 leaderless 的架构，没有主节点。&lt;/p&gt;

&lt;h3 id=&#34;1-writing-to-the-database-when-a-node-is-down&#34;&gt;(1) Writing to the Database When a Node Is Down&lt;/h3&gt;

&lt;p&gt;leaderless 中，不存在 failover，因为没有主节点，因为一个节点在读数据的时候，并不需要从所有的节点都读数据，
二是给每个节点发送数据，然后每个节点都返回数据，根据 Version numbers 决定哪个是正确的。&lt;/p&gt;

&lt;h4 id=&#34;read-repair-and-anti-entropy&#34;&gt;Read repair and anti-entropy&lt;/h4&gt;

&lt;p&gt;系统需要保证每个 replica 上面的数据都是最终一致的，如果出现了节点挂掉然后恢复，如何保证它的数据是一致？&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Read repair ，当用户读书节的时候，能够发现 stale 的值并且修复&lt;/li&gt;
&lt;li&gt;Anti-entropy process 通过后台进程扫描数据发现不一致&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&#34;quorums-for-reading-and-writing&#34;&gt;Quorums for reading and writing&lt;/h4&gt;

&lt;p&gt;也就是上面所说的，读和写不需要全成功，只要保证 读成功数 r 和 写成功数 w 满足 r+w&amp;gt;n即可。这个叫做仲裁原理，类似抽屉原理，
n个抽屉，里面有 w 个抽屉放了一样的纸条，打开r个抽屉，想要看到纸条内容，只要 r + w &amp;gt;n。
一般情况下 r=w=(n+1)/2&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Limitations of Quorum Consistency&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;上面我们说了 r=w=(n+1)/2，但是如果你的数据库写的量特别小，读特别大，你也可以设置 r=n，w=1，总之只要覆盖即可。&lt;/p&gt;

&lt;p&gt;如果你对一致性要求不高，也可以使用 w+r &amp;lt;= n，这样返回值可能 stale，不过这样的可用性更高。&lt;/p&gt;

&lt;p&gt;然而尽管使 w+r &amp;gt; n，也会有一些其他的异常情况：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;sloppy quorum 下无法保证&lt;/li&gt;
&lt;li&gt;concurrently 写，和上一节类似，出现了写冲突？&lt;/li&gt;
&lt;li&gt;读和写 concurrently，这时候写只提交了一部分，其他的可能还未完全在 w 个节点成功&lt;/li&gt;
&lt;li&gt;如果写 成功不够w，但是这时候也没法 rollback，后续可能会读到失败的值，也可能读不到&lt;/li&gt;
&lt;li&gt;如果保存了 new value 的节点失败重启，从另一个保存了 old value 的节点修复数据，导致包含 new value 数据的节点少于 n&lt;/li&gt;
&lt;li&gt;还有极端情况，例如  “Linearizability and quorums”.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;因此，即便是 达到了 quorum 的条件，也没法真正的实现一致性。
特别的，至于在 single-leader 中的 “reading your writes, monotonic reads, consistent prefix reads” 等弱一致性也没法得到保障。
所以前面说的异常可能也会发生，强一致性需要 consensus 或者 transaction 来保证。&lt;/p&gt;

&lt;h4 id=&#34;monitoring-staleness&#34;&gt;Monitoring staleness&lt;/h4&gt;

&lt;p&gt;对于应用而言，是否返回最新版本的数据是很重要的。对于 leader-based 的设计，所有的问题都可以交给 replication-lag 来解决，
可以通过监控 replication-lag 监控系统
对于 leaderless 的设计，所有的数据并没有确定的顺序，监控很困难，而且如果没有 anti-entropy，数据可能过时很久都不修复。&lt;/p&gt;

&lt;h3 id=&#34;sloppy-quorums-and-hinted-handoff&#34;&gt;“Sloppy Quorums and Hinted Handoff”&lt;/h3&gt;

&lt;p&gt;简单介绍一下，例如 cassandra 中，保存三副本，读写都要成功 2 个 replica，我们写数据要成功 2个，
但是如果此时这三个节点挂了两个，只能写成功一个，可以先将另一份数据放到其他的节点上。
等对应的剩下的节点都恢复了，再将数据放回去，这个过程就是 hinted handoff。&lt;/p&gt;

&lt;h3 id=&#34;multi-datacenter-operation-1&#34;&gt;Multi-datacenter operation&lt;/h3&gt;

&lt;p&gt;这时候我们还是有N个副本，我们需要保证每在 每个 datacenter 都达到了 对应的 n/2 个副本。&lt;/p&gt;

&lt;h3 id=&#34;detecting-concurrent-writes&#34;&gt;Detecting Concurrent Writes&lt;/h3&gt;

&lt;p&gt;类似之前的 handling write conflict，也是由于顺序不一样导致，所以也需要有保证 converge 的方法，
这样的方案也就是我们前面说的一样。但是我们这里讨论的更细致一点。&lt;/p&gt;

&lt;h4 id=&#34;last-write-wins-discarding-concurrent-writes&#34;&gt;Last write wins (discarding concurrent writes)&lt;/h4&gt;

&lt;p&gt;对于不知道哪个 happens-first 的操作，我们称之为 concurrent，他们的 order 是不知道的。&lt;/p&gt;

&lt;p&gt;尽管他们没有 order，我们可以人为给一个 order，例如 timestamp，也就是 Last write wins。&lt;/p&gt;

&lt;p&gt;llw 解决了冲突问题，但是丢失了一些数据。所有使用 cassandra 的时候有人会给每个 key 后面加一个 uuid，这样能保证每个 key 多次写都不会丢失。&lt;/p&gt;

&lt;h4 id=&#34;the-happens-before-relationship-and-concurrency&#34;&gt;The “happens-before” relationship and concurrency&lt;/h4&gt;

&lt;p&gt;happens-before 的关系是决定是否并发的关键，以下情况下我们称两个操作是 concurrency：
“neither happens before the other (neither knows about the other)”&lt;/p&gt;

&lt;p&gt;因此，对于任何两个操作 A 和 B，只有两种可能，A happens-before B，B happens-before A，A 和 B 是 concurrency。&lt;/p&gt;

&lt;h4 id=&#34;capturing-the-happens-before-relationship&#34;&gt;Capturing the happens-before relationship&lt;/h4&gt;

&lt;p&gt;上面已经知道两个操作只能有 3 种关系，我们如果发现 happens-before  的关系呢？其实有算法的。&lt;/p&gt;

&lt;p&gt;首先假如 A 和 B两个人都在往同一个购物车添加商品，一开始 A添加了 商品1，B添加了商品 2，这两个是 concurrency的，
然后 A再添加 3，B添加4，这时候 这两个操作都是 3和4是 concurrency的，但是1和2 都是 happens-before 3 和4 的。&lt;/p&gt;

&lt;p&gt;发现 concurrency 关系的算法如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;server 的每个key有一个 version number，每次更新写这个 key 的时候，version number 就 +1并且跟新数据。&lt;/li&gt;
&lt;li&gt;当 client 读数据，返回所有没有 overwritten 的数据和最新的 version number。&lt;/li&gt;
&lt;li&gt;client 写数据的时候，必须包含之前读到的那个 version number，并且合并所有的数据，&lt;/li&gt;
&lt;li&gt;当 server 收到带 version number 的写请求，覆盖所有小于等于 version number 的数据，保留大于 version number 的数据。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这样的算法就能发现所有的 happens-before 关系&lt;/p&gt;

&lt;h4 id=&#34;merging-concurrently-written-values&#34;&gt;Merging concurrently written values&lt;/h4&gt;

&lt;p&gt;类似前面的 conflict resolution，上面的算法需要 client 进行数据合并，合并算法大家自己查资料，例如 CRDT 。&lt;/p&gt;

&lt;h4 id=&#34;version-vectors&#34;&gt;Version vectors&lt;/h4&gt;

&lt;p&gt;上面的算法是 single-replica 情况，对于 multi-replica 的情况，需要每个副本一个 Version number，这被叫做 Version vectors。&lt;/p&gt;

&lt;h2 id=&#34;summary&#34;&gt;SUMmary&lt;/h2&gt;

&lt;p&gt;通过上面的讨论，我们明白了一句话，其实分布式 replica 最大的问题就是发现 happens-before 的依赖关系，也就是操作的顺序。&lt;/p&gt;

&lt;p&gt;另外有大神说过，分布式只有两个问题， exactly-once 和 order。其中 exactly-once 后续会讨论，order 就是我们这里讨论最多的。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>design-data-intensive-architecture</title>
      <link>https://dengziming.github.io/post/data-intensive-architecture/design-data-intensive-architecture/</link>
      <pubDate>Sat, 22 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/data-intensive-architecture/design-data-intensive-architecture/</guid>
      
        <description>

&lt;h1 id=&#34;第一部分-前言&#34;&gt;第一部分、前言&lt;/h1&gt;

&lt;p&gt;ACID，bigdata，NoSql ，bigTable ，CAP，2PC，3PC，quorum，raft，paxos，cloud，real-time，高并发架构将会逐渐成为基础。&lt;/p&gt;

&lt;p&gt;主要内容：
（一）基础&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;“reliability, scalability, and maintainability”&lt;/li&gt;
&lt;li&gt;数据模型&lt;/li&gt;
&lt;li&gt;存储引擎&lt;/li&gt;
&lt;li&gt;数据结构&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;（二）分布式&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;replication&lt;/li&gt;
&lt;li&gt;partitioning/sharding&lt;/li&gt;
&lt;li&gt;transaction&lt;/li&gt;
&lt;li&gt;distributed system&lt;/li&gt;
&lt;li&gt;consistency and consensus&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;（三）交互&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;batch process&lt;/li&gt;
&lt;li&gt;realtime&lt;/li&gt;
&lt;li&gt;put everything together&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;（四）案例及资料&lt;/p&gt;

&lt;h1 id=&#34;第二部分-数据系统的基础&#34;&gt;第二部分、数据系统的基础&lt;/h1&gt;

&lt;h2 id=&#34;一-reliable-scalable-和-maintainable-的含义&#34;&gt;一、Reliable,Scalable 和 Maintainable 的含义&lt;/h2&gt;

&lt;p&gt;数据系统分为 data-intensive 和 compute-intensive，一般的大数据应用会采用的技术：
1. 存储，方便后续的查询 &amp;ndash; databases
2. 缓存，加速访问 &amp;ndash; caches
3. 允许用户通过关键字搜索、按照过滤  &amp;ndash; indexes
4. 给另一个进程发送消息，异步处理  &amp;ndash; streaming process
5. 周期性处理全量数据  &amp;ndash; batch process&lt;/p&gt;

&lt;p&gt;这一切都是需要数据系统，我们并不会从头开始搭建存储、计算系统，因为有很多现成的技术可以应用。数据库、消息队列功能都很相似，但是实现完全不同，导致不同的性能，所以我们用处不同，我们为什么要将他们结合到一起呢？&lt;/p&gt;

&lt;p&gt;其实许多工具功能比较重复，redis 其实也可以当消息队列，kafka 也可以做数据持久化。这些工具的界限越来越模糊。第二是我们得系统一般比较复杂，一个工具完成不了，我们必须组件一个数据系统。组件的过程
有很多需要考虑的事情，其中最重要的三点：
Reliable： 系统必须一致能够提供服务，即便是发生了一些不可预知的错误
Scalable： 系统升级、功能扩展、访问量增加，导致我们需要能够很方便扩展我们的系统
Maintainable： 随着时间推移，可能有很多其他人接受项目，必须方便别人进行维护&lt;/p&gt;

&lt;p&gt;有关这三点的介绍，我们逐渐展开：&lt;/p&gt;

&lt;h3 id=&#34;1-reliable&#34;&gt;1. Reliable&lt;/h3&gt;

&lt;h3 id=&#34;2-scalable&#34;&gt;2. Scalable&lt;/h3&gt;

&lt;h3 id=&#34;3-maintainable&#34;&gt;3. Maintainable&lt;/h3&gt;

&lt;h2 id=&#34;二-数据模型和查询语言&#34;&gt;二、数据模型和查询语言&lt;/h2&gt;

&lt;p&gt;数据模型是最重要的一部分，&lt;/p&gt;

&lt;h3 id=&#34;1-关系型与文档型&#34;&gt;1.关系型与文档型&lt;/h3&gt;

&lt;h3 id=&#34;2-nosql&#34;&gt;2.NoSQL&lt;/h3&gt;

&lt;h3 id=&#34;3-object-relational-mismatch&#34;&gt;3. Object-Relational Mismatch&lt;/h3&gt;

&lt;h3 id=&#34;4-many-to-one-and-many-to-many-relationships&#34;&gt;4. Many-to-One and Many-to-Many Relationships&lt;/h3&gt;

&lt;h3 id=&#34;5-query-languages-for-data&#34;&gt;5. Query Languages for Data&lt;/h3&gt;

&lt;h3 id=&#34;6-graph-like-data-models&#34;&gt;6.Graph-Like Data Models&lt;/h3&gt;

&lt;h2 id=&#34;三-storage-and-retrieval&#34;&gt;三、Storage and Retrieval&lt;/h2&gt;

&lt;h3 id=&#34;1-hash-indexes&#34;&gt;1. Hash Indexes&lt;/h3&gt;

&lt;h3 id=&#34;2-sstables-and-lsm-trees&#34;&gt;2. SSTables and LSM-Trees&lt;/h3&gt;

&lt;h3 id=&#34;3-b-trees&#34;&gt;3. B-Trees&lt;/h3&gt;

&lt;h3 id=&#34;4-multi-column-indexes&#34;&gt;4. Multi-column indexes&lt;/h3&gt;

&lt;h3 id=&#34;5-full-text-search-and-fuzzy-indexes&#34;&gt;5. Full-text search and fuzzy indexes&lt;/h3&gt;

&lt;h3 id=&#34;6-keeping-everything-in-memory&#34;&gt;6.Keeping everything in memory&lt;/h3&gt;

&lt;h3 id=&#34;7-olap-vs-oltp&#34;&gt;7. OLAP vs OLTP&lt;/h3&gt;

&lt;h3 id=&#34;8-数据仓库&#34;&gt;8. 数据仓库&lt;/h3&gt;

&lt;h3 id=&#34;9-stars-and-snowflakes-schemas-for-analytics&#34;&gt;9. Stars and Snowflakes: Schemas for Analytics&lt;/h3&gt;

&lt;h3 id=&#34;10-column-oriented-storage&#34;&gt;10. Column-Oriented Storage&lt;/h3&gt;

&lt;h3 id=&#34;11-column-compression&#34;&gt;11. Column Compression&lt;/h3&gt;

&lt;h3 id=&#34;12-sort-order-in-column-storage&#34;&gt;12. Sort Order in Column Storage&lt;/h3&gt;

&lt;h3 id=&#34;13-writing-to-column-oriented-storage&#34;&gt;13. Writing to Column-Oriented Storage&lt;/h3&gt;

&lt;p&gt;前面写的有关列存储的优化在数据仓库中很重要，排序、位图索引、压缩都能加速查询，但是会让写数据更加麻烦。&lt;/p&gt;

&lt;p&gt;以 BTree 为理论的写方法，在这里完全没有用，如果你要在数据中插入一条数据，你需要重写所有的数据。&lt;/p&gt;

&lt;p&gt;但是还好的是，如果是 LSM-tree 的方式，首先写进内存排序，然后写磁盘。查询的时候，也是两部分结果进行合并，Vertica 数据仓库就是这么做的。&lt;/p&gt;

&lt;h3 id=&#34;14-aggregation-data-cubes-and-materialized-views&#34;&gt;14. Aggregation: Data Cubes and Materialized Views&lt;/h3&gt;

&lt;p&gt;并不是所有的数据仓库都是使用列存储，很多其他存储方式也会用到，但是列存储确实能够很大程度加快访问，所以越来越流行。&lt;/p&gt;

&lt;p&gt;另一种值得一提的数据仓库技术就是 materialized aggregates。前面所说的，查询经常会遇到一些聚合， 例如 count，sum，如果一个查询被很多查询共用，可以将结果缓存。
其中一种方法就是 materialized view，类似查询视图，只不过这个视图的结果已经计算好保存起来了。如果数据改变了，你的 materialized view 也要改变。&lt;/p&gt;

&lt;p&gt;一种常见的案例就是 data cube 后者 OLAP cube，就是根据不同纬度就行聚合后的结果。举个列子，如果表格有 日期、商品种类、销售额三列，我们可以计算销售额在 日期、商品种类 两个维度下的聚合只，得到一个二维表格。
二维表格的两个维度分别为日期和商品种类，表格的值就是销售额，然后如果要计算每个维度下的 sum，只需要将对应维度所有值 sum 到一起。&lt;/p&gt;

&lt;p&gt;实际上，表格都不止两个维度，假如有五个维度，情况就很复杂了。但是原则不变，每个 cell 保存五个维度组合下的聚合值。&lt;/p&gt;

&lt;p&gt;通过 cube 可以加速某些查询，但是如果要计算订单额度大于 100 的比例，那就没法计算了，因为额度很难作为一个维度。所以一般只作为一个加快部分查询的工具。&lt;/p&gt;

&lt;h2 id=&#34;四-encoding-and-evolution&#34;&gt;四、Encoding and Evolution&lt;/h2&gt;

&lt;p&gt;服务总是要变的，服务变了，服务之间的代码也要变。服务端可以做滚动升级，也就是部分服务器先升级，保证没错的话，其他的继续升级。
而客户端也就是用户端，代码变化必须做到向前向后兼容，也就是说，高版本要能够处理低版本的数据，这很简单；低版本的代码，必须能从高版本的架构读数据，这是比较难的。
接下来我们将会看一些数据结构，JSON, XML, Protocol Buffers, Thrift, and Avro ，主要看他们在这些data storage and for communication系统中如何使用：
in web services, Representational State Transfer (REST), and remote procedure calls (RPC), as well as message-passing systems such as actors and message queues&lt;/p&gt;

&lt;h3 id=&#34;1-formats-for-encoding-data&#34;&gt;1. Formats for Encoding Data&lt;/h3&gt;

&lt;p&gt;一般的应用都会处理两种数据。内存主要是通过对象、数组、树、hash表等，而需要保存、传输时候，需要进行序列化，由于指针引用序列化后是没用的，所以序列化后的结构和内存结构完全不用。
从设计模式角度考虑，我们需要进行一个适配，两边都要适配的话，实际上就是做一个转换。内存到 byte sequence 的过程称为 encoding（或者 serialization、marshalling），反过来讲decoding（deserialization、parsing、unmarshalling）&lt;/p&gt;

&lt;p&gt;** 专业术语冲突
serialization 在数据库的事务中也会用到，但是完全是另外一码子事情，后续会进行介绍。这里使用的时候我们以 encoding 为主，虽然平时 serialization 用的更多。&lt;/p&gt;

&lt;p&gt;这里我们将会以一条数据为例,对它进行编码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
    &amp;quot;userName&amp;quot;: &amp;quot;Martin&amp;quot;,
    &amp;quot;favoriteNumber&amp;quot;: 1337,
    &amp;quot;interests&amp;quot;: [&amp;quot;daydreaming&amp;quot;, &amp;quot;hacking&amp;quot;]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;1-language-specific-formats&#34;&gt;(1) Language-Specific Formats&lt;/h4&gt;

&lt;p&gt;java.io.Serializable 属于java 自带的序列化机制，很多编程语言都自带了。当然也有很多第三方的，比如 Kryo for Java。这些很方便，因为你只需要直接读然后调用java对应的方法即可。但是问题也很多。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;通用性问题，不多说。&lt;/li&gt;
&lt;li&gt;数据会有类型，例如java 序列化必须只能解析为特定的类。所以会定义很多类，这样可能导致一些人得到你的类信息，然后远程植入代码。&lt;/li&gt;
&lt;li&gt;版本问题。&lt;/li&gt;
&lt;li&gt;效率问题。&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&#34;2-json-xml-and-binary-variants&#34;&gt;(2) JSON, XML, and Binary Variants&lt;/h4&gt;

&lt;p&gt;JSON, XML 一般都很熟悉，优缺点都很明显，二进制的编码在某些内网的应用中很常见，还能节约空间。例如 MassagePack 就是二进制的 json。&lt;/p&gt;

&lt;h4 id=&#34;3-thrift-and-protocol-buffers&#34;&gt;(3) Thrift and Protocol Buffers&lt;/h4&gt;

&lt;p&gt;Thrift and Protocol Buffers 很多人都知道，作为两种序列化的框架，当然还提供了别的功能，例如 RPC 通信接口。都需要定义一个schema：&lt;/p&gt;

&lt;p&gt;Thrift 的定义格式：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;struct Person {
  1: required string       userName,
  2: optional i64          favoriteNumber,
  3: optional list&amp;lt;string&amp;gt; interests
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;pb 的定义格式：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;message Person {
    required string user_name       = 1;
    optional int64  favorite_number = 2;
    repeated string interests       = 3;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后他们都有代码生成的工具，可以针对各种语言生成相应的代码，用来 encode 和 decode 数据。thrift 的格式有两种 BinaryProtocol and CompactProtocol。&lt;/p&gt;

&lt;p&gt;BinaryProtocol 表示上面的数据格式需要 59 bytes ：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;0b 00 01 00 00 00 06 4d 61 72 74 69 6e     0b 是 type代表 String，紧接着 00 01 代表 field tag 为1，然后 00 00 00 06 代表长度为6，剩下的六bytes 代表数据。
0a 00 02 00 00 00 00 00 00 05 39           和刚才一样， 0a 代表int64 ，
0f 00 03 0b 00 00 00 02                    0f 代表list，00 03代表 field tag 为3，0b 代表 item type 为 string，00 00 00 02 代表长度为2，
	00 00 00 0b 64 61 79 ....              长度和数据
	00 00 00 07 68 ..... 				   长度和数据
00											结束标志
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;CompactProtocol 和 BinaryProtocol 的结构类似，但是进行了一些压缩，需要 33 bytes&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;18 06 4d ....  							  18 是 00011000 field type and tag number 合在一起，0001 是 tag，
										  1000 是 type 是string （为什么8 代表 string，上面是11），06 长度，后面是数据
										  另外int类型并不是占了 64位，这里的 06是两位，共 8 字节，
										  最高位代表数据是否已经完整，剩下的七位是数据。如果数据不完整，需要再占8位。这样 -64 到 63 之间的数据只占了1byte
										  
16 f2 14 								  16 是 00010110，0001 代表 field tag 比上一个加一，0110 数据类型 后面的 f214 是数据，f2 最高位是1，数据没完，需要往下一位读。
19 28
    0b 64 .....
    07 68 ....
00
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ProtocolBuffer 和 Thrift 的 CompactProtocol 类似，需要 33 bytes&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;0a 06 4d 61 72 74 69 6e       			  0a 是 tag(00001) 和 type(010), 06 是长度。 后面是数据
10 b9 0a 								  10 是tag (00010) 和 type（000）,后面是长度+数据
1a 0b ........							  1a 是 tag 和 type，后面是长度+数据
1a 07 ........							  1a 是 tag 和 type，后面是长度+数据
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;和 CompactProtocol 稍微有些不一样，tag+type 为两位，另外 list 类型直接存两个。&lt;/p&gt;

&lt;p&gt;需要注意的是，前面我们的例子中，数据要么是 要么是 required or optional，实际上这对于数据的存储没有任何影响，都是一样的存储，只是在最后解析的时候影响，例如你设置的是 required ，但是没有解析出来，会失败。&lt;/p&gt;

&lt;h4 id=&#34;4-field-tags-and-schema-evolution&#34;&gt;(4) Field tags and schema evolution&lt;/h4&gt;

&lt;p&gt;前面讲过数据需要改变，那 pb 和 thrift 如何应对schema 的变化，做到向前向后兼容呢？
其实我们从上面的数据可以看出，最重要就是 field tag 了，只要 field tag 唯一，哪怕是增加了数据，减少了数据，最终的结果其实还是一样的。你可以改变变量名字，但是不能改变 tag。
首先是向后兼容，只要你新加的 field tag 不被设置为 required，就不会报错，另外删除数据也是，不能删除 required 的数据。
另外是向前兼容，如果是旧的代码读取新的数据，如果遇到不认识的 field tag ，会直接忽略。&lt;/p&gt;

&lt;h4 id=&#34;5-datatypes-and-schema-evolution&#34;&gt;(5) Datatypes and schema evolution&lt;/h4&gt;

&lt;p&gt;数据类型发生变化，一方面可能是数据精度收到影响，另一方面，我们可以看出 pb 是可以讲啊 list 类型变成 optional 的，但是 thrift 不行，因为 thrift 提供了 list 类型，但是这样可以用来写嵌套数据。&lt;/p&gt;

&lt;h4 id=&#34;6-avro&#34;&gt;(6) avro&lt;/h4&gt;

&lt;p&gt;avro 也是一个序列胡框架，他和 pb、thrift 不同点在于没有 tagNumber，这其实很适合动态生成schema，我们不介绍了。&lt;/p&gt;

&lt;h4 id=&#34;7-dynamically-generated-schemas&#34;&gt;(7) Dynamically generated schemas&lt;/h4&gt;

&lt;p&gt;如果我们的数据增加了一列，并且减少了一列，对于 avro 来说，只需要重新生成一个 schema 即可，而如果使用 pb、thrift，需要手动进行配置tag和field 的映射，&lt;/p&gt;

&lt;h4 id=&#34;8-code-generation-and-dynamically-typed-languages&#34;&gt;(8) Code generation and dynamically typed languages&lt;/h4&gt;

&lt;p&gt;代码生成，例如 pb 可以生成 java 和 python 的代码。&lt;/p&gt;

&lt;h4 id=&#34;9-the-merits-of-schemas&#34;&gt;(9) The Merits of Schemas&lt;/h4&gt;

&lt;h3 id=&#34;二-modes-of-dataflow&#34;&gt;二、 Modes of Dataflow&lt;/h3&gt;

&lt;p&gt;我们需要在进程之间发送数据，&lt;/p&gt;

&lt;h4 id=&#34;1-dataflow-through-databases&#34;&gt;1.Dataflow Through Databases&lt;/h4&gt;

&lt;h4 id=&#34;2-dataflow-through-services-rest-and-rpc&#34;&gt;2.Dataflow Through Services: REST and RPC&lt;/h4&gt;

&lt;h4 id=&#34;3-message-passing-dataflow&#34;&gt;3.  Message-Passing Dataflow&lt;/h4&gt;

&lt;h1 id=&#34;第二部分-分布式&#34;&gt;第二部分、分布式&lt;/h1&gt;

&lt;p&gt;使用分布式的原因很多，例如 Scalability，high available，latency。数据分布式有很多种方式，首先是副本、分区，也就是 Replication 和 Partition。&lt;/p&gt;

&lt;h1 id=&#34;一-replication&#34;&gt;一、Replication&lt;/h1&gt;

&lt;p&gt;Replication 就是将你的数据放在多个节点，这样有很多好处。数据从地理上可以隔用户更近，保持高可用，增加吞吐量。&lt;/p&gt;

&lt;p&gt;本章我们假设数据都是完整的进行副本存放，后面再讲分区。数据的副本解决难点就是数据变化的一致性，如果数据一致不变，那就太简单了，没什么可以讨论的。&lt;/p&gt;

&lt;p&gt;对于数据变化的副本存储，有很多需要注意的地方，例如每次写数据操作数据复制是同步的还是异步的，节点挂了怎么办，数据一致性问题，时效性问题。
我们的学习主要分为三种设计：single-leader, multi-leader, and leaderless&lt;/p&gt;

&lt;h2 id=&#34;1-leaders-and-followers&#34;&gt;1. Leaders and Followers&lt;/h2&gt;

&lt;p&gt;每个节点上面的副本叫做 replica ，当有 multi replica 的时候，一个问题出现了，怎么保证数据写到了每一个副本。
数据库的每一次写都必须保证被每个副本处理，否则就会出现不一致。
最常用的解决方案就是 leader-based-replication，也叫 active-passive、master-slave。&lt;/p&gt;

&lt;p&gt;在主从结构中，写只能发送给 leader，leader 写的时候会发送数据给 follower，而读操作可以读任何一个节点。&lt;/p&gt;

&lt;h3 id=&#34;1-synchronous-versus-asynchronous-replication&#34;&gt;(1) Synchronous Versus Asynchronous Replication&lt;/h3&gt;

&lt;p&gt;replicated 系统重要特点就是 同步写还是异步写，一般关系型数据库可以配置，其他的都是硬编码写死的。&lt;/p&gt;

&lt;p&gt;客户端发送数据给leader，然后leader转发给follower，最后通知client。如果 leader 收到 follower 的确认消息再回复client，这就是同步。
如果leader发送给了 follower后就直接回复client，就是异步。一般情况，系统都只有一个follower是同步，其余的都是异步。这叫 semi-synchronous。&lt;/p&gt;

&lt;p&gt;为了效率经常设置为 Asynchronous，这样如何 leader 失败了而且不恢复，那还没有被副本保存的数据就丢失了，
这样的好处就是可以提供持续服务，尽管follower很慢。即便是配置使用 Synchronous，实际上也是有一台是同步，其余的是异步。&lt;/p&gt;

&lt;p&gt;Weakening durability 可能是一个比较糟糕的 trade-off，但是 Asynchronous 还是很常用，尤其是地理上分布式的集群。&lt;/p&gt;

&lt;p&gt;当然也有很多其他的方案在研究中，例如 Azure 使用了 chain replication。&lt;/p&gt;

&lt;h3 id=&#34;2-setting-up-new-followers&#34;&gt;(2) Setting Up New Followers&lt;/h3&gt;

&lt;p&gt;系统有时候需要添加新的节点，如何保证新节点和leader的数据一致呢？直接复制数据过来肯定是不行的，leader 一直处在 flux 中，一般步骤有四步：
1. leader 在某个确定一致性的时间点拍一个 snapshot 而不用锁住系统，大部分数据库都提供了这个功能，有的能通过第三方工具提供这个功能。
2. follower 将数据复制过去。
3. follower 将从这个时间点以后的数据复制过去，这需要拍 snapshot 时候的日志位置，这个点的名字叫 Log sequence number 或者 binlog coordinates。
4. 当 follower 将所有的change都同步后，我们称之为 catch up，就可以和其他follower一样，处理 leader 发生的变化了。&lt;/p&gt;

&lt;p&gt;真正的步骤其实是有很多不同，有时候可以自动配置，有时候需要手动配置。&lt;/p&gt;

&lt;h3 id=&#34;3-handling-node-outages&#34;&gt;(3) Handling Node Outages&lt;/h3&gt;

&lt;p&gt;有时候节点会挂掉，可能是我们不知道的原因，也可能是为了安装系统模块而重启，这时候应该怎么保证集群的高可用&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;follower 挂掉&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;follower 挂点然后重启的步骤是很简单的，类似前面的添加新节点，启动以后，根据日志能找到最后一个处理的事务，然后从leader请求这个时间点以后的更改，然后进行更改，完成后就 catch up。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;leader failover&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;leader 挂掉后可能麻烦一点，必须马上选择一名新的leader，这个过程被称为 failover&lt;/p&gt;

&lt;p&gt;failover的具体步骤有4：
1. 确定leader失败了，失败的原因很多，磁盘、电源等等，目前用的最多的是 timeout 方法，节点相互 bounce message，如果突然在某个时间点收不到，就是失败了。
2. 选择新的 leader，需要在大多数的follwer 进行选举，最好是选一个数据最新的。所有的节点同意这个节点成为 consensus。
3. 告知系统使用新的leader，client 发送数据到新的 leader，旧的 leader 回复后也要认同新的 leader。&lt;/p&gt;

&lt;p&gt;failover 总是会有些不期而遇的问题：
1. 如果是 asynchronous replica ，可能遇到一些数据没有同步过来，导致数据少了，如果这时候 older leader起来了，就会发生错误，这时候一般选择 discard 这部分数据。
2. 如果系统和别的系统进行结合，discard 是很危险的，github 发生了一次 mysql leader挂掉，然后discard 了一部分，删掉了部分自增的primary key，可是这部分pk已经保存到 redis了，导致了错误
3. 有时候两个节点都认为自己是 leader，这个叫做 split brain，这时候一半需要去掉一个，但是有可能两个都被去掉了。
4. timeout 的具体值多少合适？如果太长了，可能导致发现的晚。如果太短了可能误判，尤其是系统压力大的时候，还重新换一次会造成更大的压力。&lt;/p&gt;

&lt;p&gt;这些问题解决起来还是很棘手的，所以有很多算法，我们后续会介绍。&lt;/p&gt;

&lt;h3 id=&#34;4-implementation-of-replication-logs&#34;&gt;(4) Implementation of Replication Logs&lt;/h3&gt;

&lt;p&gt;上面各种恢复，添加，都涉及到日志，日志记录了数据的更改，实际上我们学习 Hadoop 的时候，也知道里面有日志，现在我们来研究一下，日志怎么实现。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Statement-based replication&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;基于 statement 的 replication，这是最简单的方法，数据库记录每一次更新，也就是说，你的 UPDATE、INSERT、DELETE都会记录到日志中，然后发送给follower。
这样的问题是：
对于 current_timestamp、random 这样的方法返回值不一样，基于判断的例如 update &amp;hellip; where &amp;hellip; 必须按照顺序，一些触发器、存储过程等 side effects。&lt;/p&gt;

&lt;p&gt;对于这些问题，当然也有特定的解决方案，例如先计算一些可能导致不一致的函数。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;WAL-log&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;前面讲过 B-Tree 和 LSM-Tree，都是基于WAL-log，是一种只能 append 的二进制日志，leader 不仅将 wal-log 保存到磁盘，而且发送到 followers。&lt;/p&gt;

&lt;p&gt;WAL 的 disadvantage 是太底层，wal 包含了磁盘那个位置的 block 需要修改哪些 bytes，这样耦合性比较高，如果format 改变了，基本上很难让 leader和follower 使用不同版本。&lt;/p&gt;

&lt;p&gt;这个问题看起来更像是实现的问题，如果 replication 的协议支持follower 使用新的版本，就能完成 zore-downtime 的 upgrade。
首先更新 follower的版本，然后让leader failover，选择一个 follower 为 leader。如果replication 的协议不支持版本不一直，这就叫做 wal-shipping，会造成 upgrade-downtime。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Logical (row-based) log replication&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这种方式就是 replication log 和具体的 storage device 解耦，使用不同格式。 relational database 的 logical log 一般都是一系列表示行的recorders。一个改变很多行的 statement 会
产生很多行这样的日志，后面有一个transaction 完成的标识符。这种方式是解耦的，所以可以容忍不同的版本，甚至不同的存储引擎。这也让外部的系统容易获得数据，
例如数仓、二级索引，外部缓存，这个技术叫做 change data capture。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;trigger based replication&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;前面讲的都是数据库系统的，有时候我们需要更灵活的，例如你只想一部分数据，或者你想从一个数据库迁移到另一个数据库。这样你就应该将replication 提升到应用程序层面。
很多数据库提供了工具完成这个事情。许多关系型数据库提供了一项功能：triggers and stored procedures。
trigger 能让你注册用户代码，一旦数据发生改变，将会将数据放到另一给表格，外部系统可以访问。&lt;/p&gt;

&lt;h3 id=&#34;5-problems-with-replication-lag&#34;&gt;(5) Problems with Replication Lag&lt;/h3&gt;

&lt;p&gt;之前说了 Replication 是保证数据安全、忍受 node failures，实际上还可以提高 scalability。&lt;/p&gt;

&lt;p&gt;Leader-based 需要所有的 write 请求都通过 master，将 read 分发到 slave，增加节点就能增加扩展性，
节点太多了就只能使用 异步的数据同步方式，否则可用性就很差。&lt;/p&gt;

&lt;p&gt;然而一旦 slave 的数据 fallen behind 了，可能访问的数据过时了，这看起来问题不大，毕竟等等就行，这叫做  eventual consistency。&lt;/p&gt;

&lt;p&gt;这个 eventually 是一个很NB的词，并没有告诉你具体时间，只是说最终会一致。我们称时间差为 the replication lag。这个 lag导致的问题我们接下来会讨论。&lt;/p&gt;

&lt;h4 id=&#34;1-read-your-own-writes&#34;&gt;1. Read your own writes&lt;/h4&gt;

&lt;p&gt;之前的 qq 空间有这个问题，当你发表了评论后页面会刷新，但是刷新后看不到自己发表的评论。
这时候你以为评论失败了，所以重新评论一下，然后刷新发现了自己评论了两次。&lt;/p&gt;

&lt;p&gt;这种问题被叫做 “read-after-write consistency”，也叫做 “read-your-writes consistency”。有一些解决方案：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;如果是用户可以修改的信息，从 master 读。&lt;/li&gt;
&lt;li&gt;如果是一分钟之内修改过的数据，从 master 读，或者可以监控 lag 大于1分钟的follow 排除。&lt;/li&gt;
&lt;li&gt;客户端保存最近一次写的 timestamp，这样服务端保证读的时候先等 slave 的数据同步已经到了这个时间。这里的 timestamp 可以是逻辑时间。&lt;/li&gt;
&lt;li&gt;如果有多个数据中心，可能更复杂。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;上面的解决方案只是一个 client，如果用户同事有多个 client 访问，例如手机和pc，这时候要考虑更多问题。&lt;/p&gt;

&lt;h4 id=&#34;2-monotonic-reads&#34;&gt;2. Monotonic Reads&lt;/h4&gt;

&lt;p&gt;类似上面，如果连续两次访问来自于不同的 slave，可能出现时间倒退，例如看球赛的时候刚开始比分 1:1,刷新一下变成 1:0 了！&lt;/p&gt;

&lt;p&gt;Monotonic Reads 就是保证上面的异常不发生的一致性。他比 eventually consistency 更强，比 strong consistency 更弱。&lt;/p&gt;

&lt;p&gt;这种一致性的解决方案就是保证每个 userid 都从同一个 slave 读数据，例如根据 userid 的 hash。&lt;/p&gt;

&lt;h4 id=&#34;3-consistent-prefix-reads&#34;&gt;3. Consistent Prefix Reads&lt;/h4&gt;

&lt;p&gt;在一个群聊系统，如果你发现两个人的对话顺序反了，就是 Consistent Prefix Reads 没有得到保证。&lt;/p&gt;

&lt;p&gt;这个问题解决方案需要保证有相互依赖的数据都按顺序写入。后续再  causal dependencies 和 happens-before 会讨论。&lt;/p&gt;

&lt;h3 id=&#34;6-solutions-for-replication-lag&#34;&gt;(6). Solutions for Replication Lag&lt;/h3&gt;

&lt;p&gt;后续会讲解 事务 和 分布式事务。&lt;/p&gt;

&lt;h2 id=&#34;2-multi-leader-replication&#34;&gt;2. Multi-Leader Replication&lt;/h2&gt;

&lt;p&gt;一个 leader 可能会有一些问题，例如写太多了可能会有性能问题。也有一些其他的选择， 例如多个 leader 。&lt;/p&gt;

&lt;h3 id=&#34;1-use-cases-for-multi-leader-replication&#34;&gt;(1). “Use Cases for Multi-Leader Replication”&lt;/h3&gt;

&lt;p&gt;一般情况如果只有一个 datacenter 是不需要用多个 leader 的，所以在多个 datacenter 的时候会使用 multi-leader&lt;/p&gt;

&lt;h4 id=&#34;multi-datacenter-operation&#34;&gt;Multi-datacenter operation&lt;/h4&gt;

&lt;p&gt;如果有多个 datacenter ，那么每个 datacenter 一个 leader 是一个不错的选择。
我们对比一下在 multi-datacenter 中使用 single-leader 和    multi-leader 的fare&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Performance ，multi-datacenter 的性能可能好一些&lt;/li&gt;
&lt;li&gt;Tolerance of datacenter outages ，multi-datacenter 不用担心leader 没了，所以更简单一点&lt;/li&gt;
&lt;li&gt;Tolerance of network problems，由于 datacenter 之间的网络稳定性肯定不如 datacenter 内部，所以 multi-leader 更好点&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;总之，如果有多个 datacenter ，选择 multi-leader。尽管如此也会有缺点，
例如需要同时修改一份数据，可能发生冲突导致一边成功一边失败了，后面的 handle write conflict 会讨论。&lt;/p&gt;

&lt;p&gt;同时，multi-leader 可能会有一些陷阱、和数据库的其他特性放一起经常出问题，例如主键自增。所以 multi-leader 需要尽量避免。&lt;/p&gt;

&lt;h4 id=&#34;clients-with-offline-operation&#34;&gt;“Clients with offline operation”&lt;/h4&gt;

&lt;p&gt;以 印象笔记、icloud 为例，我们在手机、电脑 的笔记可以在线编辑，也可能离线编辑。
这个架构就和 Multi-Leader 类似，每个设备是一个 datacenter，服务端也是一个 datacenter，网络连接并不靠谱。&lt;/p&gt;

&lt;h4 id=&#34;collaborative-editing&#34;&gt;Collaborative editing&lt;/h4&gt;

&lt;p&gt;以 Google docs、wiki 为例，大家可以同事协同编辑，相关的算法就是 automatic conflict resolution。
你可能以为这个和 multi-leader 的 replication 不一样，实际上很类似。
一个人编辑的时候，change 马上就出现在 local replica ，然后是异步同步到服务器和其他的人。
如果你希望保证没有冲突，Application 需要在文档上面加锁，其余人获得锁之前不能编辑，这种就类似与 single-leader 的replica。
但是为了快速工作，你可能将锁分的很小，大家可以同时编辑，这就类似 multi-leader ，同时带来了 conflict resolution。&lt;/p&gt;

&lt;h4 id=&#34;handling-write-conflicts&#34;&gt;Handling Write Conflicts&lt;/h4&gt;

&lt;p&gt;如果两个人同时编辑，一个修改了标题一个修改了内容这不会有冲突，但是如果两个人都修改了标题该如何解决这个问题。
如果是 single-leader，并不会有这种问题，因为写都是有时间顺序的。后面的那个要么等待前面的修改事务成功或者失败，要么自己让让写任务失败。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;“Synchronous versus asynchronous conflict detection”&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如果两个任务修改同一个数据都成功了，然后异步发现了冲突，这时候解决已经晚了，因为不知道放弃哪个。
当然原则上也可以通过同步修改的方式避免冲突，这样就和 single-leader 没什么区别了。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Conflict avoidance&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;避免冲突是目前最好的方法了，例如用户可以修改自己信息的话，就让所有的修改都发送到同一个 datacenter 的 leader。&lt;/p&gt;

&lt;p&gt;有时候也可能某个 datacenter 挂了或者用户去了另一个地方需要 datacenter 换了，需要使用别的方法避免冲突。这时候还是需要处理冲突。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Converging toward a consistent state&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于 single-leader 的 writes 都有一个 sequential order，统一数据的最后一次写决定了数据。
对于 multi-leader 就不一样了，并不知道哪个 datacenter 的才是正确的，如果不进行修改就会出现各个 datacenter 不一致，
所以所有的更改必须 最终是 convergent ，也就是说 最终一致性。几种常见的 convergent 方法：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;简单粗暴，直接每个操作一个 uuid，例如 hash，或者时间戳，最大的那个胜出，如果使用时间戳，这种方法就叫做 Last-Write—wins（LLW）。&lt;/li&gt;
&lt;li&gt;给每个 replica 一个 id，id 最高的占主导地位，和上面的类似。&lt;/li&gt;
&lt;li&gt;合并数据，例如 一个标题是A，一个是B，最后就是 A|B&lt;/li&gt;
&lt;li&gt;用户自己实现冲突合并逻辑。&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
&lt;li&gt;Custom conflict resolution logic&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;自己实现冲突解决一般有两种方案：On Read 和 On Write ，相关细节比较简单&lt;/p&gt;

&lt;h4 id=&#34;what-is-a-conflict&#34;&gt;What is a conflict?&lt;/h4&gt;

&lt;p&gt;上面说的两个人同时修改标题的冲突很明显，所以可以通过一些程序控制，实际上也有一些不明显的的冲突。&lt;/p&gt;

&lt;p&gt;例如两个人预定会议室，同一个会议室同一时间只能有一个人预定成功，但如果是 multi-leader 可能两个人都预订成功了。
或者注册的时候要求 username 不重复，但是有可能两个人同时申请同一个 username。&lt;/p&gt;

&lt;h3 id=&#34;2-multi-leader-replication-topologies&#34;&gt;(2) Multi-Leader Replication Topologies&lt;/h3&gt;

&lt;p&gt;Replication Topologies 是用来描述数据在节点间传播的路径。例如 circular 、star、all-2-all。&lt;/p&gt;

&lt;p&gt;每一种都有优势和劣势。&lt;/p&gt;

&lt;h2 id=&#34;3-leaderless-replication&#34;&gt;3. Leaderless Replication&lt;/h2&gt;

&lt;p&gt;实际上 cassandra 采用了 leaderless 的架构，没有主节点。&lt;/p&gt;

&lt;h3 id=&#34;1-writing-to-the-database-when-a-node-is-down&#34;&gt;(1) Writing to the Database When a Node Is Down&lt;/h3&gt;

&lt;p&gt;leaderless 中，不存在 failover，因为没有主节点，因为一个节点在读数据的时候，并不需要从所有的节点都读数据，
二是给每个节点发送数据，然后每个节点都返回数据，根据 Version numbers 决定哪个是正确的。&lt;/p&gt;

&lt;h4 id=&#34;read-repair-and-anti-entropy&#34;&gt;Read repair and anti-entropy&lt;/h4&gt;

&lt;p&gt;系统需要保证每个 replica 上面的数据都是最终一致的，如果出现了节点挂掉然后恢复，如何保证它的数据是一致？&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Read repair ，当用户读书节的时候，能够发现 stale 的值并且修复&lt;/li&gt;
&lt;li&gt;Anti-entropy process 通过后台进程扫描数据发现不一致&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&#34;quorums-for-reading-and-writing&#34;&gt;Quorums for reading and writing&lt;/h4&gt;

&lt;p&gt;也就是上面所说的，读和写不需要全成功，只要保证 读成功数 r 和 写成功数 w 满足 r+w&amp;gt;n即可。这个叫做仲裁原理，类似抽屉原理，
n个抽屉，里面有 w 个抽屉放了一样的纸条，打开r个抽屉，想要看到纸条内容，只要 r + w &amp;gt;n。
一般情况下 r=w=(n+1)/2&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Limitations of Quorum Consistency&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;上面我们说了 r=w=(n+1)/2，但是如果你的数据库写的量特别小，读特别大，你也可以设置 r=n，w=1，总之只要覆盖即可。&lt;/p&gt;

&lt;p&gt;如果你对一致性要求不高，也可以使用 w+r &amp;lt;= n，这样返回值可能 stale，不过这样的可用性更高。&lt;/p&gt;

&lt;p&gt;然而尽管使 w+r &amp;gt; n，也会有一些其他的异常情况：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;sloppy quorum 下无法保证&lt;/li&gt;
&lt;li&gt;concurrently 写，和上一节类似，出现了写冲突？&lt;/li&gt;
&lt;li&gt;读和写 concurrently，这时候写只提交了一部分，其他的可能还未完全在 w 个节点成功&lt;/li&gt;
&lt;li&gt;如果写 成功不够w，但是这时候也没法 rollback，后续可能会读到失败的值，也可能读不到&lt;/li&gt;
&lt;li&gt;如果保存了 new value 的节点失败重启，从另一个保存了 old value 的节点修复数据，导致包含 new value 数据的节点少于 n&lt;/li&gt;
&lt;li&gt;还有极端情况，例如  “Linearizability and quorums”.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;因此，即便是 达到了 quorum 的条件，也没法真正的实现一致性。
特别的，至于在 single-leader 中的 “reading your writes, monotonic reads, consistent prefix reads” 等弱一致性也没法得到保障。
所以前面说的异常可能也会发生，强一致性需要 consensus 或者 transaction 来保证。&lt;/p&gt;

&lt;h4 id=&#34;monitoring-staleness&#34;&gt;Monitoring staleness&lt;/h4&gt;

&lt;p&gt;对于应用而言，是否返回最新版本的数据是很重要的。对于 leader-based 的设计，所有的问题都可以交给 replication-lag 来解决，
可以通过监控 replication-lag 监控系统
对于 leaderless 的设计，所有的数据并没有确定的顺序，监控很困难，而且如果没有 anti-entropy，数据可能过时很久都不修复。&lt;/p&gt;

&lt;h3 id=&#34;sloppy-quorums-and-hinted-handoff&#34;&gt;“Sloppy Quorums and Hinted Handoff”&lt;/h3&gt;

&lt;p&gt;简单介绍一下，例如 cassandra 中，保存三副本，读写都要成功 2 个 replica，我们写数据要成功 2个，
但是如果此时这三个节点挂了两个，只能写成功一个，可以先将另一份数据放到其他的节点上。
等对应的剩下的节点都恢复了，再将数据放回去，这个过程就是 hinted handoff。&lt;/p&gt;

&lt;h3 id=&#34;multi-datacenter-operation-1&#34;&gt;Multi-datacenter operation&lt;/h3&gt;

&lt;p&gt;这时候我们还是有N个副本，我们需要保证每在 每个 datacenter 都达到了 对应的 n/2 个副本。&lt;/p&gt;

&lt;h3 id=&#34;detecting-concurrent-writes&#34;&gt;Detecting Concurrent Writes&lt;/h3&gt;

&lt;p&gt;类似之前的 handling write conflict，也是由于顺序不一样导致，所以也需要有保证 converge 的方法，
这样的方案也就是我们前面说的一样。但是我们这里讨论的更细致一点。&lt;/p&gt;

&lt;h4 id=&#34;last-write-wins-discarding-concurrent-writes&#34;&gt;Last write wins (discarding concurrent writes)&lt;/h4&gt;

&lt;p&gt;对于不知道哪个 happens-first 的操作，我们称之为 concurrent，他们的 order 是不知道的。&lt;/p&gt;

&lt;p&gt;尽管他们没有 order，我们可以人为给一个 order，例如 timestamp，也就是 Last write wins。&lt;/p&gt;

&lt;p&gt;llw 解决了冲突问题，但是丢失了一些数据。所有使用 cassandra 的时候有人会给每个 key 后面加一个 uuid，这样能保证每个 key 多次写都不会丢失。&lt;/p&gt;

&lt;h4 id=&#34;the-happens-before-relationship-and-concurrency&#34;&gt;The “happens-before” relationship and concurrency&lt;/h4&gt;

&lt;p&gt;happens-before 的关系是决定是否并发的关键，以下情况下我们称两个操作是 concurrency：
“neither happens before the other (neither knows about the other)”&lt;/p&gt;

&lt;p&gt;因此，对于任何两个操作 A 和 B，只有两种可能，A happens-before B，B happens-before A，A 和 B 是 concurrency。&lt;/p&gt;

&lt;h4 id=&#34;capturing-the-happens-before-relationship&#34;&gt;Capturing the happens-before relationship&lt;/h4&gt;

&lt;p&gt;上面已经知道两个操作只能有 3 种关系，我们如果发现 happens-before  的关系呢？其实有算法的。&lt;/p&gt;

&lt;p&gt;首先假如 A 和 B两个人都在往同一个购物车添加商品，一开始 A添加了 商品1，B添加了商品 2，这两个是 concurrency的，
然后 A再添加 3，B添加4，这时候 这两个操作都是 3和4是 concurrency的，但是1和2 都是 happens-before 3 和4 的。&lt;/p&gt;

&lt;p&gt;发现 concurrency 关系的算法如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;server 的每个key有一个 version number，每次更新写这个 key 的时候，version number 就 +1并且跟新数据。&lt;/li&gt;
&lt;li&gt;当 client 读数据，返回所有没有 overwritten 的数据和最新的 version number。&lt;/li&gt;
&lt;li&gt;client 写数据的时候，必须包含之前读到的那个 version number，并且合并所有的数据，&lt;/li&gt;
&lt;li&gt;当 server 收到带 version number 的写请求，覆盖所有小于等于 version number 的数据，保留大于 version number 的数据。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这样的算法就能发现所有的 happens-before 关系&lt;/p&gt;

&lt;h4 id=&#34;merging-concurrently-written-values&#34;&gt;Merging concurrently written values&lt;/h4&gt;

&lt;p&gt;类似前面的 conflict resolution，上面的算法需要 client 进行数据合并，合并算法大家自己查资料，例如 CRDT 。&lt;/p&gt;

&lt;h4 id=&#34;version-vectors&#34;&gt;Version vectors&lt;/h4&gt;

&lt;p&gt;上面的算法是 single-replica 情况，对于 multi-replica 的情况，需要每个副本一个 Version number，这被叫做 Version vectors。&lt;/p&gt;

&lt;h2 id=&#34;summary&#34;&gt;SUMmary&lt;/h2&gt;

&lt;p&gt;通过上面的讨论，我们明白了一句话，其实分布式 replica 最大的问题就是发现 happens-before 的依赖关系，也就是操作的顺序。&lt;/p&gt;

&lt;p&gt;另外有大神说过，分布式只有两个问题， exactly-once 和 order。其中 exactly-once 后续会讨论，order 就是我们这里讨论最多的。&lt;/p&gt;

&lt;h1 id=&#34;二-partitioning&#34;&gt;二、Partitioning&lt;/h1&gt;

&lt;p&gt;分区，also known as Sharding。相对其他的理论，这部分很简单了。主要就是跨分区的事务稍微复杂点。&lt;/p&gt;

&lt;h1 id=&#34;三-transactions&#34;&gt;三、Transactions&lt;/h1&gt;

&lt;p&gt;在一个数据系统中，很多东西可能出错，有可能是网络问题，硬件问题，系统 bug。为了可靠，我们必须让系统可靠，事务就是一个选择。
事务是我们设计应用的时候遵循的的一套规定，但实际上他并不是理所当然的，二是要通过设计底层算法来满足的，这次我们就看看这些算法。
这一节我们主要将重点放在单机的事务，下一节将会讨论分布式事务。&lt;/p&gt;

&lt;h2 id=&#34;1-the-slippery-concept-of-a-transaction&#34;&gt;1. The Slippery Concept of a Transaction&lt;/h2&gt;

&lt;p&gt;现在几乎所有的 relational databases，以及一些 NoSql 都支持 transaction，但是 transaction 的定义到底是什么？
实际上一些 NoSql 摆脱了事务的约束，因为事务有时候严重影响了扩展性。都是一些  trade-offs&lt;/p&gt;

&lt;h2 id=&#34;2-the-meaning-of-acid&#34;&gt;2. The Meaning of ACID&lt;/h2&gt;

&lt;p&gt;不同数据库的 ACID 实现方式是不同的，例如 isolation 的定义有很多，除了 ACID 还有一种约束叫做 BASE，就更加模糊了，一个一个看吧。&lt;/p&gt;

&lt;h3 id=&#34;1-atomicity&#34;&gt;(1) Atomicity&lt;/h3&gt;

&lt;p&gt;原子性其实有很多种定义，一般代表一个事物不可分，例如同时如果是多线程，另一个线程不可能看到转账中间的状态。&lt;/p&gt;

&lt;p&gt;但是在事务中，并不是这个意思，上面的问题是 隔离性讨论范畴，Atomic 是说，例如一个转账的事务，分为两步，不可能只完成一步而另一步失败。&lt;/p&gt;

&lt;p&gt;没有原子性保证，如果一个事物中途失败，我们没法知道哪些改变了哪些没改变。&lt;/p&gt;

&lt;h3 id=&#34;2-consistency&#34;&gt;(2) Consistency&lt;/h3&gt;

&lt;p&gt;Consistency 是一个被严重重载的词，前面的 replica 中它是说主备不一致；CAP 中指的是 线性一致性，这里指的是一种 good state。&lt;/p&gt;

&lt;p&gt;这个定义很模糊，因为他很难描述清楚。一是转账的时候，事务前后两个人的总金额要一样，
注册用户名的时候保证用户名不重复，不能违背外键约束，这都是一致性的问题。
AID 都是 properties of databases，数据库可能依赖 AID 来实现 Consistency，所以严格来说 Consistency 并不属于 ACID&lt;/p&gt;

&lt;h3 id=&#34;3-isolation&#34;&gt;(3) Isolation&lt;/h3&gt;

&lt;p&gt;大部分数据都是多线程操作的，如果多个线程访问了同一条数据，就会出现类似前面讨论的 concurrency-write 问题，但是这里还有 read 的问题。
有的经典课本将它改为 serializability，意思是每个线程都可以假设只有他一个线程在操作数据库，数据库保证并发执行的结果和顺序执行的结果是一样的。
实际上真正的数据库很少会让操作只在一个线程里面，这样性能太低。&lt;/p&gt;

&lt;h3 id=&#34;4-durability&#34;&gt;(4) Durability&lt;/h3&gt;

&lt;p&gt;这个就是说事务一旦提交就持久化了，例如已经存入了 nonvolatile storage，例如 hard drive or SSD，结合 write-ahead log 等方式实现。
在 replicated databases 中，可能意味着已经复制到了好几个副本之中。
perfect durability 是不可能存在的，如果你的机房被恐怖分子炸掉了，硬盘都没了。&lt;/p&gt;

&lt;p&gt;自从分布式理论有了以后， REPLICATION AND DURABILITY 就出现了一些变化，相关的可以查看资料。&lt;/p&gt;

&lt;h2 id=&#34;single-object-and-multi-object-operations&#34;&gt;Single-Object and Multi-Object Operations&lt;/h2&gt;

&lt;p&gt;上面我们知道了，atomicity 和 isolation 描述了在客户端执行多个操作时候 ，数据库的做法。
前者表示如果事务进行到一半失败了必须删掉所有修改的数据，后者表示各个线程互不干扰。接下来看一个例子。&lt;/p&gt;

&lt;p&gt;一个邮件系统需要显示未读邮件数量，如果每次用户登录都执行 count 操作会比较耗时，所以添加了一个变量 counter 表示未读邮件。&lt;/p&gt;

&lt;p&gt;这时候每次收到一个邮件就 counter++ ，读了一份邮件就 counter &amp;ndash;。
atomicity 能够保证 不会出现异常（例如已经没用邮件但是 counter &amp;gt;0）,不会出现 消息标记为已读但是 counter&amp;ndash; 执行失败。
如果标记已读的时候，突然收到一份邮件，这时候 counter 的 ++ 和 &amp;ndash; 是并发的，Isolation 保证不会出现异常。&lt;/p&gt;

&lt;p&gt;Multi-object transactions 需要决定哪个读和写操作是同一个 事务。
对于 relational，就是同一个 tcp 连接的一个 start 和 commit 之间的操作属于同一个事务。
但是对于 Nosql，并没有相应的方法，尽管提供了对于的API，但是实际上不一定能保证事务。&lt;/p&gt;

&lt;h3 id=&#34;1-single-object-writes&#34;&gt;(1) Single-object writes&lt;/h3&gt;

&lt;p&gt;如果一个事务修改了一个 json 文件，但是由于网络问题，修改了一半。或者多个事务同时修改了一个字段，这时候也和上面一样需要进行单个对象事务控制。&lt;/p&gt;

&lt;p&gt;一些数据库提供了 更复杂的 atomic 操作，例如 increment 可以避免 read-modify-write，类似的有 compare-and-set ，思想类似了乐观锁。
这样的操作实际上和 ACID 类似，compare-and-set 实际上也是 lightweight transactions，或者是市场化的 ACID。&lt;/p&gt;

&lt;h3 id=&#34;2-the-need-for-multi-object-transactions&#34;&gt;(2) The need for multi-object transactions&lt;/h3&gt;

&lt;p&gt;许多 NoSql 放弃了 multi-object transactions， 因为需要在不同的 partitions 上面实现事务。
我们真的需要 multi-object transactions 么？我们的应用能否只通过 single-object operations 实现。考虑以下情形：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;外键约束 关系型数据库可能有外键，我们执行一个操作的时候，需要考虑满足这些约束&lt;/li&gt;
&lt;li&gt;document databases 很难 join，所以会有 denormalization，对应的修改必须是同时修改。&lt;/li&gt;
&lt;li&gt;考虑 secondary indexes，值修改了索引也要进行修改。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这些情形可以不用事务，但是容错就会相当复杂。&lt;/p&gt;

&lt;h3 id=&#34;3-handling-errors-and-aborts&#34;&gt;(3) Handling errors and aborts&lt;/h3&gt;

&lt;p&gt;ACID 的主要目的是如果发生了错误，我可以完全丢弃掉这些改变，但是也有例外。
leaderless replication 在你的数据出现错误以后并不会进行修改。
如果发生了错误就直接 abort 然后重试也不是很好的选择：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;如果事务已经 succeeded，但是网络问题导致没有返回成功消息，你可能会标记为失败，然后重试会执行两次？&lt;/li&gt;
&lt;li&gt;如果集群 load 过高你这时候再次重试反而会是问题严重。&lt;/li&gt;
&lt;li&gt;有时候重试再多次也是失败的&lt;/li&gt;
&lt;li&gt;如果事物内部有一些操作，例如发邮件？你总不能每次重试就发一次邮件吧。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;2-weak-isolation-levels&#34;&gt;2. Weak Isolation Levels&lt;/h2&gt;

&lt;p&gt;上面说过 serializable 并不是最好的隔离方式，还有一些其他的方式。&lt;/p&gt;

&lt;h3 id=&#34;1-read-committed&#34;&gt;(1) Read Committed&lt;/h3&gt;

&lt;p&gt;Read Committed 有两个保证，1 是读数据的时候只能看到提交成功的数据，2 是写数据的时候只会覆盖写成功的数据。也就是没有脏读和脏写。&lt;/p&gt;

&lt;h4 id=&#34;no-dirty-reads&#34;&gt;No dirty reads&lt;/h4&gt;

&lt;p&gt;如果你读了别的事务还没提交的数据就是脏读，如果覆盖了别的事务还没提交的数据就是脏写。
举个例子，如果你的账户余额为0，查看余额的时候，有人刚好在给你转账，
先给你加了 500，然后他的账户余额不够事务失败了，然后又 rollback，结果你刷新发现又变成了0。你看到 500 是脏数据，所以是脏读。
同理如果你给自己存钱的时候覆盖了这个 500，就是脏写。&lt;/p&gt;

&lt;p&gt;Read Committed 隔离级别数据库需要保证不会出现脏读，也就是看到的数据是已经 commit 的。&lt;/p&gt;

&lt;h4 id=&#34;no-dirty-writes&#34;&gt;No dirty writes&lt;/h4&gt;

&lt;p&gt;同样需要保证不会出现脏写，一般需要加锁，等待前一个事务 commit 或者 abort&lt;/p&gt;

&lt;h4 id=&#34;implementing-read-committed&#34;&gt;Implementing read committed&lt;/h4&gt;

&lt;p&gt;简单粗暴的方式就是加锁，这也是大部分数据库 保证 没有 No dirty writes 的方式，但是脏读也加锁就会降低性能。
所以脏读一般是通过保存两份数据，一个是未提交的数据，一个是已经提交的数据。读的时候返回已经提交的数据。&lt;/p&gt;

&lt;h3 id=&#34;2-snapshot-isolation-and-repeatable-read&#34;&gt;(2) Snapshot Isolation and Repeatable Read&lt;/h3&gt;

&lt;p&gt;貌似 上面已经很完美了，但是看看下面的例子：
还是两个人转账，A 把 500 块转给 B，首先把 A 设置为 0，然后 B 设置为 500。
此时B 查看了两个用户的账户，在还没开始的时候查看了A的，结束的时候查看了B的，发现两个人都是 0.&lt;/p&gt;

&lt;p&gt;上面的问题和之前的反了过来。脏读是读了修改到一半的数据，不可重复读是修改了读到一半的数据。
这貌似不是一个大问题，只要等一会儿再读，就发现是正确的。但是注意以下情况：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Backups 如果我再备份数据，类似这里的读。&lt;/li&gt;
&lt;li&gt;Analytic queries&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Snapshot isolation 就是解决这个问题的方法。实现 快照隔离需要用 MVCC 算法。
之前每条数据都有两个版本，未提交的和提交的，现在改成多个，也就是每一条保存多个版本的数据，算法如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;每个事务都有一个 txid。&lt;/li&gt;
&lt;li&gt;每一行数据有一个 created_by 段包含了 tx_id。&lt;/li&gt;
&lt;li&gt;每一行数据有一个 deleted_by 字段默认空值，如果数据删除了，实际上不会删的，二是 deleted_by 会有一个 txid 标志它被删掉。&lt;/li&gt;
&lt;li&gt;如果确定没有实物能访问到被删掉的数据，就真的删了被删掉的数据。&lt;/li&gt;
&lt;li&gt;update 等于 delete + create&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;读数据时候的 Visibility rules 如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;每个 transaction 开始的时候会将所有正在执行的 transactions 列出，这些事务的修改无论成功与否都 ignore。&lt;/li&gt;
&lt;li&gt;所有 abort 的事务做的修改 都 ignore。&lt;/li&gt;
&lt;li&gt;比它大的 txid 做出的修改都 ignore。&lt;/li&gt;
&lt;li&gt;其他的修改都是 可见的。&lt;/li&gt;
&lt;li&gt;这里的修改指的是 create 和 delete&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;换个方式，只有下面的情况才是 visible：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;transaction 开始的时候，已经 commit 的数据&lt;/li&gt;
&lt;li&gt;别的事物虽然已经删了数据，但是还没有提交的。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;3-indexes-and-snapshot-isolation&#34;&gt;(3) Indexes and snapshot isolation&lt;/h3&gt;

&lt;p&gt;在 multi-version 的 databases 里面 index 如何工作? 可以和数据一样，有多个索引，但也可以在有多个版本的时候避免索引更新。
还有的结构是 appendOnly 的结构。。。。&lt;/p&gt;

&lt;h3 id=&#34;4-repeatable-read-and-naming-confusion&#34;&gt;(4) Repeatable read and naming confusion&lt;/h3&gt;

&lt;p&gt;Repeatable read 其实一开始是没有的，也是慢慢被发现的，所以它的名字有很多，实现方式也不一样，提供的一致性保证也不一定一样。&lt;/p&gt;

&lt;h2 id=&#34;3-preventing-lost-updates&#34;&gt;3. Preventing Lost Updates&lt;/h2&gt;

&lt;p&gt;前面只讨论了 dirty writes，类似我们讨论的并发写，还有一些问题我们没有讨论。最有名的就是  Lost Updates，例如并发给某个数据 +1.
一般可能出现的情况：
1. 查询数据并增加，例如：转账的时候同时转账。
2. 同时编辑某个 wiki 文档的一个部分。
3. 修改一个 json 字符串&lt;/p&gt;

&lt;h3 id=&#34;1-atomic-write-operations&#34;&gt;(1) Atomic write operations&lt;/h3&gt;

&lt;p&gt;大部分 relational db 都支持 atomic 写，即 &lt;code&gt;UPDATE counters SET value = value + 1 WHERE key = &#39;foo&#39;;&lt;/code&gt; 而不需要 read-modify-write。&lt;/p&gt;

&lt;p&gt;一般情况下都是通过 exclusive lock 方式实现，这种方式也被称为  cursor stability，另一种就是强制要求所有的操作在一个线程里。&lt;/p&gt;

&lt;p&gt;但是有些 orm 框架让你很容易写出 unsafe 的并发更新bug而且很难查出来。&lt;/p&gt;

&lt;h3 id=&#34;2-explicit-locking&#34;&gt;(2) Explicit locking&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;SELECT * FROM figures
  WHERE name = &#39;robot&#39; AND game_id = 222
  FOR UPDATE;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样直接锁住记录&lt;/p&gt;

&lt;h3 id=&#34;3-automatically-detecting-lost-updates&#34;&gt;(3) Automatically detecting lost updates&lt;/h3&gt;

&lt;p&gt;加锁的方式是让操作序列化执行，同样可以让每个并发执行，如果发生了错误就 abort 重试。
Lost update detection 是一种很高级的特性。&lt;/p&gt;

&lt;h3 id=&#34;4-compare-and-set&#34;&gt;(4) Compare-and-set&lt;/h3&gt;

&lt;p&gt;这种方式就是写之前先读，如果数据没有变化再写。
但是如果写之前读的是还没提交的数据，可能还是有问题的。&lt;/p&gt;

&lt;h3 id=&#34;5-conflict-resolution-and-replication&#34;&gt;(5) Conflict resolution and replication&lt;/h3&gt;

&lt;p&gt;如果有多个副本，写数据都是并发，同步数据是异步的，这时候 locks 和 compare-and-set 的方式以及没法保证数据一致性了。&lt;/p&gt;

&lt;h2 id=&#34;4-write-skew-and-phantoms&#34;&gt;4. Write Skew and Phantoms&lt;/h2&gt;

&lt;p&gt;除了上面的问题，可能还有新的问题。看下嘛的例子：
我们有个医生门诊应用，每个医生可以请假，但是至少保证有一个医生没有请假。医生请假的时候会查询目前在岗的医生，发现有人就可以请假。
但是如果有一个时间所有的医生都同时查看，发现大家都在，然后大家都一起请假，就违背了约束。&lt;/p&gt;

&lt;h3 id=&#34;1-characterizing-write-skew&#34;&gt;(1) Characterizing write skew&lt;/h3&gt;

&lt;p&gt;这种问题叫做 write skew，既不是脏写也不是 lost update。对应的读问题叫做 phantoms read。
因为两个事务更新的是两个数据（每个医生设置自己的状态为请假），这里的冲突不太明显，但是确实违背了约束。&lt;/p&gt;

&lt;p&gt;write skew 可以看出广义的 lost update，write skew 是多个事务读了同一个数据然后更新了其中的部分数据。
如果更新的是同一条数据就会得到 dirty write 或者 lost update。&lt;/p&gt;

&lt;p&gt;lost update 我们有很多种方案可以解决，但是貌似这里就不行了：
1. Atomic single-object 没法解决，因为 这里有多个 obj
2. automatic detection 很难，因为这种问题需要 Serializable 的隔离级别
3. 通过添加唯一位数、主键约束都不一定可以，例如这里有多个医生的在岗状态，如果加约束需要每次都扫描全表。&lt;/p&gt;

&lt;p&gt;还有很多 write skew 的问题，
会议室预约系统如何保证一个会议室在同一时间不会被两个人预定？
Multiplayer game 怎么保证两个用户不会出现在同一个点，
用户名唯一性约束等&lt;/p&gt;

&lt;h3 id=&#34;2-phantoms-causing-write-skew&#34;&gt;(2) Phantoms causing write skew&lt;/h3&gt;

&lt;p&gt;上面的问题有一个共同点：
1. 查询是否满足多个条件
2. 根据上面的结果做出更新
3. 第二步的更新导致第一步的条件不成立&lt;/p&gt;

&lt;p&gt;注意一定是有更改，如果是只读的， snapshot isolation 就能保证不会出现异常。&lt;/p&gt;

&lt;h3 id=&#34;3-materializing-conflicts&#34;&gt;(3) Materializing conflicts&lt;/h3&gt;

&lt;p&gt;貌似有的问题是我们没法加锁，例如第一个 不知道给哪个医生加锁？但是如果能够物化冲突，找到加锁的地方就能够解决了。&lt;/p&gt;

&lt;p&gt;会议室预定的，我们可以给每个会议室每个时间段创建一条记录，然后加锁。问题在于有的我们很难物化，例如用户名唯一性，
不可能给每个用户名都创建一个索引。大多数还是使用 Serializability 的隔离级别&lt;/p&gt;

&lt;h2 id=&#34;4-serializability&#34;&gt;4. Serializability&lt;/h2&gt;

&lt;p&gt;这是目前 strongest isolation level，让数据库表现的类似所有的事务都 execute in parallel。&lt;/p&gt;

&lt;p&gt;如何实现  implementing 并且它的效率如何是需要考虑的问题。目前三种实现：顺序执行、2PL、SSI&lt;/p&gt;

&lt;h3 id=&#34;1-actual-serial-execution&#34;&gt;(1) Actual Serial Execution&lt;/h3&gt;

&lt;h4 id=&#34;encapsulating-transactions-in-stored-procedures&#34;&gt;Encapsulating transactions in stored procedures&lt;/h4&gt;

&lt;h3 id=&#34;2-two-phase-locking-2pl&#34;&gt;(2) Two-Phase Locking (2PL)&lt;/h3&gt;

&lt;p&gt;很久一段时间，实现 serial 只有一种方法，那就是 2PL，注意这里的 2pl 和 后面的 2pc 不一样。&lt;/p&gt;

&lt;p&gt;我们知道锁的作用就是更新时候保证一个是在另一个完成后进行的。2PL类似，很多个线程都可以同时读一条数据，
但是如果想要更改数据，就需要使用 exclusive access ：
1. A 如果读了数据 然后 B 想更改这条数据，B 需要等待 A 事务 commit 或者 abort
2. A 如果修改了数据 然后 B 想读这条数据，B 需要等待 A 事务 commit 或者 abort&lt;/p&gt;

&lt;p&gt;2PL 和 前面的锁不一样的地方在于，writers 不仅会锁住其他的 writers，还会锁住其他的 readers，反过来也一样。
这就是和 snapshot isolation 不一样的地方，所以他能够防止前面所有的错误。&lt;/p&gt;

&lt;h4 id=&#34;implementation-of-two-phase-locking&#34;&gt;Implementation of two-phase locking&lt;/h4&gt;

&lt;p&gt;实现 2PL 的方式就是给每个记录加锁，锁可以是 shared 或者 exclusive。使用方式如下：
1. 如果线程 A 想读 一个记录，需要获得 shared mode 的锁。所谓 shared mode 就是多个 shared mode 的线程可以共用锁
2. 如果线程 A 想写 一个记录，需要获得 exclusive mode 的锁。所谓 exclusive mode，就是只能一个人获得锁
3. 如果一个事务先 读然后写，需要将对应的 shared mode 的锁 升级为 exclusive mode 的锁。
4. 一个事务获得了锁，就需要一个 hold 住，知道结束（commit或者 abort）.&lt;/p&gt;

&lt;p&gt;这就是 two-phase 的由来，phase1 就是 acquired lock(execute transaction), phase2 就是 release locks(end of the transaction)
由于使用了很多 locks，很容易出现 A 等待 B释放锁，同时 B 等待 A，这就是 死锁，数据库一般可以自动发现死锁。&lt;/p&gt;

&lt;h4 id=&#34;performance-of-two-phase-locking&#34;&gt;Performance of two-phase locking&lt;/h4&gt;

&lt;p&gt;一直依赖不用 2PC 的原因就是性能太差了，一方面获得锁释放锁有性能开销，更重要的还是并发度太低。
而且一旦出现事务等待，不知道要等待多久。另外死锁出现的概率大了很多。&lt;/p&gt;

&lt;h4 id=&#34;predicate-locks&#34;&gt;Predicate locks&lt;/h4&gt;

&lt;p&gt;上面的预定会议室程序，如果一个用户预定了一个某个时间段的某个地点，另一个用户此时还可以预约，只要不是相同的时间和地点。&lt;/p&gt;

&lt;p&gt;这时候我们可以使用 Predicate locks，和 2PL 类似，但是不一样的是 2PL 对查询的某一条数据加锁，
Predicate locks 是加在所有的符合某个条件的所有数据。&lt;/p&gt;

&lt;p&gt;这里的关键在于 Predicate locks 不仅可以加在目前存在的记录上面，还可以加在未来会写进来的数据上。
如果 2PL 包含了 Predicate locks，就可以防止任何形式的 phantom，也就成了 Serializable。&lt;/p&gt;

&lt;h4 id=&#34;index-range-locks&#34;&gt;Index-range locks&lt;/h4&gt;

&lt;p&gt;Predicate locks 并不好用，如果有太多 lock，判断每个lock 很耗时，所以很多 2PL 使用的是 index-range locking。&lt;/p&gt;

&lt;p&gt;例如我们要锁住 某个会议室在 明天下午三点 ，这判断起来很麻烦，我们可以直接将这个会议室锁起来！或者把所有的会议室 下午三点都锁起来。
由于 room_id 或者 time 可能有索引，所以判断起来会比较快。&lt;/p&gt;

&lt;p&gt;index-range locks 并没有 Predicate locks 精确，但是可以降低查询量，也是一个好的折中。
如果没有索引，就锁住整个表格，这样就会降低性能，但是也是一个折中。&lt;/p&gt;

&lt;h3 id=&#34;serializable-snapshot-isolation-ssi&#34;&gt;Serializable Snapshot Isolation (SSI)&lt;/h3&gt;

&lt;p&gt;前面我们发现，提供的隔离性越好，性能就越差？貌似性能和隔离性一定是 odds？不一定，有个 SSI 算法比较新，提供了很好的性能。&lt;/p&gt;

&lt;h4 id=&#34;pessimistic-versus-optimistic-concurrency-control&#34;&gt;Pessimistic versus optimistic concurrency control&lt;/h4&gt;

&lt;p&gt;2PL 是 pessimistic 悲观锁，如果可能会出错那么就等待安全了再操作，类似 mutual exclusion。
Serial execution ，就是极端的 pessimistic。&lt;/p&gt;

&lt;p&gt;serializable snapshot isolation 是一种 optimistic concurrency control technique.
先假定不会出错，然后 commit 的时候判断有没有出错。&lt;/p&gt;

&lt;h3 id=&#34;decisions-based-on-an-outdated-premise&#34;&gt;Decisions based on an outdated premise&lt;/h3&gt;

&lt;p&gt;我们前面出现的 write skew 的时候，都有一个通用的模式：
一个 transaction 读一些数据，检查结果并且做一些动作，但是提交数据的时候，这些数据可能已经被另一个事务更改。&lt;/p&gt;

&lt;p&gt;transaction 会基于 premise 做出一些 action，等后面准备 commit 的时候，这个 premise 已经不正确了。
换句话说，读的数据和写的数据之间有一个 causal dependency。如何发现 outdated premise，主要有两种：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Detecting reads of a stale MVCC object version (uncommitted write occurred before the read)&lt;/li&gt;
&lt;li&gt;Detecting writes that affect prior reads (the write occurs after the read)&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&#34;detecting-stale-mvcc-reads&#34;&gt;Detecting stale MVCC reads&lt;/h4&gt;

&lt;p&gt;在 snapshot isolation 中使用了 multi-version 的数据，每个事务不会读到当前还没有提交的数据，
所以其他的线程可能修改了这个数据，为了防着这种情况发生，db 需要记录由于 MVCC visibility rules 而忽略的数据，
当这个事务 commit 的时候，检查这些数据是否提交，如果提交了就 abort。&lt;/p&gt;

&lt;p&gt;为何要等待 commit 的时候判断而不是直接放弃？因为 当前事务可能是只读，或者另一个事务会失败，或者另一个事务会持续很久。&lt;/p&gt;

&lt;h4 id=&#34;detecting-writes-that-affect-prior-reads&#34;&gt;Detecting writes that affect prior reads&lt;/h4&gt;

&lt;p&gt;当一个 transaction 写数据的时候，他需要查看最近读了这个数据的事务，通知这些事务他们的他们的数据已经过时了。&lt;/p&gt;

&lt;h4 id=&#34;performance-of-serializable-snapshot-isolation&#34;&gt;Performance of serializable snapshot isolation&lt;/h4&gt;

&lt;p&gt;和 2PL 相比，SSI 不用阻塞线程。读写之间是不会阻塞，导致延迟低。&lt;/p&gt;

&lt;p&gt;aborts rate 对性能影响很大。但是 SSI 对 长事务 的敏感程度 肯定比 2PL 和 real serial 低。&lt;/p&gt;

&lt;h1 id=&#34;第五部分-案例&#34;&gt;第五部分、案例&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://medium.freecodecamp.org/how-to-system-design-dda63ed27e26&#34;&gt;https://medium.freecodecamp.org/how-to-system-design-dda63ed27e26&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://blog.gainlo.co/index.php/2015/10/22/8-things-you-need-to-know-before-system-design-interviews/&#34;&gt;http://blog.gainlo.co/index.php/2015/10/22/8-things-you-need-to-know-before-system-design-interviews/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://engineering.videoblocks.com/web-architecture-101-a3224e126947&#34;&gt;https://engineering.videoblocks.com/web-architecture-101-a3224e126947&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;第六部分-资料&#34;&gt;第六部分、资料&lt;/h1&gt;

&lt;p&gt;es设计架构，良心参考资料：
&lt;a href=&#34;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-i-7ac9a13b05db&#34;&gt;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-i-7ac9a13b05db&lt;/a&gt;
&lt;a href=&#34;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-ii-6db4e821b571&#34;&gt;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-ii-6db4e821b571&lt;/a&gt;
&lt;a href=&#34;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-iii-8bb6ac84488d&#34;&gt;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-iii-8bb6ac84488d&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;lsm b+tree 资料
&lt;a href=&#34;https://medium.com/databasss/on-disk-io-part-3-lsm-trees-8b2da218496f&#34;&gt;https://medium.com/databasss/on-disk-io-part-3-lsm-trees-8b2da218496f&lt;/a&gt;&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>分布式存储-partition</title>
      <link>https://dengziming.github.io/post/data-intensive-architecture/partition/</link>
      <pubDate>Sat, 22 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/data-intensive-architecture/partition/</guid>
      
        <description>

&lt;h1 id=&#34;二-partitioning&#34;&gt;二、Partitioning&lt;/h1&gt;

&lt;p&gt;分区，also known as Sharding、region、tablet、vBucket&lt;/p&gt;

&lt;p&gt;partition 的主要目的是 scalability，其实相对于 其他分布式理论，partition 是最简单的，没有很多复杂的一致性问题。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>es架构-1</title>
      <link>https://dengziming.github.io/post/es/es%E6%9E%B6%E6%9E%84-1/</link>
      <pubDate>Sun, 22 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/es/es%E6%9E%B6%E6%9E%84-1/</guid>
      
        <description>

&lt;p&gt;es设计架构，良心参考资料：
&lt;a href=&#34;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-i-7ac9a13b05db&#34;&gt;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-i-7ac9a13b05db&lt;/a&gt;
&lt;a href=&#34;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-ii-6db4e821b571&#34;&gt;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-ii-6db4e821b571&lt;/a&gt;
&lt;a href=&#34;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-iii-8bb6ac84488d&#34;&gt;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-iii-8bb6ac84488d&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;一-anatomy-of-an-elasticsearch-cluster&#34;&gt;一、Anatomy of an Elasticsearch Cluster&lt;/h2&gt;

&lt;p&gt;很遗憾，Google的搜索技术不开源，es是搜索引擎的一个很好的替代品，本文主要覆盖了es的底层结构、数据原型、读写过程。es的功能主要有：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;全文搜索
例如：怎么找到Wikipedia上面和某个名字最相关的文章&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;聚合&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;例如 显示广告网络上面的词条出价直方图&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;地理空间API&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;例如：设计一个能找到和骑手最近司机的骑行分享平台&lt;/p&gt;

&lt;p&gt;接下来就是内容，主要有以下几个方面：
1. 是主从架构还是无主架构
2. 存储模型
3. 读写工作流程
4. 搜索结果怎么相关&lt;/p&gt;

&lt;h3 id=&#34;1-the-confusion-between-elasticsearch-index-and-lucene-index-other-common-terms&#34;&gt;1.The confusion between Elasticsearch Index and Lucene Index + other common terms…&lt;/h3&gt;

&lt;p&gt;es 的 index 是一个组织数据的逻辑空间，就类似一个数据库。es index有一到多个 shards，一个shard就是一个真正存数据的lucene index，内部就是一个搜索引擎。&lt;/p&gt;

&lt;p&gt;每个 shard 都有0到多个replica，es index 有 type 的概念，就好比数据库里面的表，一个type里的所以type有相同的properties，就像schema一样。&lt;/p&gt;

&lt;h3 id=&#34;2-types-of-nodes&#34;&gt;2. Types of nodes&lt;/h3&gt;

&lt;h4 id=&#34;1-master-node&#34;&gt;（1）Master Node&lt;/h4&gt;

&lt;p&gt;控制集群，负责集群的操作，例如创建删除index，和集群的nodes联系，给节点分配shards。主节点一次处理一个集群状态，并将状态广播到所有节点，收到广播的节点对主节点进行确认回复。
an be configured to be eligible to become a master node by setting the node.master property to be true (default) in elasticsearch.yml.
大集群最好有专门的master node，去空值集群，不用处理任何用户请求&lt;/p&gt;

&lt;h4 id=&#34;2-data-node&#34;&gt;（2） Data Node&lt;/h4&gt;

&lt;p&gt;保存数据和倒排索引，By default, every node is configured to be a data node and the property node.data is set to true in elasticsearch.yml.
If you would like to have a dedicated master node, then change the node.data property to false.&lt;/p&gt;

&lt;h4 id=&#34;3-client-node&#34;&gt;（3）Client Node:&lt;/h4&gt;

&lt;p&gt;If you set both node.master and node.data to false, then the node gets configured as a client node and acts as a load balancer routing incoming requests to different nodes in the cluster.&lt;/p&gt;

&lt;h4 id=&#34;4-coordinating-node&#34;&gt;（4）coordinating node&lt;/h4&gt;

&lt;p&gt;注意没有专门的coordinating node，通过client连上的的es节点称为 coordinating node，将client request 路由到合适的shard。对于读请求，每次选择不同的shard 从而 balance the load.&lt;/p&gt;

&lt;h3 id=&#34;2-storage-model&#34;&gt;2. Storage Model&lt;/h3&gt;

&lt;p&gt;Elasticsearch uses Apache Lucene, a full-text search library written in Java and developed by Doug Cutting (creator of Apache Hadoop)。
es内部通过倒排索引的数据结构，从而处理可能延迟的查询。es中 document 是数据的存储 unit，通过将document的词进行分词创建 inverted index ，倒排索引能够创建排序的term并将和这个term相关的document进行管理。
和每本书背后的index类似，包含了很多词和那一页可以找到这些词，例如下面的两个document。&lt;/p&gt;

&lt;p&gt;Doc 1: Insight Data Engineering Fellows Program
Doc 2: Insight Data Science Fellows Program&lt;/p&gt;

&lt;p&gt;If we want to find documents which contain the term “insight”, we can scan the inverted index (where words are sorted), find the word “insight” and return the document IDs which contain this word, which in this case would be Doc 1 and Doc 2.&lt;/p&gt;

&lt;p&gt;为了更好的搜索性，文档先被分析。一般就是分词+标准化。&lt;/p&gt;

&lt;p&gt;综上，我们知道每个document存储模型，存储了document，以及对他们分词后的倒排索引。&lt;/p&gt;

&lt;h3 id=&#34;3-anatomy-of-a-write&#34;&gt;3.Anatomy of a Write&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&amp;copy;reate&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;When you send a request to the coordinating node to index a new document, the following set of operations take place:&lt;/p&gt;

&lt;p&gt;es所有的节点都包含了集群的元数据信息，包括哪些节点或者，有哪些分片。The coordinating node 通过 documentId将document和他对应的shard route起来，
es再通过 murmur3 hash算法将documentId进行取值，得到shard。&lt;code&gt;shard = hash(document_id) % (num_of_primary_shards)&lt;/code&gt;。
当节点收到 coordinating node 的 request ，request 会被写入到 translog 中，document 会被放进 memory buffer（&lt;a href=&#34;http://www.linfo.org/buffer.html），&#34;&gt;http://www.linfo.org/buffer.html），&lt;/a&gt;
如果在primary shard上执行成功，reques也会被发送到 replica shard上，
当 translog fsynced （&lt;a href=&#34;https://linux.die.net/man/2/fsync）&#34;&gt;https://linux.die.net/man/2/fsync）&lt;/a&gt; on all primary and replica shards.client receives acknowledgement that the request was successful。&lt;/p&gt;

&lt;p&gt;memory buffer会周期性更新 (defaults to 1 sec)，contents 会被写到一个 a new segment in filesystem cache ，
This segment is not yet fsync’ed, however, the segment is open and the contents are available for search.&lt;/p&gt;

&lt;p&gt;The translog is emptied and filesystem cache is fsync’ed every 30 minutes or when the translog gets too big. 这个过程称为flush
the in-memory buffer is cleared and the contents are written to a new segment.
A new commit point is created with the segments fsync’ed and flushed to disk. The old translog is deleted and a fresh one begins.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;(U)pdate and (D)elete&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;es的记录是无法更改的，删除和update实际上是新建，更改版本号。每个segment 都有一个 .del  file。
When a delete request is sent, the document is not really deleted, but marked as deleted in the .del file.
This document may still match a search query but is filtered out of the results.
When segments are merged, the documents marked as deleted in the .del file are not included in the new merged segment.&lt;/p&gt;

&lt;p&gt;update则是新建+删除，es给每个document一个version，每次改变，version都+增加，旧版本会被.del 文件标记为删除，
和删除一样，This older document may still match a search query but is filtered out of the results.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Anatomy of a &amp;reg;ead
Read operations consist of two parts:&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Query Phase
Fetch Phase&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Query Phase
coordinating node route the search request to all the shards (primary or replica) in the index.
每个shard单独search，然后将结果放进一个优先队列，根据 relevance score (we’ll cover relevance score later in the post).
所有 shards将结果汇总，创建一个新的优先队列，取出相关度最高的一部分。这个过程类似spark的topn&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Fetch Phase
coordinating node 排好序之后，
it then requests the original documents from all the shards. All the shards enrich the documents and return them to the coordinating node.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;相关度：tf/idf （term frequency/inverse document frequency）算法。
tf 词频，在某文档出现的频率
idf 出现过得所有文档数&lt;/p&gt;

&lt;h2 id=&#34;what-next&#34;&gt;What next?&lt;/h2&gt;

&lt;p&gt;lit brain problem in Elasticsearch and how to avoid it
Transaction log
Lucene segments
Why deep pagination during search is dangerous?
Difficulties and trade-offs in calculating search relevance
Concurrency control
Why is Elasticsearch near real-time?
How to ensure consistent writes and reads?&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>es架构-2</title>
      <link>https://dengziming.github.io/post/es/es%E6%9E%B6%E6%9E%84-2/</link>
      <pubDate>Sun, 22 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/es/es%E6%9E%B6%E6%9E%84-2/</guid>
      
        <description>

&lt;p&gt;es设计架构，良心参考资料：
&lt;a href=&#34;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-i-7ac9a13b05db&#34;&gt;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-i-7ac9a13b05db&lt;/a&gt;
&lt;a href=&#34;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-ii-6db4e821b571&#34;&gt;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-ii-6db4e821b571&lt;/a&gt;
&lt;a href=&#34;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-iii-8bb6ac84488d&#34;&gt;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-iii-8bb6ac84488d&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;一-anatomy-of-an-elasticsearch-cluster-2&#34;&gt;一、Anatomy of an Elasticsearch Cluster -2&lt;/h2&gt;

&lt;p&gt;上一节我们说了：underlying storage model and CRUD operations in Elasticsearch.这一节的内容主要包括：
Consensus — split-brain problem and importance of quorum
Concurrency
Consistency: Ensuring consistent writes and reads
Translog (Write Ahead Log — WAL)
Lucene segments&lt;/p&gt;

&lt;h3 id=&#34;1-consensus-split-brain-problem-and-importance-of-quorum&#34;&gt;1. Consensus- Split-brain problem and importance of quorum&lt;/h3&gt;

&lt;p&gt;Consensus 算法包括 Raft、Paxos等，为了解决一致性问题，es的一致性算法有两个部分：&lt;/p&gt;

&lt;p&gt;Ping: The process nodes use to discover each other
Unicast: The module that contains a list of hostnames to control which nodes to ping&lt;/p&gt;

&lt;p&gt;es是一个P2P的系统，所有节点都和其他节点沟通，有一个主节点，控制和更新集群操作。一个新的集群需要经过选举，一个节点被选为master，其他的加入master。
As nodes join, they send a join request to the master with a default join_timeout which is 20 times the ping_timeout.
如果mster节点挂了，cluster重新开始ping，开始新的选举。这种ping过程也帮忙解决脑裂（某个节点突然觉得maste挂了，开始寻找新master）&lt;/p&gt;

&lt;p&gt;为了容错，master会ping 所有的 节点去检查是否 alive然后节点会ping master进行response。
默认配置下，es可能会有脑裂， 由于 network partition,a node 觉得 master 已经 failed然后自己当上master，导致连个master。
This may result in data loss and it may not be possible to merge the data correctly.
This can be avoided by setting the following property to a quorum of master eligible nodes.
&lt;code&gt;discovery.zen.minimum_master_nodes = int(# of master eligible nodes/2)+1&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;设置这个配置之后，需要有 quorum of active master eligible nodes 参加完成master选举过程，并接受他的master身份。
This is an extremely important property to ensure cluster stability and can be dynamically updated if the cluster size changes.
NOTE: For a production cluster, it is recommended to have 3 dedicated master nodes, which do not serve any client requests, out of which 1 is active at any given time.&lt;/p&gt;

&lt;p&gt;这就是 Consensus 的内容&lt;/p&gt;

&lt;h3 id=&#34;2-concurrency&#34;&gt;2. Concurrency&lt;/h3&gt;

&lt;p&gt;对于高并发，es使用 optimistic concurrency control 乐冠锁进行控制，保证新纪录不被就记录覆盖。
每个document indexed 有一个 version number which is incremented with every change applied to that document。保证每次更新都能按照顺序。
为了保证数据不丢失，es可以让你自己指定id，如果你指定的id比present的小，更新就失败了。
How failed requests are handled can be controlled at the application level.
There are also other locking options available and you can read about them：&lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/guide/2.x/concurrency-solutions.html&#34;&gt;https://www.elastic.co/guide/en/elasticsearch/guide/2.x/concurrency-solutions.html&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;3-consistency-ensuring-consistent-writes-and-reads&#34;&gt;3. Consistency — Ensuring consistent writes and reads&lt;/h3&gt;

&lt;p&gt;写一致
对于怎样算写成功，可以设置 available的 shards 数量
The available options are quorum, one and all. By default it is set to quorum and that means that a write operation will be permitted only if a majority of the shards are available.&lt;/p&gt;

&lt;p&gt;尽管大多数available，也有可能出错。the replica is said to be faulty and the shard would be rebuilt on a different node.&lt;/p&gt;

&lt;p&gt;读一致：
new documents are not available for search until after the refresh interval.
为了保证读到最新的document，replication can be set to sync (default) 。这样只有 primary and replica shards 都写完了才会返回 write request 。
这样，从任何一个shard查询，都将返回最新的document。
Even if your application requires replication=async for higher indexing rate, there is a _preference parameter which can be set to primary for search requests.
这样，查询都走 primary shard ，保证结果都来自最新版本。&lt;/p&gt;

&lt;h3 id=&#34;2-translog&#34;&gt;2. Translog&lt;/h3&gt;

&lt;p&gt;WAL来自关系型数据库的世界，translog保证事件失败时候的数据完整性，通过底层的原则 :在将数据提交到磁盘的实际更改之前，必须记录并提交预期的更改。&lt;/p&gt;

&lt;p&gt;当新 document被index，或者旧的更改，Lucene index会改变，然后改变会被提交到磁盘进行持久化。每次update都进行提交很expensive，更好地办法是一次性提交很多。
上一节提到的，flush操作默认30min一次或者translog太大，这样的话，有可能丢失30min的数据。为了避免这个问题，es使用translog，update操作都将被写到translog，
translog is fsync’ed after every index/delete/update operation (or every 5 sec by default) 保证改变被持久化。
The client receives acknowledgement for writes after the translog is fsync’ed on both primary and replica shards.&lt;/p&gt;

&lt;p&gt;当两次flush之间出现问题，translog将会重新执行，从而恢复丢失的change，
NOTE: It is recommended to explicitly flush the translog before restarting Elasticsearch instances,
as the startup will be faster because the translog to be replayed will be empty.
POST /_all/_flush command can be used to flush all indices in the cluster.&lt;/p&gt;

&lt;p&gt;上面已经说了translog的flush操作，segments文件会被提交到磁盘来保证改变持久化。接下来我们看看什么是lucene的 segment。&lt;/p&gt;

&lt;h3 id=&#34;3-lucene-segments&#34;&gt;3.Lucene Segments&lt;/h3&gt;

&lt;p&gt;Lucene索引由多个段组成，并且段本身是一个全功能倒置索引。
是不可变的，它允许Lucene在不从头开始重建索引的情况下增量地向索引添加新文档。
对于每一个搜索请求，索引中的所有段都被搜索，并且每个段消耗CPU周期、文件句柄和内存。这意味着段数越高，搜索性能就越低。
为了解决这个问题，es将小的段合并成更大的段，将新合并的段提交到磁盘并删除旧的较小的段。
对于搜索请求，搜索给定的弹性搜索索引碎片中的所有Lucene段，但是，在排名结果中提取所有匹配的文档或文档对于弹性搜索集群是危险的。&lt;/p&gt;

&lt;h2 id=&#34;what-next&#34;&gt;What next?&lt;/h2&gt;

&lt;p&gt;在后续文章中，我们将看到这一点，并回顾下面的主题，其中包括在弹性搜索中进行的一些权衡，以在低延迟下服务相关搜索结果。&lt;/p&gt;

&lt;p&gt;lit brain problem in Elasticsearch and how to avoid it
Transaction log
Lucene segments
Why deep pagination during search is dangerous?
Difficulties and trade-offs in calculating search relevance
Concurrency control
Why is Elasticsearch near real-time?
How to ensure consistent writes and reads?&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>es架构-3</title>
      <link>https://dengziming.github.io/post/es/es%E6%9E%B6%E6%9E%84-3/</link>
      <pubDate>Sun, 22 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/es/es%E6%9E%B6%E6%9E%84-3/</guid>
      
        <description>

&lt;p&gt;es设计架构，良心参考资料：
&lt;a href=&#34;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-i-7ac9a13b05db&#34;&gt;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-i-7ac9a13b05db&lt;/a&gt;
&lt;a href=&#34;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-ii-6db4e821b571&#34;&gt;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-ii-6db4e821b571&lt;/a&gt;
&lt;a href=&#34;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-iii-8bb6ac84488d&#34;&gt;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-iii-8bb6ac84488d&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;一-anatomy-of-an-elasticsearch-cluster-3&#34;&gt;一、Anatomy of an Elasticsearch Cluster -3&lt;/h2&gt;

&lt;p&gt;上一节我们说了：how Elasticsearch approaches some of the fundamental challenges of a distributed system.这一节的内容主要包括：&lt;/p&gt;

&lt;p&gt;Near real-time search&lt;/p&gt;

&lt;p&gt;Why deep pagination in distributed search can be dangerous?&lt;/p&gt;

&lt;p&gt;Trade-offs in calculating search relevance&lt;/p&gt;

&lt;h3 id=&#34;1-near-real-time-search&#34;&gt;1. Near real-time search&lt;/h3&gt;

&lt;p&gt;尽管改变不会立即可见，但是es确实提供了近实时查询，前面的内容提到，lucene的segment改变提交到磁盘是很消耗性能的。&lt;/p&gt;

&lt;p&gt;为了避免search的同时提交改变到磁盘，es在memory buffer和disk之间提供了一个 filesystem cache，memory buffer 默认1s refreshed 一次，
一个包含倒排索引的segment也会在 filesystem cache中生成。segment是开放的可以查询。&lt;/p&gt;

&lt;p&gt;filesystem cache有文件句柄而且可以打开，读写，关闭，尽管它在内存中。
Since, the refresh interval is 1 sec by default,  the changes are not visible right away ，所以是准实时。
Since, the translog is a persistent record of changes not persisted to the disk, it also helps with the near real-time aspect for CRUD operations.
在查找相关段之前，在 translog 中搜索任何最近的变化，因此，客户端可以访问近实时的所有变化。&lt;/p&gt;

&lt;p&gt;你可以每次更新后手动刷新index，但是这样会产生很多小segment，不推荐。
For a search request, all Lucene segments in a given shard of an Elasticsearch index are searched。
however, fetching all matching documents or documents deep in the resulting pages is dangerous for your Elasticsearch cluster. Let’s see why that is.&lt;/p&gt;

&lt;h3 id=&#34;2-why-deep-pagination-in-distributed-search-can-be-dangerous&#34;&gt;2. Why deep pagination in distributed search can be dangerous?&lt;/h3&gt;

&lt;p&gt;当我们查到的es中有很多匹配到的document，默认返回最相关的10条，分页操作会提供from和size两个参数，然后每个shard上面会产生from+size个结果放进优先队列，最后汇总。&lt;/p&gt;

&lt;p&gt;加入你需要的是10000到100010个结果，每个shard的优先队列会返回10010个结果排序后放进 memory中，这样将会有很大的隐患。
scroll API （&lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-scroll.html）可以让你返回所有的结果。&#34;&gt;https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-scroll.html）可以让你返回所有的结果。&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;前面说到es使用tf-idf算法，分布式系统计算idf时很麻烦的，需要有aggregate操作。es的做法是返回一个local idf：
Instead, every shard calculates a local idf to assign a relevance score to the resulting documents and returns the result for only the documents on that shard.
Similarly, all the shards return the resulting documents with relevant scores calculated using local idf and the coordinating node sorts all the results to return the top ones。
大多数情况可靠，但是数据倾斜时候就不太可靠了。&lt;/p&gt;

&lt;p&gt;对于上面的问题有两种 trade-off，都不太适合大规模数据。一种是只有一个shard，这样local idf就是 globe idf。另一种是 dfs_query_then_search，先把local idf合并成globe idf，然后在计算。&lt;/p&gt;

&lt;h2 id=&#34;what-next&#34;&gt;What next?&lt;/h2&gt;

&lt;p&gt;In the last few posts, we reviewed some of the fundamental principles of Elasticsearch which are important to understand in order to get started.&lt;/p&gt;

&lt;p&gt;接下来，我们要看看他的源码，或者看看怎么和spark、hadoop结合使用。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>hadoop网站日志分析项目架构</title>
      <link>https://dengziming.github.io/post/project/hadoop/hadoop/</link>
      <pubDate>Fri, 30 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/project/hadoop/hadoop/</guid>
      
        <description>

&lt;p&gt;项目简介：大数据涉及到的业务很多很复杂，从一开始的项目架构，再到后台的网站搭建，以及数据的收集，数据的分析，数据的迁移，业务开发，后台运维，等等。我们没办法一个实验将所有的过程都学习到。本次试验我们将会将重点放在项目架构上，后面的项目我们将重点放在每一部分的实现上。通过本次实验，你将能了解到一个大数据架构师工作的基本步骤，虽然本次实验我们也有复杂的代码分析过程，但是大家没有必要将自己的重点放在代码上面，大家应该更加站在架构师的角度，专注于整个项目每一部分的连接，每个部分具体实现的细节，大家可以不必太深入，我们后期会有专门的实验放在这上面。
有关代码我们已经实现并且提供，大家直接打开，然后阅读熟悉每部分的意义即可。
本次项目我们是架构一个日志分析，我们的要完成的任务包括后台和前端的实现，网站的搭建，nginx反向代理的搭建，etl数据清洗程序，数据分析，数据报表的实现。&lt;/p&gt;

&lt;h2 id=&#34;一-业务分析和需求文档&#34;&gt;一、业务分析和需求文档&lt;/h2&gt;

&lt;h3 id=&#34;1-业务分析概述&#34;&gt;1.业务分析概述&lt;/h3&gt;

&lt;p&gt;本次试验我们主要是分析类似淘宝等购物网站上的点击流，从而进行展示分析。在本次项目中我们分别从七个大的角度来进行分析，分别为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;用户基本信息分析模块、浏览器信息分析模块、地域信息分析模块、用户浏览深度分析模块、外链数据分析模块、订单分析模块以及事件分析模块。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意几个概念:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;用户/访客：表示同一个浏览器代表的用户。唯一标示用户
会员：表示网站的一个正常的会员用户。
会话：一段时间内的连续操作，就是一个会话中的所有操作。
Pv：访问页面的数量
在本次项目中，所有的计数都是去重过的。比如：活跃用户/访客，计算uuid的去重后的个数。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们分析数据的需求文档和最终的展示结果大概如下。&lt;/p&gt;

&lt;h3 id=&#34;2-用户基本信息分析模块&#34;&gt;2.用户基本信息分析模块&lt;/h3&gt;

&lt;p&gt;用户基本信息分析模块主要是从用户/访客和会员两个主要角度分析浏览相关信息，包括但不限于新增用户，活跃用户，总用户，新增会员，活跃会员，总会员以及会话分析等。下面就各个不同的用户信息角度来进行分析：&lt;/p&gt;

&lt;h4 id=&#34;1-用户分析&#34;&gt;(1).用户分析&lt;/h4&gt;

&lt;p&gt;该分析主要分析新增用户、活跃用户以及总用户的相关信息。
新访客:老访客(活跃访客中) =  1:7~10
&lt;img src=&#34;../images/2017-06-20-21-21-54.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;2-会员分析&#34;&gt;(2).会员分析&lt;/h4&gt;

&lt;p&gt;该分析主要分析新增会员、活跃会员以及总会员的相关信息。
&lt;img src=&#34;../images/2017-06-20-21-22-26.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;3-会话分析&#34;&gt;(3).会话分析&lt;/h4&gt;

&lt;p&gt;该分析主要分析会话个数、会话长度和平均会话长度相关的信息。
&lt;img src=&#34;../images/2017-06-20-21-23-17.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;4-hourly分析&#34;&gt;(4).Hourly分析&lt;/h4&gt;

&lt;p&gt;该分析主要分析每天每小时的用户、会话个数以及会话长度的相关信息。
&lt;img src=&#34;../images/2017-06-20-21-24-07.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;3-其他模块分析&#34;&gt;3.其他模块分析&lt;/h3&gt;

&lt;p&gt;在用户模块的基础上，我们可以添加其他的六个模块分析，我们本次试验先不展示所有的模块，只是作简单介绍，例如地域分布模块：
&lt;img src=&#34;../images/2017-06-20-21-25-43.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;上面分析的业务需求大家可能不太懂，没关系，注意在下面的项目中，时不时回头看看我们的需求，就能明白了。&lt;/p&gt;

&lt;h2 id=&#34;二-开发环境搭建&#34;&gt;二、开发环境搭建&lt;/h2&gt;

&lt;p&gt;为了方便管理，我们以后按照管理，尽量使用maven构建java和scala项目。另外我们的软件安装在D盘的&lt;code&gt;soft&lt;/code&gt;目录下，我们的开发项目放在D盘的&lt;code&gt;workspace&lt;/code&gt;目录下。&lt;/p&gt;

&lt;h3 id=&#34;1-下载安装软件&#34;&gt;1.下载安装软件&lt;/h3&gt;

&lt;p&gt;分别在&lt;code&gt;http://tomcat.apache.org&lt;/code&gt;和&lt;code&gt;http://maven.apache.org&lt;/code&gt;下载tomcat和maven，解压后放在D盘的soft目录，然后配置环境变量，需要配置的环境变量包括 &lt;code&gt;MAVEN_HOME&lt;/code&gt;和&lt;code&gt;TOMCAT_HOME&lt;/code&gt;，并且将他们的bin目录添加到&lt;code&gt;PATH&lt;/code&gt;中。安装配置完成后，在命令行输入start-up和mvn命令，检查是否安装正确。
确保无误后，我们的开发环境使用IDEA，安装好IDEA，打开，配置maven的目录，如下图的方法，搜索maven，在Maven的配置填写maven的路径。
&lt;img src=&#34;../images/2017-06-20-19-07-00.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;2-搭建服务器&#34;&gt;2.搭建服务器&lt;/h3&gt;

&lt;h4 id=&#34;1-布置开发环境&#34;&gt;(1).布置开发环境&lt;/h4&gt;

&lt;p&gt;如果大家熟悉javaEE开发，这一段就比较简单。我们搭建服务器就是新建一个javaEE项目，然后启动，这个过程需要借助tomcat实现。首先打开IDEA，IDEA中已经配置好了maven的路径。
点击File -&amp;gt;New -&amp;gt; Project ，选择java的web application，然后下一步:
&lt;img src=&#34;../images/2017-06-20-19-09-55.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在下一步我们设置项目路径，我们的项目名为&lt;code&gt;taobaopayment&lt;/code&gt;，放在D盘下的workspace目录下。然后点击完成。这时候就新建了一个web项目，我们在项目的web文件下能看到一个index.jsp文件，这个文件你可以修改为自定义的内容，例如我修改为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;%@ page contentType=&amp;quot;text/html;charset=UTF-8&amp;quot; language=&amp;quot;java&amp;quot; %&amp;gt;
&amp;lt;html&amp;gt;
  &amp;lt;head&amp;gt;
    &amp;lt;title&amp;gt;taobaopayment&amp;lt;/title&amp;gt;
  &amp;lt;/head&amp;gt;
  &amp;lt;body&amp;gt;
  支付页面
  &amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;2-本地发布项目&#34;&gt;(2).本地发布项目&lt;/h4&gt;

&lt;p&gt;菜单栏选择Run -&amp;gt;Edit Configuration或者点击右上角按钮添加tomcat的发布参数，依次点击 加号 -&amp;gt;tomcat server -&amp;gt; local ，添加tomcat：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/2017-06-20-19-18-24.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在右边的配置页面配置好名字、地址、端口：
&lt;img src=&#34;../images/2017-06-20-19-20-30.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;然后在deployment选项下面点击加号添加发布选项，然后设置你content名字，我们设置为 &lt;code&gt;taobaopayment&lt;/code&gt;：
&lt;img src=&#34;../images/2017-06-20-19-21-52.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;点击确定后，我们可以看到右上方和下方都出现了可以启动的三角形按钮，点击启动：
&lt;img src=&#34;../images/2017-06-20-19-25-24.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;启动成功后打开浏览器，输入&lt;code&gt;http://localhost:8080/taobaopayment/&lt;/code&gt;，出现我们刚刚编辑的jsp页面，剩下的操作自己实验。
&lt;img src=&#34;../images/2017-06-20-19-24-42.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;三-web服务器开发&#34;&gt;三、Web服务器开发&lt;/h2&gt;

&lt;p&gt;根据我们的需求文档，我们需要实现支付成功和退款页面。这里又分为两部分，一是前端的页面传来的请求数据，这部分代码使用JavaScript编写，另一方面是后台的服务器发送过来的代码，通过Java语言编写。&lt;/p&gt;

&lt;h3 id=&#34;1-后端开发&#34;&gt;1.后端开发&lt;/h3&gt;

&lt;h4 id=&#34;1-程序后台事件分析&#34;&gt;(1).程序后台事件分析&lt;/h4&gt;

&lt;p&gt;本项目中在程序后台会有chargeSuccess事件，本事件的主要作用是发送订单成功的信息给nginx服务器。发送格式同pc端发送方式， 也是访问同一个url来进行数据的传输。格式为:
&lt;code&gt;http://hongyahuayu.com/index.jpg?query1=spark&lt;/code&gt;
当会员最终支付成功的时候触发chargeSuccess该事件，该事件需要程序主动调用，然后向后台发送数据：
&lt;code&gt;u_mid=maomao&amp;amp;c_time=1449142044528&amp;amp;oid=orderid_1&amp;amp;ver=1&amp;amp;en=e_cs&amp;amp;pl=jdk&amp;amp;sdk=java&lt;/code&gt;，其中 &lt;code&gt;u_mid&lt;/code&gt;和&lt;code&gt;oid&lt;/code&gt;代表用户id和订单id。
前面我们分析了后端的业务，如果你不太懂，我们尅简单地说，后端程序的工作流如下：
&lt;img src=&#34;../images/2017-06-20-21-52-16.jpg&#34; alt=&#34;&#34; /&gt;
简单说，后端就是要设计方法，当&lt;code&gt;chargeSuccess&lt;/code&gt;触发的时候，我们给后台发送数据。&lt;/p&gt;

&lt;h4 id=&#34;2-后端程序开发&#34;&gt;(2).后端程序开发&lt;/h4&gt;

&lt;p&gt;程序开发有一定难度，另外由于我们本次试验的重点是后面的数据分析，这一块不作太高的要求，大家能够理解即可，核心代码如下，其余代码可以在项目中查看：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;	public static boolean onChargeSuccess(String orderId, String memberId) {
		try {
			if (isEmpty(orderId) || isEmpty(memberId)) {
				// 订单id或者memberid为空
				log.log(Level.WARNING, &amp;quot;订单id和会员id不能为空&amp;quot;);
				return false;
			}
			// 代码执行到这儿，表示订单id和会员id都不为空。
			Map&amp;lt;String, String&amp;gt; data = new HashMap&amp;lt;String, String&amp;gt;();
			data.put(&amp;quot;u_mid&amp;quot;, memberId);
			data.put(&amp;quot;oid&amp;quot;, orderId);
			data.put(&amp;quot;c_time&amp;quot;, String.valueOf(System.currentTimeMillis()));
			data.put(&amp;quot;ver&amp;quot;, version);
			data.put(&amp;quot;en&amp;quot;, &amp;quot;e_cs&amp;quot;);
			data.put(&amp;quot;pl&amp;quot;, platformName);
			data.put(&amp;quot;sdk&amp;quot;, sdkName);
			// 创建url
			String url = buildUrl(data);
			// 发送url&amp;amp;将url加入到队列
			SendDataMonitor.addSendUrl(url);
			return true;
		} catch (Throwable e) {
			log.log(Level.WARNING, &amp;quot;发送数据异常&amp;quot;, e);
		}
		return false;
	}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;注意事项&lt;/em&gt;&lt;/strong&gt; 修改代码这里url地址为自己服务器的地址：
&lt;img src=&#34;../images/2017-06-20-23-42-54.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;2-前端开发&#34;&gt;2.前端开发&lt;/h3&gt;

&lt;h4 id=&#34;1-前端事件分析&#34;&gt;(1).前端事件分析&lt;/h4&gt;

&lt;p&gt;前面我们说后端的事件主要是chargeSuccess，前端的时间处理就更复杂了。针对我们最终的不同分析模块，我们需要不同的数据，接下来分别从各个模块分析，每个模块需要的数据。
1. 用户基本信息就是用户的浏览行为信息分析，也就是我们只需要pageview事件就可以了；
2. 浏览器信息分析以及地域信息分析其实就是在用户基本信息分析的基础上添加浏览器和地域这个维度信息，其中浏览器信息我们可以通过浏览器的window.navigator.userAgent来进行分析，地域信息可以通过nginx服务器来收集用户的ip地址来进行分析，也就是说pageview事件也可以满足这两个模块的分析。
3. 外链数据分析以及用户浏览深度分析我们可以在pageview事件中添加访问页面的当前url和前一个页面的url来进行处理分析，也就是说pageview事件也可以满足这两个模块的分析。
4. 订单信息分析要求pc端发送一个订单产生的事件，那么对应这个模块的分析，我们需要一个新的事件chargeRequest。对于事件分析我们也需要一个pc端发送一个新的事件数据，我们可以定义为event。
我们要分析的模块包括：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;用户基本信息分析
浏览器信息分析
地域信息分析
外链数据分析
用户浏览深度分析
订单信息分析
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们处理的事件包括：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pageview事件
chargeRequest事件
launch事件
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;第一，Launch事件。当用户第一次访问网站的时候触发该事件，不提供对外调用的接口，只实现该事件的数据收集。
第二，Pageview事件，当用户访问页面/刷新页面的时候触发该事件。该事件会自动调用，也可以让程序员手动调用。
第三，chargeRequest事件。当用户下订单的时候触发该事件，该事件需要程序主动调用。
每次都会发送对应的数据，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;u_sd=8E9559B3-DA35-44E1-AC98-85EB37D1F263&amp;amp;c_time=1449139048231&amp;amp;oid=orderid123&amp;amp;on=%E4%BA%A7%E5%93%81%E5%90%8D%E7%A7%B0&amp;amp;cua=1000&amp;amp;cut=%E4%BA%BA%E6%B0%91%E5%B8%81&amp;amp;pt=%E6%B7%98%E5%AE%9D&amp;amp;ver=1&amp;amp;en=e_cr&amp;amp;pl=website&amp;amp;sdk=js&amp;amp;b_rst=1920*1080&amp;amp;u_ud=12BF4079-223E-4A57-AC60-C1A04D8F7A2F&amp;amp;b_iev=Mozilla%2F5.0%20(Windows%20NT%206.1%3B%20WOW64)%20AppleWebKit%2F537.1%20(KHTML%2C%20like%20Gecko)%20Chrome%2F21.0.1180.77%20Safari%2F537.1&amp;amp;l=zh-CN
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个url的字段比较多，字段词典如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;参数名称	类型	描述
en	string	事件名称, eg: e_pv
ver	string	版本号, eg: 0.0.1
pl	string	平台, eg: website
sdk	string	Sdk类型, eg: js
b_rst	string	浏览器分辨率，eg: 1800*678
b_iev	string	浏览器信息useragent
u_ud	string	用户/访客唯一标识符
l	string	客户端语言
u_mid	string	会员id，和业务系统一致
u_sd	string	会话id
c_time	string	客户端时间
p_url	string	当前页面的url
p_ref	string	上一个页面的url
tt	string	当前页面的标题
ca	string	Event事件的Category名称
ac	string	Event事件的action名称
kv_*	string	Event事件的自定义属性
du	string	Event事件的持续时间
oid	string	订单id
on	string	订单名称
cua	string	支付金额
cut	string	支付货币类型
pt	string	支付方式
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;2-前端程序开发&#34;&gt;(2).前端程序开发&lt;/h4&gt;

&lt;p&gt;前面我们简单实现了后端开发，现在前端的JavaScript代码实现可能就更复杂了，对大家来说难度略大，但是还好这不是我们的重点，我大概展示几个函数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;onPageView: function() {
				// 触发page view事件
				if (this.preCallApi()) {
					var time = new Date().getTime();
					var pageviewEvent = {};
					pageviewEvent[this.columns.eventName] = this.keys.pageView;
					pageviewEvent[this.columns.currentUrl] = window.location.href; // 设置当前url
					pageviewEvent[this.columns.referrerUrl] = document.referrer; // 设置前一个页面的url
					pageviewEvent[this.columns.title] = document.title; // 设置title
					this.setCommonColumns(pageviewEvent); // 设置公用columns
					this.sendDataToServer(this.parseParam(pageviewEvent)); // 最终发送编码后的数据ss
					this.updatePreVisitTime(time);
				}
			},

			onChargeRequest: function(orderId, name, currencyAmount, currencyType, paymentType) {
				// 触发订单产生事件
				if (this.preCallApi()) {
					if (!orderId || !currencyType || !paymentType) {
						this.log(&amp;quot;订单id、货币类型以及支付方式不能为空&amp;quot;);
						return ;
					}

					if (typeof(currencyAmount) == &amp;quot;number&amp;quot;) {
						// 金额必须是数字
						var time = new Date().getTime();
						var chargeRequestEvent = {};
						chargeRequestEvent[this.columns.eventName] = this.keys.chargeRequestEvent;
						chargeRequestEvent[this.columns.orderId] = orderId;
						chargeRequestEvent[this.columns.orderName] = name;
						chargeRequestEvent[this.columns.currencyAmount] = currencyAmount;
						chargeRequestEvent[this.columns.currencyType] = currencyType;
						chargeRequestEvent[this.columns.paymentType] = paymentType;
						this.setCommonColumns(chargeRequestEvent); // 设置公用columns
						this.sendDataToServer(this.parseParam(chargeRequestEvent)); // 最终发送编码后的数据ss
						this.updatePreVisitTime(time);
					} else {
						this.log(&amp;quot;订单金额必须是数字&amp;quot;);
						return ;
					}	
				}
			},
			
			onEventDuration: function(category, action, map, duration) {
				// 触发event事件
				if (this.preCallApi()) {
					if (category &amp;amp;&amp;amp; action) {
						var time = new Date().getTime();
						var event = {};
						event[this.columns.eventName] = this.keys.eventDurationEvent;
						event[this.columns.category] = category;
						event[this.columns.action] = action;
						if (map) {
							for (var k in map){
								if (k &amp;amp;&amp;amp; map[k]) {
									event[this.columns.kv + k] = map[k];
								}
							}
						}
						if (duration) {
							event[this.columns.duration] = duration;
						}
						this.setCommonColumns(event); // 设置公用columns
						this.sendDataToServer(this.parseParam(event)); // 最终发送编码后的数据ss
						this.updatePreVisitTime(time);
					} else {
						this.log(&amp;quot;category和action不能为空&amp;quot;);
					}
				}
			}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;完整的代码，如果有兴趣自己可以详细研究，
*** 注意事项 ***，发布的时候一定要将代码中的url改为你的服务器的url：
&lt;img src=&#34;../images/2017-06-20-23-05-24.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;3-项目发布&#34;&gt;3.项目发布&lt;/h3&gt;

&lt;h4 id=&#34;1-本地项目发布&#34;&gt;(1).本地项目发布&lt;/h4&gt;

&lt;p&gt;前面我们已经简单的实现了后台和前端的代码，首先我们在本地启动服务，方法和前面一样，只是我们将自己的代码添加进去了，点击启动按钮。
&lt;img src=&#34;../images/2017-06-20-22-14-58.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;然后去浏览器访问 &lt;code&gt;http://localhost:8080/taobaopayment/demo4.jsp&lt;/code&gt;，然后点击跳转按钮测试
&lt;img src=&#34;../images/2017-06-20-22-16-22.jpg&#34; alt=&#34;&#34; /&gt;
截止现在我们的后台项目基本完成，这里面的代码比较难，大家可以根据情况查看，不用花太多的精力。&lt;/p&gt;

&lt;h4 id=&#34;2-发布到linux的tomcat上&#34;&gt;(2).发布到linux的tomcat上&lt;/h4&gt;

&lt;p&gt;我们的项目不可能放在本地运行，需要放到Linux的集群环境才能正常运行。首先打开Xshell，连上linux节点。
第一步，在Linux上安装tomcat。
安装的过程很简单，首先要安装java配置JAVA相关环境变量，然后在tomcat的官网下载tomcat的tar.gz包，解压，然后配置tomcat相关环境变量。启动tomcat的命令是tomcat安装目录下面的bin下面的&lt;code&gt;startup.sh&lt;/code&gt;，执行就能启动tomcat，然后访问节点的8080端口，如图：
&lt;img src=&#34;../images/2017-06-20-22-27-11.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;第二步，将项目打成war包。
类似以前打jar包，点开project structure -&amp;gt; artificts，添加一个artifict，名字为taobaopayment，type选择 Web Application:Archive，设置好对应的输出目录output idrectory，一般默认即可，然后点击确定&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/2017-06-20-22-28-57.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;设置好后，点击build -&amp;gt; build artifacts，构建war包
&lt;img src=&#34;../images/2017-06-20-22-32-33.jpg&#34; alt=&#34;&#34; /&gt;
构建结束后，进入刚刚设置的输出目录，你将会在刚刚设置的目录下看到一个war包。&lt;/p&gt;

&lt;p&gt;通过xshell的xftp工具，将打出来的war包拷贝放在linux的tomcat安装目录的webapps目录下：
&lt;img src=&#34;../images/2017-06-20-22-35-41.jpg&#34; alt=&#34;&#34; /&gt;
然后tomcat会自动解压war包，我们就可以在浏览器访问刚刚发布的项目了：
&lt;img src=&#34;../images/2017-06-20-22-39-45.jpg&#34; alt=&#34;&#34; /&gt;
到这里我们的整个后端项目就发布成功了。&lt;/p&gt;

&lt;h2 id=&#34;四-数据分析系统开发&#34;&gt;四、数据分析系统开发&lt;/h2&gt;

&lt;p&gt;本次我们的重心是整个系统的搭建，这部分的开发过程比较复杂，大家酌情进行学习，代码我们已经写好，大家只要稍作理解。细节和逻辑我们后续的实验还会讲解。&lt;/p&gt;

&lt;h3 id=&#34;1-需求回顾&#34;&gt;1.需求回顾&lt;/h3&gt;

&lt;p&gt;之前我们已经做过需求分析，这时候大家再回头看看我们一开始的需求设计，我们是要完成几个关键指标的设计分析。假设我们已经配置好了tomcat和nginx，那么我们知道每次用有浏览等行为时，我们的服务器就会给我们的设置的url发送数据，然后nginx就会收到我们发送的数据。接下来就是分析nginx收集到的数据。&lt;/p&gt;

&lt;p&gt;随便选取一条数据查看：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;192.168.126.1^A1458731952.690^A192.168.126.11^A/log.gif?en=e_pv&amp;amp;p_url=http%3A%2F%2Flocalhost%3A8080%2FBIG_DATA_LOG2%2Fdemo.jsp&amp;amp;p_ref=http%3A%2F%2Flocalhost%3A8080%2FBIG_DATA_LOG2%2Fdemo.jsp&amp;amp;tt=%E6%B5%8B%E8%AF%95%E9%A1%B5%E9%9D%A21&amp;amp;ver=1&amp;amp;pl=website&amp;amp;sdk=js&amp;amp;u_ud=EAB36BC9-0347-4D33-8579-AA8C331D001A&amp;amp;u_mid=laoxiao&amp;amp;u_sd=2D24B8A2-B2EF-450C-8C86-4F8B0F3E2785&amp;amp;c_time=1458731943823&amp;amp;l=zh-CN&amp;amp;b_iev=Mozilla%2F5.0%20(Windows%20NT%2010.0%3B%20WOW64%3B%20rv%3A45.0)%20Gecko%2F20100101%20Firefox%2F45.0&amp;amp;b_rst=1366*768
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这条数据提取了很多来自url的query信息，通过^A隔开，我们需要编写代码分析这种数据。&lt;/p&gt;

&lt;h3 id=&#34;2-etl处理&#34;&gt;2.ETL处理&lt;/h3&gt;

&lt;h4 id=&#34;1-定义工具类&#34;&gt;(1).定义工具类&lt;/h4&gt;

&lt;p&gt;本次试验的工具类主要是从一个url中抽取KPI信息，我们前面的业务需要的信息包括了地址、浏览器、操作系统等，根据我们的分析，所以我们需要使用IP地址解析等工具。解析url的工具类我们放在&lt;code&gt;com.hongya.etl.util&lt;/code&gt;下面，大家自己认真分析。&lt;/p&gt;

&lt;h4 id=&#34;2-定义相关常量类&#34;&gt;(2).定义相关常量类&lt;/h4&gt;

&lt;p&gt;我们的数据分析是有时间段的，我们分析的结果放进hbase中的表中，表名、字段名、时间范围等都是需要用到的常量，我们放在&lt;code&gt;com.hongya.common&lt;/code&gt;下面，大家可以查看。&lt;/p&gt;

&lt;h4 id=&#34;3-业务代码&#34;&gt;(3).业务代码&lt;/h4&gt;

&lt;p&gt;我们的数据字段含义在前面已经讲过了，现在我们需要将数据解析后放进hbase中。ETL过程就是简单的字符串处理，只需要一个Mapper程序即可完成。相应的代码在&lt;code&gt;com.hongya.etl.mr.ald&lt;/code&gt;中，大家可以查看。&lt;/p&gt;

&lt;h4 id=&#34;4-本地测试运行&#34;&gt;(4).本地测试运行&lt;/h4&gt;

&lt;p&gt;然后我们在本地新建一个文件，将上面哪一行测试数据放进去：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;192.168.126.1^A1458731952.690^A192.168.126.11^A/log.gif?en=e_pv&amp;amp;p_url=http%3A%2F%2Flocalhost%3A8080%2FBIG_DATA_LOG2%2Fdemo.jsp&amp;amp;p_ref=http%3A%2F%2Flocalhost%3A8080%2FBIG_DATA_LOG2%2Fdemo.jsp&amp;amp;tt=%E6%B5%8B%E8%AF%95%E9%A1%B5%E9%9D%A21&amp;amp;ver=1&amp;amp;pl=website&amp;amp;sdk=js&amp;amp;u_ud=EAB36BC9-0347-4D33-8579-AA8C331D001A&amp;amp;u_mid=laoxiao&amp;amp;u_sd=2D24B8A2-B2EF-450C-8C86-4F8B0F3E2785&amp;amp;c_time=1458731943823&amp;amp;l=zh-CN&amp;amp;b_iev=Mozilla%2F5.0%20(Windows%20NT%2010.0%3B%20WOW64%3B%20rv%3A45.0)%20Gecko%2F20100101%20Firefox%2F45.0&amp;amp;b_rst=1366*768
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后我们将&lt;code&gt;com.hongya.etl.mr.ald.AnalyserLogDataRunner&lt;/code&gt;中的&lt;code&gt;setJobInputPaths&lt;/code&gt;方法的路径改为刚刚添加的文件的路径，并且在&lt;code&gt;setConf&lt;/code&gt;方法设置一下zookeeper地址，并且启动zookeeper，就可以点击运行，在本地观察结果。
&lt;img src=&#34;../images/2017-06-21-01-16-19.jpg&#34; alt=&#34;&#34; /&gt;
如图我们可以看出ETL后将这一行数据转化为了Hbase的一条Put，这里一直运行不结束是因为我没有启动Hbase，所以一直没法写进去，发布项目的时候是需要启动的。
这个Mapper读取数据格式上面有，而写出的数据格式是Hbase的Put。不知道大家是否记得Hbase的javaAPI，我们介绍过Put的使用。最后我们每一条记录将会放进Hbase的表格中，例如上面的示例数据最后会解析为一条Put数据，我们可以看出它的rowKey是带着日期的：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rowKey 1458731952690_84973288
cf: info ,key:tt value:测试页面1
cf: info ,key:country value:unknown
cf: info ,key:ver value:1
cf: info ,key:u_mid value:laoxiao
cf: info ,key:os value:Windows
cf: info ,key:city value:unknown
cf: info ,key:ip value:192.168.126.1
cf: info ,key:b_rst value:1366*768
cf: info ,key:en value:e_pv
cf: info ,key:c_time value:1458731943823
cf: info ,key:l value:zh-CN
cf: info ,key:u_sd value:2D24B8A2-B2EF-450C-8C86-4F8B0F3E2785
cf: info ,key:u_ud value:EAB36BC9-0347-4D33-8579-AA8C331D001A
cf: info ,key:os_v value:Windows
cf: info ,key:p_ref value:http://localhost:8080/BIG_DATA_LOG2/demo.jsp
cf: info ,key:province value:unknown
cf: info ,key:s_time value:1458731952690
cf: info ,key:p_url value:http://localhost:8080/BIG_DATA_LOG2/demo.jsp
cf: info ,key:browser value:Firefox
cf: info ,key:sdk value:js
cf: info ,key:pl value:website
cf: info ,key:browser_v value:45.0
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;3-关键指标分析&#34;&gt;3.关键指标分析&lt;/h3&gt;

&lt;p&gt;上面的ETL完成后我们的结果数据都放在hbase的event_logs表格的info列族中，现在我们需要运行代码分析这些数据，对我们的数据进行我们前面的设计文档中的关键指标分析。&lt;/p&gt;

&lt;h4 id=&#34;1-代码结构规范&#34;&gt;(1).代码结构规范&lt;/h4&gt;

&lt;p&gt;我们的程序需要定期分析hbase的数据，我们分析的指标有很多，我们需要从hbase中提取的数据也有很多。最后分析完每个指标后我们放进mysql中，供echart做展示。
1. 首先我们分析的指标需要即KPI需要专门的类定义好，放在&lt;code&gt;com.hongya.common.KpiType&lt;/code&gt;下面。
2. 我们需要自定义Key和Value的类型，这些类型包含了需要统计的关键维度信息，作为mapreduce任务的输入输出key，我们定义好了放在&lt;code&gt;com.hongya.transformer.model.dim&lt;/code&gt;下面。
3. 我们的hadoop任务写mysql需要有专门的OutputFormat，我们放在&lt;code&gt;com.hongya.transformer.service&lt;/code&gt;下面，由于每个唯独统计任务写mysql都不一样，所以我们通过配置文件的方式传入，在&lt;code&gt;output-collector.xml&lt;/code&gt;中有相关配置。
4. 无论是读写mysql还是hbase都有配置，我们通过配置文件的方式传入，配置文件有&lt;code&gt;query-mapping.xml&lt;/code&gt;，&lt;code&gt;transfomer-env.xml&lt;/code&gt;。&lt;/p&gt;

&lt;h4 id=&#34;2-业务代码实现&#34;&gt;(2).业务代码实现&lt;/h4&gt;

&lt;p&gt;上面分析了业务，我们开始写mapreduce程序统计分析指标。
我们知道我们现在的mapreduce读取hbase的数据，然后写进mysql中，相关代码我们放在了&lt;code&gt;com.hongya.transformer.mr&lt;/code&gt;包下面。
由于时间关系，我们的业务只实现了new user指标的统计，大家可以查看&lt;code&gt;com.hongya.transformer.mr.nu&lt;/code&gt;包下的内容。
当我们的hbase中有数据时，运行&lt;code&gt;com.hongya.transformer.mr.nu.NewInstallUserRunner&lt;/code&gt;，就能看到下面的结果。
&lt;img src=&#34;../images/2017-06-21-13-10-29.jpg&#34; alt=&#34;&#34; /&gt;
如果你map完成后reduce就失败 ，没关系，是因为你的mysql还没有配置好，我们在后面会介绍的。&lt;/p&gt;

&lt;h2 id=&#34;四-数据分析系统架构&#34;&gt;四、数据分析系统架构&lt;/h2&gt;

&lt;p&gt;我们有了服务器后，接下来的任务就是对服务器的数据进行收集处理，处理后的数据进行展示。我们需要搭建一个完整的服务器，这时候首先需要一个集群，我们在云端直接使用一个节点的centos作为服务器。首先打开Xshell，连上centos节点，我们这里节点名为&lt;code&gt;node1&lt;/code&gt;。&lt;/p&gt;

&lt;h3 id=&#34;1-部署hadoop和hbase&#34;&gt;1.部署hadoop和Hbase&lt;/h3&gt;

&lt;h4 id=&#34;1-部署hadoop&#34;&gt;(1).部署hadoop&lt;/h4&gt;

&lt;p&gt;hadoop单节点安装很简单，以前讲过。直接解压后，配置&lt;code&gt;hadoop-env.sh、core-site.xml、hdfs-site.xml&lt;/code&gt;
1. hadoop-env.sh配置 JAVA_HOME
2. core-site.xml配置&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt; &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;fs.defaultFS&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;hdfs://localhost:8020&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hadoop.tmp.dir&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;/opt/data/hadoop&amp;lt;/value&amp;gt;
      &amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后就能够格式化，启动：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hdfs namenode -format
start-dfs.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;2-配置hbase&#34;&gt;(2).配置Hbase&lt;/h4&gt;

&lt;p&gt;首先安装单节点的zookeeper，安装好后启动，这个不细说：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;zkServer.sh start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后解压Hbase安装包.
1. 配置hbase-env.sh，配置JAVA_HOME，然后将 HBASE_MANAGES_ZK改为false
2. 配置hbase-site.xm&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hbase.rootdir&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;hdfs://localhost:8020/hbase&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hbase.cluster.distributed&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hbase.zookeeper.quorum&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;localhost&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;复制hadoop配置
复制hadoop的 core-site.xml和hdf-site.xml到hbase的conf目录下
然后就能启动Hbase了。
启动hbase后使用hbase shell进入交互窗口，执行建表语句：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;create &#39;event_logs&#39; ,&#39;info&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果执行成功了就可以开始下一步了。&lt;/p&gt;

&lt;h3 id=&#34;2-部署nginx服务&#34;&gt;2.部署nginx服务&lt;/h3&gt;

&lt;p&gt;前面我们将项目发布到tomcat上面了。正常情况下我们需要通过nginx进行负载均衡，同时收集url的请求日志。
我们可以安装nginx，也可以安装淘宝开源的tengine，比一般nginx多一些功能，而且淘宝上有中文文档。&lt;/p&gt;

&lt;h4 id=&#34;1-nginx安装&#34;&gt;(1).nginx安装&lt;/h4&gt;

&lt;p&gt;步骤如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;1.安装GCC编译器等工具：
yum install -y gcc gcc-c++ autoconf automake libtool make openssl openssl-devel pcre pcre-devel
2.下载安装Nginx:
wget http://nginx.org/download/nginx-1.6.3.tar.gz
注：这里也可以下载tengine压缩包，比一般nginx多一些功能
tar -zxvf nginx-1.6.3.tar.gz 
cd nginx-1.6.3/  
./configure --prefix=/usr/local/nginx
--sbin-path=/usr/local/nginx/sbin/nginx
--conf-path=/usr/local/nginx/conf/nginx.conf
--pid-path=/usr/local/nginx/logs/nginx.pid \
--with-http_ssl_module \
--with-http_stub_status_module \
--with-http_gzip_static_module \ 
make &amp;amp;&amp;amp; make install 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果正常的话，就安装好了，然后启动nginx服务即可。记住启动之前需要先关闭之前的tomcat，因为他们两个的端口冲突了。启动命令是就是nginx，启动以后，如果不修改配置，我们可以直接打开浏览器访问8080端口，出现这样就算是成功了。
&lt;img src=&#34;../images/2017-06-21-15-52-28.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&#34;2-nginx配置&#34;&gt;(2).nginx配置&lt;/h4&gt;

&lt;p&gt;现在我们已经安装了nginx，我们要配置nginx的反向代理，让他替我们监听我们的需要监听的端口。上面我们安装的时候已经配置了配置文件的路径：&lt;code&gt;/usr/local/nginx/conf/nginx.conf&lt;/code&gt;。现在我们修改他的内容。
我们让它监听80端口，这样我们给80端口发送的数据就可以被nginx收集然后我们后期处理：
我们只需要添加下面的内容：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;log_format my_format &#39;$remote_addr^A$msec^A$http_host^A$request_uri&#39;;

location = /log.gif {
   root html;
   ## 配置日志文件保存位置
   access_log /opt/data/access.log my_format;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改后的nginx.conf内容大概是这样的，其实就是添加了一行log_format，然后修改了端口信息：
&lt;img src=&#34;../images/2017-06-21-16-06-08.jpg&#34; alt=&#34;&#34; /&gt;
然后我们通过命令让配置文件生效： sudo nginx -s reload&lt;/p&gt;

&lt;h4 id=&#34;3-测试nginx收集日志&#34;&gt;(3).测试nginx收集日志&lt;/h4&gt;

&lt;p&gt;这时候我们可以启动我们之前的web项目，启动之前记得修改代码的url为刚刚配置的地址格式：
&lt;img src=&#34;../images/2017-06-21-16-11-05.jpg&#34; alt=&#34;&#34; /&gt;
然后我们发布运行项目，或者打war包放在tomcat里面，然后启动tomcat。在浏览器输入：&lt;code&gt;http://localhost:8080/taobaopayment/demo4.jsp&lt;/code&gt; 模拟用户点击。如果你发现浏览器一直处于刷新状态，可能你需要换一个浏览器：
&lt;img src=&#34;../images/2017-06-21-16-14-49.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;你还可以运行我们的Test类，来模拟后台的数据，报错没关系，只需要手动点击停止程序：
&lt;img src=&#34;../images/2017-06-21-16-16-11.jpg&#34; alt=&#34;&#34; /&gt;
然后我们查看刚刚配置的文件，已经有了几条记录，是刚刚我们发送的，而且都是我们配置的格式：
&lt;img src=&#34;../images/2017-06-21-16-25-15.jpg&#34; alt=&#34;&#34; /&gt;
到这里就恭喜，我们的gninx基本完成。&lt;/p&gt;

&lt;h3 id=&#34;3-日志收集系统&#34;&gt;3.日志收集系统&lt;/h3&gt;

&lt;p&gt;日志手机一般有两种方式，shell实现和flume实现。
shell命令前面大家都熟悉过，flume使用在前面的SparkStreaming实验也使用过，我们不介绍过多，简单回顾一下即可。
首先安装好flume，配置JAVA_HOME和HADOOP_HOME，然后新建或者复制一个配置文件，log.cfg ，添加下面的内容：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# 配置三个组件的名字
agent.sources = r1
agent.channels = c1
agent.sinks = k1

# For each one of the sources, the type is defined
agent.sources.r1.type = exec
## 这里配置你刚刚手机日志的文件
agent.sources.r1.command = tail -F /Users/dengziming/opt/data/hongya/taobaopayment/access.log
agent.sources.r1.port = 44444

# The channel can be defined as follows.
agent.channels.c1.type = memory
agent.channels.c1.capacity = 1000
agent.channels.c1.transactionCapacity = 1000

# Each sink&#39;s type must be defined
agent.sinks.k1.type = hdfs
agent.sinks.k1.hdfs.path = hdfs://localhost:8020/flume/events/%Y-%m-%d/%H%M/
agent.sinks.k1.hdfs.filePrefix = events-
agent.sinks.k1.hdfs.round = true
agent.sinks.k1.hdfs.roundValue = 10
agent.sinks.k1.hdfs.roundUnit = minute
agent.sinks.k1.hdfs.useLocalTimeStamp = true

#Specify the channel the sink should use
agent.sources.r1.channels = c1
agent.sinks.k1.channel = c1
agent.channels.memoryChannel.capacity = 100
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置好后适用命令启动：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/flume-ng agent --conf conf --conf-file conf/log.cfg --name agent -D flume.root.logger=INFO,console
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样就会收集我们刚刚nginx的日志到hadoop的 &lt;code&gt;/flume/events/%Y-%m-%d/%H%M/&lt;/code&gt; 路径下
恭喜你，马上就要进入数据分析部分&lt;/p&gt;

&lt;h3 id=&#34;4-提交数据分析任务&#34;&gt;4.提交数据分析任务&lt;/h3&gt;

&lt;p&gt;现在我们要开始运行程序，分析数据了。&lt;/p&gt;

&lt;h4 id=&#34;1-启动zookeeper-hadoop-hbase&#34;&gt;(1).启动zookeeper、hadoop、hbase&lt;/h4&gt;

&lt;p&gt;启动命令就不说了，然后记得之前我们已经在hbase中创建了表格：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;create &#39;event_logs&#39; ,&#39;info&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;../images/2017-06-21-16-45-52.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;2-运行etl任务&#34;&gt;(2).运行ETL任务&lt;/h4&gt;

&lt;p&gt;我们的etl的任务是 &lt;code&gt;com.hongya.etl.mr.ald.AnalyserLogDataRunner&lt;/code&gt;，打开这段代码，我们修改几个路径和配置，因为是测试，我们把数据放在本地运行。修改的主要是zookeeper地址和我们刚刚的日志路径：
&lt;img src=&#34;../images/2017-06-21-16-41-30.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;然后运行程序：
&lt;img src=&#34;../images/2017-06-21-16-46-55.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;然后我们可以根据日志看到打印的rowKey，我们可以在hbase中查看这些rowKey
&lt;img src=&#34;../images/2017-06-21-16-48-51.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;3-创建mysql表格&#34;&gt;(3).创建mysql表格&lt;/h4&gt;

&lt;p&gt;我们的etl完成后数据放在hbase中，然后我们需要进行统计分析，结果放在mysql，首先是建立mysql表格，我们的表格统一放在数据库report下面，首先建库，然后按照下面的语句依次建表：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;DROP TABLE IF EXISTS `stats_user`;
CREATE TABLE `stats_user` (
  `date_dimension_id` int(11) NOT NULL,
  `platform_dimension_id` int(11) NOT NULL,
  `active_users` int(11) DEFAULT &#39;0&#39; COMMENT &#39;活跃用户数&#39;,
  `new_install_users` int(11) DEFAULT &#39;0&#39; COMMENT &#39;新增用户数&#39;,
  `total_install_users` int(11) DEFAULT &#39;0&#39; COMMENT &#39;总用户数&#39;,
  `sessions` int(11) DEFAULT &#39;0&#39; COMMENT &#39;会话个数&#39;,
  `sessions_length` int(11) DEFAULT &#39;0&#39; COMMENT &#39;会话长度&#39;,
  `total_members` int(11) unsigned DEFAULT &#39;0&#39; COMMENT &#39;总会员数&#39;,
  `active_members` int(11) unsigned DEFAULT &#39;0&#39; COMMENT &#39;活跃会员数&#39;,
  `new_members` int(11) unsigned DEFAULT &#39;0&#39; COMMENT &#39;新增会员数&#39;,
  `created` date DEFAULT NULL,
  PRIMARY KEY (`platform_dimension_id`,`date_dimension_id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8 ROW_FORMAT=COMPACT COMMENT=&#39;统计用户基本信息的统计表&#39;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这只是一个表格，由于我们的表格太多，这里不展示，我们会将所有数据库的建表语句放在文件中，大家可以参考，最终如图：
&lt;img src=&#34;../images/2017-06-21-16-55-12.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;4-运行统计任务&#34;&gt;(4).运行统计任务&lt;/h4&gt;

&lt;p&gt;新用户点击分析任务放在&lt;code&gt;com.hongya.transformer.mr.nu.NewInstallUserRunner&lt;/code&gt;中，大家可以查看代码，然后我们修改一下配置，主要是mysql的用户名密码，在DimensionConverterImpl中，另外我们还有查看核对 src/transformer-env.xml 下的内容：
如图，修改用户名密码为你的mysql设置：
&lt;img src=&#34;../images/2017-06-21-16-58-32.jpg&#34; alt=&#34;&#34; /&gt;
修改zookeeper地址和运行的起始日期，你可以设置的小一点：
&lt;img src=&#34;../images/2017-06-21-16-57-33.jpg&#34; alt=&#34;&#34; /&gt;
然后我们点击运行，我们就可以根据日志看到map和reduce执行的过程:
&lt;img src=&#34;../images/2017-06-21-17-02-13.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;执行完成后，我们查看我们刚刚的mysql的report库的stats_device_browser和stats_user表格：
&lt;img src=&#34;../images/2017-06-21-17-03-45.jpg&#34; alt=&#34;&#34; /&gt;
当然我们还可以查看dimension_browser等其他表格。
程序执行成功。&lt;/p&gt;

&lt;h3 id=&#34;4-数据转移系统&#34;&gt;4.数据转移系统&lt;/h3&gt;

&lt;p&gt;数据转移系统我们使用sqoop，由于我们的部分mapreduce任务每次运行的结果都放在hadoop或者Hbase上面，我们可能需要手动将关键指标转移到关系型数据库，然后编写代码进行展示。但是在这里，我们都写进了mysql，就暂时不适用sqoop了，也是为了减轻大家的负担。&lt;/p&gt;

&lt;h3 id=&#34;5-数据展示&#34;&gt;5.数据展示&lt;/h3&gt;

&lt;p&gt;数据展示我们使用 jquery + Echart吧。真正项目的echart展示部分一般不需要我们管，会有专门的前端高手负责，所以我们就简单的做一下吧。以浏览器维度为例，我们直接写sql语句&lt;code&gt;select name,count(*) from dimension_browser group by browser_name&lt;/code&gt;，将结果文件写到echart的option属性中：
&lt;img src=&#34;../images/2017-06-21-20-20-50.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;然后重新再浏览器段查看吧：
&lt;img src=&#34;../images/2017-06-21-20-21-47.jpg&#34; alt=&#34;&#34; /&gt;
当然我这是一种不可取的做法，因为这种方式显然是不符合企业生产环境的。真正的生产环境肯定是通过后台和数据库交互，通过ajax将数据传给前端展示，我们做的很敷衍，是因为这不是我们的重点。&lt;/p&gt;

&lt;h3 id=&#34;6-项目发布&#34;&gt;6.项目发布&lt;/h3&gt;

&lt;p&gt;这上面的所有步骤都完成了，就可以发布项目了，我们需要一套任务调度系统。azkaban是目前来说用的比较多的任务调度系统，我们推荐大家课后了解一下azkaban的安装使用。这里我们没法演示了。&lt;/p&gt;

&lt;h2 id=&#34;项目上线&#34;&gt;项目上线&lt;/h2&gt;

&lt;h3 id=&#34;1-基本步骤&#34;&gt;1.基本步骤&lt;/h3&gt;

&lt;p&gt;将整个项目设计好以后，就可以上线了，这里我们总结一下真实项目上线的过程。
1. 软件的安装
本地安装开发环境需要的东西，以及相关的依赖。服务器需要安装tomcat、nginx、zookeeper、hadoop、Hbase
2. 项目开发
这里的项目开发有三部分，服务端程序，日志分析程序，前端展示程序，其中日志分析程序我们只完成了new user 开发，剩下的业务由大家自己开发。前端展示程序我们只是简单展示，没有开发，这不是重点。
3. 搭建nginx服务器，监听80端口
nginx的安装和部署需要注意很多，安装完成后修改配置文件。
4. 启动flume，收集来自nginx的数据
flume配置完成后会收集日志文件的日志，按照时间放到hadoop上面。
5. 发布web项目
将项目打成war包，放到tomcat上，然后浏览器就可以访问，有访问时会向80端口发送数据。
6. 建表
新建hbase的表格和mysql表格
7. 定时启动hadoop任务
我们通过以前说过的方法将程序打成jar包，上传到linux，在Linux上通过&lt;code&gt;hadoop jar&lt;/code&gt;命令提交。
8. 启动展示任务
这里我们简单处理一下忽略掉。
9. 将运行和展示任务添加到定时任务进行调度
这部分比较复杂，需要专门的时间学习。&lt;/p&gt;

&lt;h3 id=&#34;2-自己动手发布程序&#34;&gt;2.自己动手发布程序&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;安装相关软件，我这里已经安装完成，大家自己检查安装。&lt;/li&gt;

&lt;li&gt;&lt;p&gt;安装hadoop、zookeeper、hbase、flume并配置
这部分不详细讲解，配置方法上面都有，配置好以后启动相应集群。启动命令分别为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;start-dfs.sh
start-yarn.sh
start-hbase.sh
bin/flume-ng agent -c ./conf -f ./conf/log.cfg -n agent 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;至少有这些java进程：
&lt;img src=&#34;../images/2017-06-21-21-38-57.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;安装nginx，启动nginx服务，修改nginx的配置文件，监听80端口，并且收集格式为： /log.gif 的url。
可以在浏览器访问linux的80端口：
&lt;img src=&#34;../images/2017-06-21-21-40-36.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;项目打war包，放到tomcat中。
打包之前，修改代码的url为你的linux节点加上log.gif
&lt;img src=&#34;../images/2017-06-21-21-57-51.jpg&#34; alt=&#34;&#34; /&gt;
然后打成war包，名字为：&lt;code&gt;taobaopayment.war&lt;/code&gt;，放在tomcat的webapps目录下，使员工startup.sh 启动tomcat，访问8080端口，然后访&lt;code&gt;http://node1:8080/taobaopayment/demo4.jsp&lt;/code&gt;：
如果这里一直在刷新，那么需要换一个浏览器：
&lt;img src=&#34;../images/2017-06-21-22-06-20.jpg&#34; alt=&#34;&#34; /&gt;
比如我用Safari浏览器，不断点击，产生数据：
&lt;img src=&#34;../images/2017-06-21-22-07-45.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;创建hbase和mysql表格
hbase创建event_logs表格，info列族，
mysql建表语句文件里有。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;执行mapreduce的ETL任务
你可以选择打jar包，或者在本地执行，执行的主类是：&lt;code&gt;com.hongya.etl.mr.ald.AnalyserLogDataRunner&lt;/code&gt;
需要修改zookeeper配置和输入路径，如果在集群上运行，这个输入路径是前面配置的flume手机日志的路径。
&lt;img src=&#34;../images/2017-06-21-22-12-59.jpg&#34; alt=&#34;&#34; /&gt;
执行完成后，可以去hbase查看数据。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;运行分析程序
分析程序基于我们刚刚的结果，主类为：&lt;code&gt;com.hongya.transformer.mr.nu.NewInstallUserRunner&lt;/code&gt;，运行之前需要在&lt;code&gt;DimensionConverterImpl&lt;/code&gt;类中设置mysql的连接信息。
&lt;img src=&#34;../images/2017-06-21-22-16-00.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;查看mysql数据库的结果，并展示
查看mysql的结果：
&lt;img src=&#34;../images/2017-06-21-22-16-54.jpg&#34; alt=&#34;&#34; /&gt;
数据展示模块，需要使用Echart，脱离了我们的实验主题，我们简单模拟，访问浏览器的：&lt;code&gt;http://node1:8080/taobaopayment/showUser.jsp&lt;/code&gt;和&lt;code&gt;http://node1:8080/taobaopayment/showBrowser.jsp&lt;/code&gt;
&lt;img src=&#34;../images/2017-06-21-22-18-47.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
      
    </item>
    
    <item>
      <title>On Disk IO</title>
      <link>https://dengziming.github.io/post/%E6%95%B0%E6%8D%AE%E5%BA%93/%E7%A3%81%E7%9B%98io/</link>
      <pubDate>Thu, 22 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/%E6%95%B0%E6%8D%AE%E5%BA%93/%E7%A3%81%E7%9B%98io/</guid>
      
        <description>

&lt;p&gt;文章来源：&lt;a href=&#34;https://medium.com/databasss/on-disk-io-part-1-flavours-of-io-8e1ace1de017&#34;&gt;https://medium.com/databasss/on-disk-io-part-1-flavours-of-io-8e1ace1de017&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;了解 IO 原理有助于工作。Network IO 相关问题经常讨论，然而和磁盘相关的问题很少讨论。一方面实现上，网络 IO 更为复杂的巧妙，Filesystem 相关 tools 很少。
另一方面，大家都使用 databases 作为存储系统，和底层打交道的事情交给了数据库设计人员，但是了解相关资料 依然重要。&lt;/p&gt;

&lt;p&gt;目录：
- Syscalls: open, write, read, fsync, sync, close
- Standard IO: fopen, fwrite, fread, fflush, fclose
- Vectored IO: writev, readv
- Memory mapped IO: open, mmap, msync, munmap&lt;/p&gt;

&lt;h2 id=&#34;buffered-io&#34;&gt;Buffered IO&lt;/h2&gt;

&lt;p&gt;“buffering”  总是让人困惑，当使用 Standard IO，可以选择  full and line buffering 或者 out from any buffering whatsoever，
这些其实和我们要讨论的 Kernel buffering (Page Cache)  没什么关系。你可以吧上面的称之为 caching。&lt;/p&gt;

&lt;h2 id=&#34;sector-block-page&#34;&gt;Sector/Block/Page&lt;/h2&gt;

&lt;p&gt;块状设备（Block Device ）是为读取提供  HDDs or SSDs 等硬件设备提供 buffered access 的特殊文件类型。
Block Devices 工作于 Sectors（一组相邻的 bytes） 之上，Most disk devices have a sector size of 512 bytes。
Sector 是 block device 之间数据传输的最小单元，读取数据不可能比 Sector 更小，但是却可以同时读 multiple adjacent segments。
File System  最小读取单元就是 Block，Block就是一组相邻的sectors，block sizes 一般为 512, 1024, 2048 and 4096 bytes。
通常 IO操作会通过 Virtual Memory，将 filesystem blocks 缓存到 memory 中作为buffer提供中间操作，
Virtual Memory 又和 pages 一起工作，pages 会 map 到 blocks，Typical page size is 4096 bytes。&lt;/p&gt;

&lt;p&gt;总结来说，Virtual Memory 和 pages 一起工作，map 到 Filesystem blocks，blocks 又 map 到 Block Device sectors。&lt;/p&gt;

&lt;h2 id=&#34;standard-io&#34;&gt;Standard IO&lt;/h2&gt;

&lt;p&gt;通过 read() and write()  方法操作文件系统。&lt;/p&gt;

&lt;p&gt;reading: 首先访问 Page Cache，如果 Page Cache 中没有需要的数据，将会触发 Page Fault 然后将需要的数据 paged in。所以读取 unmapped 文件会慢一些，尽管对用户是透明的。&lt;/p&gt;

&lt;p&gt;writes: buffer contents 首先写到 Page Cache，数据并不会立马写进磁盘，真正的 hardware write 是当 Kernel 决定进行  writeback of the dirty page.&lt;/p&gt;

&lt;h2 id=&#34;page-cache&#34;&gt;Page Cache&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/torvalds/linux/blob/master/include/linux/buffer_head.h&#34;&gt;https://github.com/torvalds/linux/blob/master/include/linux/buffer_head.h&lt;/a&gt;
将最近访问过的 fragments 存储起来，read() and write() 操作并不会触发磁盘操作而是通过Page Cache 。&lt;/p&gt;

&lt;p&gt;读的时候，读的时候会查看 Page Cache，如果有的话直接给用户，没有的话先加载再给用户，如果满了，least recently used pages 会被刷到磁盘，并且 evicted from cache。&lt;/p&gt;

&lt;p&gt;写操作仅仅是将数据放到 Page Cache，marking the written page as dirty。后续会有 flush or writeback 的进程将数据写到磁盘。&lt;/p&gt;

&lt;p&gt;标记为dirty 的page会被flush到磁盘，这个过程被称作 writeback。writeback 有缺点，例如 queuing up IO requests。&lt;/p&gt;

&lt;p&gt;Page Cache 的理论是 时间本地性原则，也就是刚刚被访问的内容很有可能被再次访问。当然还有空间本地性原则，也就是被访问的数据附近的数据很有可能被访问，prefetch 操作就是机遇这个原则。&lt;/p&gt;

&lt;p&gt;Page Cache ，保存了最近访问的和将要访问的 file chunks，read-write-read 的操作可以直接通过缓存，加快了速度。&lt;/p&gt;

&lt;h2 id=&#34;delaying-errors&#34;&gt;Delaying Errors&lt;/h2&gt;

&lt;p&gt;既然不会立即写到磁盘，就会有 可能突然出错没有写到磁盘的问题，可以查看： &lt;a href=&#34;https://lwn.net/Articles/457667/&#34;&gt;https://lwn.net/Articles/457667/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;direct-io&#34;&gt;Direct IO&lt;/h2&gt;

&lt;p&gt;当我们不想用 Kernel Page Cache。打开文件的时候使用 O_DIRECT 标志，直接读取绕过 Page Cache，写也是直接写到磁盘。
大部分使用 Direct IO 的application 都会导致性能的 下降，但是高手可以通过 fine-grained 操作提高性能，因为会实现自己的缓存，例如 PostgreSQL and MySQL use Direct IO 。&lt;/p&gt;

&lt;p&gt;例如 write-ahead-log，一般不会立即查看。同时使用 O_DIRECT 和 PageCache 打开文件会带来很多问题，不鼓励。&lt;/p&gt;

&lt;h2 id=&#34;block-alignment&#34;&gt;Block Alignment&lt;/h2&gt;

&lt;p&gt;使用 Direct IO 的时候，最好让操作都对准 sector 的边界上。也就是说，每个操作的 starting offset 是 512 的倍数，缓存大小也是 512 的倍数，
Crossing segment boundary 会导致加载多个sectors。&lt;/p&gt;

&lt;p&gt;Page Cache 会在内存自动对齐，所以不需要。&lt;/p&gt;

&lt;h2 id=&#34;memory-mapping&#34;&gt;Memory Mapping&lt;/h2&gt;

&lt;p&gt;Memory Mapping (mmap) 让你可以像整个文件都放进内存了一样访问文件。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>数据库底层原理</title>
      <link>https://dengziming.github.io/post/%E6%95%B0%E6%8D%AE%E5%BA%93/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86/</link>
      <pubDate>Thu, 22 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/%E6%95%B0%E6%8D%AE%E5%BA%93/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86/</guid>
      
        <description>&lt;p&gt;我不太喜欢一直把技术当黑箱使用，需要花时间理解底层原理。参考资料：&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://coding-geek.com/how-databases-work/&#34;&gt;http://coding-geek.com/how-databases-work/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://blog.jobbole.com/100349/&#34;&gt;http://blog.jobbole.com/100349/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You can google by yourself “how does a relational database work” to see how few results there are. Moreover, those articles are short.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://coding-geek.com/how-does-a-hashmap-work-in-java/&#34;&gt;http://coding-geek.com/how-does-a-hashmap-work-in-java/&lt;/a&gt;&lt;/p&gt;
</description>
      
    </item>
    
  </channel>
</rss>