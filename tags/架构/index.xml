<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>架构 on 数据分析师之旅</title>
    <link>https://dengziming.github.io/tags/%E6%9E%B6%E6%9E%84/</link>
    <description>Recent content in 架构 on 数据分析师之旅</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Sat, 22 Dec 2018 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://dengziming.github.io/tags/%E6%9E%B6%E6%9E%84/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>design-data-intensive-architecture</title>
      <link>https://dengziming.github.io/post/data-intensive-architecture/design-data-intensive-architecture/</link>
      <pubDate>Sat, 22 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/data-intensive-architecture/design-data-intensive-architecture/</guid>
      
        <description>

&lt;h1 id=&#34;第一部分-前言&#34;&gt;第一部分、前言&lt;/h1&gt;

&lt;p&gt;ACID，bigdata，NoSql ，bigTable ，CAP，2PC，3PC，quorum，raft，paxos，cloud，real-time，高并发架构将会逐渐成为基础。&lt;/p&gt;

&lt;p&gt;主要内容：
（一）基础&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;“reliability, scalability, and maintainability”&lt;/li&gt;
&lt;li&gt;数据模型&lt;/li&gt;
&lt;li&gt;存储引擎&lt;/li&gt;
&lt;li&gt;数据结构&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;（二）分布式&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;replication&lt;/li&gt;
&lt;li&gt;partitioning/sharding&lt;/li&gt;
&lt;li&gt;transaction&lt;/li&gt;
&lt;li&gt;distributed system&lt;/li&gt;
&lt;li&gt;consistency and consensus&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;（三）交互&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;batch process&lt;/li&gt;
&lt;li&gt;realtime&lt;/li&gt;
&lt;li&gt;put everything together&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;（四）案例及资料&lt;/p&gt;

&lt;h1 id=&#34;第二部分-数据系统的基础&#34;&gt;第二部分、数据系统的基础&lt;/h1&gt;

&lt;h2 id=&#34;一-reliable-scalable-和-maintainable-的含义&#34;&gt;一、Reliable,Scalable 和 Maintainable 的含义&lt;/h2&gt;

&lt;p&gt;数据系统分为 data-intensive 和 compute-intensive，一般的大数据应用会采用的技术：
1. 存储，方便后续的查询 &amp;ndash; databases
2. 缓存，加速访问 &amp;ndash; caches
3. 允许用户通过关键字搜索、按照过滤  &amp;ndash; indexes
4. 给另一个进程发送消息，异步处理  &amp;ndash; streaming process
5. 周期性处理全量数据  &amp;ndash; batch process&lt;/p&gt;

&lt;p&gt;这一切都是需要数据系统，我们并不会从头开始搭建存储、计算系统，因为有很多现成的技术可以应用。数据库、消息队列功能都很相似，但是实现完全不同，导致不同的性能，所以我们用处不同，我们为什么要将他们结合到一起呢？&lt;/p&gt;

&lt;p&gt;其实许多工具功能比较重复，redis 其实也可以当消息队列，kafka 也可以做数据持久化。这些工具的界限越来越模糊。第二是我们得系统一般比较复杂，一个工具完成不了，我们必须组件一个数据系统。组件的过程
有很多需要考虑的事情，其中最重要的三点：
Reliable： 系统必须一致能够提供服务，即便是发生了一些不可预知的错误
Scalable： 系统升级、功能扩展、访问量增加，导致我们需要能够很方便扩展我们的系统
Maintainable： 随着时间推移，可能有很多其他人接受项目，必须方便别人进行维护&lt;/p&gt;

&lt;p&gt;有关这三点的介绍，我们逐渐展开：&lt;/p&gt;

&lt;h3 id=&#34;1-reliable&#34;&gt;1. Reliable&lt;/h3&gt;

&lt;h3 id=&#34;2-scalable&#34;&gt;2. Scalable&lt;/h3&gt;

&lt;h3 id=&#34;3-maintainable&#34;&gt;3. Maintainable&lt;/h3&gt;

&lt;h2 id=&#34;二-数据模型和查询语言&#34;&gt;二、数据模型和查询语言&lt;/h2&gt;

&lt;p&gt;数据模型是最重要的一部分，&lt;/p&gt;

&lt;h3 id=&#34;1-关系型与文档型&#34;&gt;1.关系型与文档型&lt;/h3&gt;

&lt;h3 id=&#34;2-nosql&#34;&gt;2.NoSQL&lt;/h3&gt;

&lt;h3 id=&#34;3-object-relational-mismatch&#34;&gt;3. Object-Relational Mismatch&lt;/h3&gt;

&lt;h3 id=&#34;4-many-to-one-and-many-to-many-relationships&#34;&gt;4. Many-to-One and Many-to-Many Relationships&lt;/h3&gt;

&lt;h3 id=&#34;5-query-languages-for-data&#34;&gt;5. Query Languages for Data&lt;/h3&gt;

&lt;h3 id=&#34;6-graph-like-data-models&#34;&gt;6.Graph-Like Data Models&lt;/h3&gt;

&lt;h2 id=&#34;三-storage-and-retrieval&#34;&gt;三、Storage and Retrieval&lt;/h2&gt;

&lt;h3 id=&#34;1-hash-indexes&#34;&gt;1. Hash Indexes&lt;/h3&gt;

&lt;h3 id=&#34;2-sstables-and-lsm-trees&#34;&gt;2. SSTables and LSM-Trees&lt;/h3&gt;

&lt;h3 id=&#34;3-b-trees&#34;&gt;3. B-Trees&lt;/h3&gt;

&lt;h3 id=&#34;4-multi-column-indexes&#34;&gt;4. Multi-column indexes&lt;/h3&gt;

&lt;h3 id=&#34;5-full-text-search-and-fuzzy-indexes&#34;&gt;5. Full-text search and fuzzy indexes&lt;/h3&gt;

&lt;h3 id=&#34;6-keeping-everything-in-memory&#34;&gt;6.Keeping everything in memory&lt;/h3&gt;

&lt;h3 id=&#34;7-olap-vs-oltp&#34;&gt;7. OLAP vs OLTP&lt;/h3&gt;

&lt;h3 id=&#34;8-数据仓库&#34;&gt;8. 数据仓库&lt;/h3&gt;

&lt;h3 id=&#34;9-stars-and-snowflakes-schemas-for-analytics&#34;&gt;9. Stars and Snowflakes: Schemas for Analytics&lt;/h3&gt;

&lt;h3 id=&#34;10-column-oriented-storage&#34;&gt;10. Column-Oriented Storage&lt;/h3&gt;

&lt;h3 id=&#34;11-column-compression&#34;&gt;11. Column Compression&lt;/h3&gt;

&lt;h3 id=&#34;12-sort-order-in-column-storage&#34;&gt;12. Sort Order in Column Storage&lt;/h3&gt;

&lt;h3 id=&#34;13-writing-to-column-oriented-storage&#34;&gt;13. Writing to Column-Oriented Storage&lt;/h3&gt;

&lt;p&gt;前面写的有关列存储的优化在数据仓库中很重要，排序、位图索引、压缩都能加速查询，但是会让写数据更加麻烦。&lt;/p&gt;

&lt;p&gt;以 BTree 为理论的写方法，在这里完全没有用，如果你要在数据中插入一条数据，你需要重写所有的数据。&lt;/p&gt;

&lt;p&gt;但是还好的是，如果是 LSM-tree 的方式，首先写进内存排序，然后写磁盘。查询的时候，也是两部分结果进行合并，Vertica 数据仓库就是这么做的。&lt;/p&gt;

&lt;h3 id=&#34;14-aggregation-data-cubes-and-materialized-views&#34;&gt;14. Aggregation: Data Cubes and Materialized Views&lt;/h3&gt;

&lt;p&gt;并不是所有的数据仓库都是使用列存储，很多其他存储方式也会用到，但是列存储确实能够很大程度加快访问，所以越来越流行。&lt;/p&gt;

&lt;p&gt;另一种值得一提的数据仓库技术就是 materialized aggregates。前面所说的，查询经常会遇到一些聚合， 例如 count，sum，如果一个查询被很多查询共用，可以将结果缓存。
其中一种方法就是 materialized view，类似查询视图，只不过这个视图的结果已经计算好保存起来了。如果数据改变了，你的 materialized view 也要改变。&lt;/p&gt;

&lt;p&gt;一种常见的案例就是 data cube 后者 OLAP cube，就是根据不同纬度就行聚合后的结果。&lt;/p&gt;

&lt;h1 id=&#34;第五部分-案例&#34;&gt;第五部分、案例&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://medium.freecodecamp.org/how-to-system-design-dda63ed27e26&#34;&gt;https://medium.freecodecamp.org/how-to-system-design-dda63ed27e26&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://blog.gainlo.co/index.php/2015/10/22/8-things-you-need-to-know-before-system-design-interviews/&#34;&gt;http://blog.gainlo.co/index.php/2015/10/22/8-things-you-need-to-know-before-system-design-interviews/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://engineering.videoblocks.com/web-architecture-101-a3224e126947&#34;&gt;https://engineering.videoblocks.com/web-architecture-101-a3224e126947&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;第六部分-资料&#34;&gt;第六部分、资料&lt;/h1&gt;

&lt;p&gt;es设计架构，良心参考资料：
&lt;a href=&#34;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-i-7ac9a13b05db&#34;&gt;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-i-7ac9a13b05db&lt;/a&gt;
&lt;a href=&#34;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-ii-6db4e821b571&#34;&gt;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-ii-6db4e821b571&lt;/a&gt;
&lt;a href=&#34;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-iii-8bb6ac84488d&#34;&gt;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-iii-8bb6ac84488d&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;lsm b+tree 资料
&lt;a href=&#34;https://medium.com/databasss/on-disk-io-part-3-lsm-trees-8b2da218496f&#34;&gt;https://medium.com/databasss/on-disk-io-part-3-lsm-trees-8b2da218496f&lt;/a&gt;&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>es架构-1</title>
      <link>https://dengziming.github.io/post/es/es%E6%9E%B6%E6%9E%84-1/</link>
      <pubDate>Sun, 22 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/es/es%E6%9E%B6%E6%9E%84-1/</guid>
      
        <description>

&lt;p&gt;es设计架构，良心参考资料：
&lt;a href=&#34;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-i-7ac9a13b05db&#34;&gt;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-i-7ac9a13b05db&lt;/a&gt;
&lt;a href=&#34;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-ii-6db4e821b571&#34;&gt;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-ii-6db4e821b571&lt;/a&gt;
&lt;a href=&#34;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-iii-8bb6ac84488d&#34;&gt;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-iii-8bb6ac84488d&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;一-anatomy-of-an-elasticsearch-cluster&#34;&gt;一、Anatomy of an Elasticsearch Cluster&lt;/h2&gt;

&lt;p&gt;很遗憾，Google的搜索技术不开源，es是搜索引擎的一个很好的替代品，本文主要覆盖了es的底层结构、数据原型、读写过程。es的功能主要有：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;全文搜索
例如：怎么找到Wikipedia上面和某个名字最相关的文章&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;聚合&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;例如 显示广告网络上面的词条出价直方图&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;地理空间API&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;例如：设计一个能找到和骑手最近司机的骑行分享平台&lt;/p&gt;

&lt;p&gt;接下来就是内容，主要有以下几个方面：
1. 是主从架构还是无主架构
2. 存储模型
3. 读写工作流程
4. 搜索结果怎么相关&lt;/p&gt;

&lt;h3 id=&#34;1-the-confusion-between-elasticsearch-index-and-lucene-index-other-common-terms&#34;&gt;1.The confusion between Elasticsearch Index and Lucene Index + other common terms…&lt;/h3&gt;

&lt;p&gt;es 的 index 是一个组织数据的逻辑空间，就类似一个数据库。es index有一到多个 shards，一个shard就是一个真正存数据的lucene index，内部就是一个搜索引擎。&lt;/p&gt;

&lt;p&gt;每个 shard 都有0到多个replica，es index 有 type 的概念，就好比数据库里面的表，一个type里的所以type有相同的properties，就像schema一样。&lt;/p&gt;

&lt;h3 id=&#34;2-types-of-nodes&#34;&gt;2. Types of nodes&lt;/h3&gt;

&lt;h4 id=&#34;1-master-node&#34;&gt;（1）Master Node&lt;/h4&gt;

&lt;p&gt;控制集群，负责集群的操作，例如创建删除index，和集群的nodes联系，给节点分配shards。主节点一次处理一个集群状态，并将状态广播到所有节点，收到广播的节点对主节点进行确认回复。
an be configured to be eligible to become a master node by setting the node.master property to be true (default) in elasticsearch.yml.
大集群最好有专门的master node，去空值集群，不用处理任何用户请求&lt;/p&gt;

&lt;h4 id=&#34;2-data-node&#34;&gt;（2） Data Node&lt;/h4&gt;

&lt;p&gt;保存数据和倒排索引，By default, every node is configured to be a data node and the property node.data is set to true in elasticsearch.yml.
If you would like to have a dedicated master node, then change the node.data property to false.&lt;/p&gt;

&lt;h4 id=&#34;3-client-node&#34;&gt;（3）Client Node:&lt;/h4&gt;

&lt;p&gt;If you set both node.master and node.data to false, then the node gets configured as a client node and acts as a load balancer routing incoming requests to different nodes in the cluster.&lt;/p&gt;

&lt;h4 id=&#34;4-coordinating-node&#34;&gt;（4）coordinating node&lt;/h4&gt;

&lt;p&gt;注意没有专门的coordinating node，通过client连上的的es节点称为 coordinating node，将client request 路由到合适的shard。对于读请求，每次选择不同的shard 从而 balance the load.&lt;/p&gt;

&lt;h3 id=&#34;2-storage-model&#34;&gt;2. Storage Model&lt;/h3&gt;

&lt;p&gt;Elasticsearch uses Apache Lucene, a full-text search library written in Java and developed by Doug Cutting (creator of Apache Hadoop)。
es内部通过倒排索引的数据结构，从而处理可能延迟的查询。es中 document 是数据的存储 unit，通过将document的词进行分词创建 inverted index ，倒排索引能够创建排序的term并将和这个term相关的document进行管理。
和每本书背后的index类似，包含了很多词和那一页可以找到这些词，例如下面的两个document。&lt;/p&gt;

&lt;p&gt;Doc 1: Insight Data Engineering Fellows Program
Doc 2: Insight Data Science Fellows Program&lt;/p&gt;

&lt;p&gt;If we want to find documents which contain the term “insight”, we can scan the inverted index (where words are sorted), find the word “insight” and return the document IDs which contain this word, which in this case would be Doc 1 and Doc 2.&lt;/p&gt;

&lt;p&gt;为了更好的搜索性，文档先被分析。一般就是分词+标准化。&lt;/p&gt;

&lt;p&gt;综上，我们知道每个document存储模型，存储了document，以及对他们分词后的倒排索引。&lt;/p&gt;

&lt;h3 id=&#34;3-anatomy-of-a-write&#34;&gt;3.Anatomy of a Write&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&amp;copy;reate&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;When you send a request to the coordinating node to index a new document, the following set of operations take place:&lt;/p&gt;

&lt;p&gt;es所有的节点都包含了集群的元数据信息，包括哪些节点或者，有哪些分片。The coordinating node 通过 documentId将document和他对应的shard route起来，
es再通过 murmur3 hash算法将documentId进行取值，得到shard。&lt;code&gt;shard = hash(document_id) % (num_of_primary_shards)&lt;/code&gt;。
当节点收到 coordinating node 的 request ，request 会被写入到 translog 中，document 会被放进 memory buffer（&lt;a href=&#34;http://www.linfo.org/buffer.html），&#34;&gt;http://www.linfo.org/buffer.html），&lt;/a&gt;
如果在primary shard上执行成功，reques也会被发送到 replica shard上，
当 translog fsynced （&lt;a href=&#34;https://linux.die.net/man/2/fsync）&#34;&gt;https://linux.die.net/man/2/fsync）&lt;/a&gt; on all primary and replica shards.client receives acknowledgement that the request was successful。&lt;/p&gt;

&lt;p&gt;memory buffer会周期性更新 (defaults to 1 sec)，contents 会被写到一个 a new segment in filesystem cache ，
This segment is not yet fsync’ed, however, the segment is open and the contents are available for search.&lt;/p&gt;

&lt;p&gt;The translog is emptied and filesystem cache is fsync’ed every 30 minutes or when the translog gets too big. 这个过程称为flush
the in-memory buffer is cleared and the contents are written to a new segment.
A new commit point is created with the segments fsync’ed and flushed to disk. The old translog is deleted and a fresh one begins.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;(U)pdate and (D)elete&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;es的记录是无法更改的，删除和update实际上是新建，更改版本号。每个segment 都有一个 .del  file。
When a delete request is sent, the document is not really deleted, but marked as deleted in the .del file.
This document may still match a search query but is filtered out of the results.
When segments are merged, the documents marked as deleted in the .del file are not included in the new merged segment.&lt;/p&gt;

&lt;p&gt;update则是新建+删除，es给每个document一个version，每次改变，version都+增加，旧版本会被.del 文件标记为删除，
和删除一样，This older document may still match a search query but is filtered out of the results.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Anatomy of a &amp;reg;ead
Read operations consist of two parts:&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Query Phase
Fetch Phase&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Query Phase
coordinating node route the search request to all the shards (primary or replica) in the index.
每个shard单独search，然后将结果放进一个优先队列，根据 relevance score (we’ll cover relevance score later in the post).
所有 shards将结果汇总，创建一个新的优先队列，取出相关度最高的一部分。这个过程类似spark的topn&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Fetch Phase
coordinating node 排好序之后，
it then requests the original documents from all the shards. All the shards enrich the documents and return them to the coordinating node.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;相关度：tf/idf （term frequency/inverse document frequency）算法。
tf 词频，在某文档出现的频率
idf 出现过得所有文档数&lt;/p&gt;

&lt;h2 id=&#34;what-next&#34;&gt;What next?&lt;/h2&gt;

&lt;p&gt;lit brain problem in Elasticsearch and how to avoid it
Transaction log
Lucene segments
Why deep pagination during search is dangerous?
Difficulties and trade-offs in calculating search relevance
Concurrency control
Why is Elasticsearch near real-time?
How to ensure consistent writes and reads?&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>es架构-2</title>
      <link>https://dengziming.github.io/post/es/es%E6%9E%B6%E6%9E%84-2/</link>
      <pubDate>Sun, 22 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/es/es%E6%9E%B6%E6%9E%84-2/</guid>
      
        <description>

&lt;p&gt;es设计架构，良心参考资料：
&lt;a href=&#34;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-i-7ac9a13b05db&#34;&gt;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-i-7ac9a13b05db&lt;/a&gt;
&lt;a href=&#34;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-ii-6db4e821b571&#34;&gt;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-ii-6db4e821b571&lt;/a&gt;
&lt;a href=&#34;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-iii-8bb6ac84488d&#34;&gt;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-iii-8bb6ac84488d&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;一-anatomy-of-an-elasticsearch-cluster-2&#34;&gt;一、Anatomy of an Elasticsearch Cluster -2&lt;/h2&gt;

&lt;p&gt;上一节我们说了：underlying storage model and CRUD operations in Elasticsearch.这一节的内容主要包括：
Consensus — split-brain problem and importance of quorum
Concurrency
Consistency: Ensuring consistent writes and reads
Translog (Write Ahead Log — WAL)
Lucene segments&lt;/p&gt;

&lt;h3 id=&#34;1-consensus-split-brain-problem-and-importance-of-quorum&#34;&gt;1. Consensus- Split-brain problem and importance of quorum&lt;/h3&gt;

&lt;p&gt;Consensus 算法包括 Raft、Paxos等，为了解决一致性问题，es的一致性算法有两个部分：&lt;/p&gt;

&lt;p&gt;Ping: The process nodes use to discover each other
Unicast: The module that contains a list of hostnames to control which nodes to ping&lt;/p&gt;

&lt;p&gt;es是一个P2P的系统，所有节点都和其他节点沟通，有一个主节点，控制和更新集群操作。一个新的集群需要经过选举，一个节点被选为master，其他的加入master。
As nodes join, they send a join request to the master with a default join_timeout which is 20 times the ping_timeout.
如果mster节点挂了，cluster重新开始ping，开始新的选举。这种ping过程也帮忙解决脑裂（某个节点突然觉得maste挂了，开始寻找新master）&lt;/p&gt;

&lt;p&gt;为了容错，master会ping 所有的 节点去检查是否 alive然后节点会ping master进行response。
默认配置下，es可能会有脑裂， 由于 network partition,a node 觉得 master 已经 failed然后自己当上master，导致连个master。
This may result in data loss and it may not be possible to merge the data correctly.
This can be avoided by setting the following property to a quorum of master eligible nodes.
&lt;code&gt;discovery.zen.minimum_master_nodes = int(# of master eligible nodes/2)+1&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;设置这个配置之后，需要有 quorum of active master eligible nodes 参加完成master选举过程，并接受他的master身份。
This is an extremely important property to ensure cluster stability and can be dynamically updated if the cluster size changes.
NOTE: For a production cluster, it is recommended to have 3 dedicated master nodes, which do not serve any client requests, out of which 1 is active at any given time.&lt;/p&gt;

&lt;p&gt;这就是 Consensus 的内容&lt;/p&gt;

&lt;h3 id=&#34;2-concurrency&#34;&gt;2. Concurrency&lt;/h3&gt;

&lt;p&gt;对于高并发，es使用 optimistic concurrency control 乐冠锁进行控制，保证新纪录不被就记录覆盖。
每个document indexed 有一个 version number which is incremented with every change applied to that document。保证每次更新都能按照顺序。
为了保证数据不丢失，es可以让你自己指定id，如果你指定的id比present的小，更新就失败了。
How failed requests are handled can be controlled at the application level.
There are also other locking options available and you can read about them：&lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/guide/2.x/concurrency-solutions.html&#34;&gt;https://www.elastic.co/guide/en/elasticsearch/guide/2.x/concurrency-solutions.html&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;3-consistency-ensuring-consistent-writes-and-reads&#34;&gt;3. Consistency — Ensuring consistent writes and reads&lt;/h3&gt;

&lt;p&gt;写一致
对于怎样算写成功，可以设置 available的 shards 数量
The available options are quorum, one and all. By default it is set to quorum and that means that a write operation will be permitted only if a majority of the shards are available.&lt;/p&gt;

&lt;p&gt;尽管大多数available，也有可能出错。the replica is said to be faulty and the shard would be rebuilt on a different node.&lt;/p&gt;

&lt;p&gt;读一致：
new documents are not available for search until after the refresh interval.
为了保证读到最新的document，replication can be set to sync (default) 。这样只有 primary and replica shards 都写完了才会返回 write request 。
这样，从任何一个shard查询，都将返回最新的document。
Even if your application requires replication=async for higher indexing rate, there is a _preference parameter which can be set to primary for search requests.
这样，查询都走 primary shard ，保证结果都来自最新版本。&lt;/p&gt;

&lt;h3 id=&#34;2-translog&#34;&gt;2. Translog&lt;/h3&gt;

&lt;p&gt;WAL来自关系型数据库的世界，translog保证事件失败时候的数据完整性，通过底层的原则 :在将数据提交到磁盘的实际更改之前，必须记录并提交预期的更改。&lt;/p&gt;

&lt;p&gt;当新 document被index，或者旧的更改，Lucene index会改变，然后改变会被提交到磁盘进行持久化。每次update都进行提交很expensive，更好地办法是一次性提交很多。
上一节提到的，flush操作默认30min一次或者translog太大，这样的话，有可能丢失30min的数据。为了避免这个问题，es使用translog，update操作都将被写到translog，
translog is fsync’ed after every index/delete/update operation (or every 5 sec by default) 保证改变被持久化。
The client receives acknowledgement for writes after the translog is fsync’ed on both primary and replica shards.&lt;/p&gt;

&lt;p&gt;当两次flush之间出现问题，translog将会重新执行，从而恢复丢失的change，
NOTE: It is recommended to explicitly flush the translog before restarting Elasticsearch instances,
as the startup will be faster because the translog to be replayed will be empty.
POST /_all/_flush command can be used to flush all indices in the cluster.&lt;/p&gt;

&lt;p&gt;上面已经说了translog的flush操作，segments文件会被提交到磁盘来保证改变持久化。接下来我们看看什么是lucene的 segment。&lt;/p&gt;

&lt;h3 id=&#34;3-lucene-segments&#34;&gt;3.Lucene Segments&lt;/h3&gt;

&lt;p&gt;Lucene索引由多个段组成，并且段本身是一个全功能倒置索引。
是不可变的，它允许Lucene在不从头开始重建索引的情况下增量地向索引添加新文档。
对于每一个搜索请求，索引中的所有段都被搜索，并且每个段消耗CPU周期、文件句柄和内存。这意味着段数越高，搜索性能就越低。
为了解决这个问题，es将小的段合并成更大的段，将新合并的段提交到磁盘并删除旧的较小的段。
对于搜索请求，搜索给定的弹性搜索索引碎片中的所有Lucene段，但是，在排名结果中提取所有匹配的文档或文档对于弹性搜索集群是危险的。&lt;/p&gt;

&lt;h2 id=&#34;what-next&#34;&gt;What next?&lt;/h2&gt;

&lt;p&gt;在后续文章中，我们将看到这一点，并回顾下面的主题，其中包括在弹性搜索中进行的一些权衡，以在低延迟下服务相关搜索结果。&lt;/p&gt;

&lt;p&gt;lit brain problem in Elasticsearch and how to avoid it
Transaction log
Lucene segments
Why deep pagination during search is dangerous?
Difficulties and trade-offs in calculating search relevance
Concurrency control
Why is Elasticsearch near real-time?
How to ensure consistent writes and reads?&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>es架构-3</title>
      <link>https://dengziming.github.io/post/es/es%E6%9E%B6%E6%9E%84-3/</link>
      <pubDate>Sun, 22 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/es/es%E6%9E%B6%E6%9E%84-3/</guid>
      
        <description>

&lt;p&gt;es设计架构，良心参考资料：
&lt;a href=&#34;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-i-7ac9a13b05db&#34;&gt;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-i-7ac9a13b05db&lt;/a&gt;
&lt;a href=&#34;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-ii-6db4e821b571&#34;&gt;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-ii-6db4e821b571&lt;/a&gt;
&lt;a href=&#34;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-iii-8bb6ac84488d&#34;&gt;https://blog.insightdatascience.com/anatomy-of-an-elasticsearch-cluster-part-iii-8bb6ac84488d&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;一-anatomy-of-an-elasticsearch-cluster-3&#34;&gt;一、Anatomy of an Elasticsearch Cluster -3&lt;/h2&gt;

&lt;p&gt;上一节我们说了：how Elasticsearch approaches some of the fundamental challenges of a distributed system.这一节的内容主要包括：&lt;/p&gt;

&lt;p&gt;Near real-time search&lt;/p&gt;

&lt;p&gt;Why deep pagination in distributed search can be dangerous?&lt;/p&gt;

&lt;p&gt;Trade-offs in calculating search relevance&lt;/p&gt;

&lt;h3 id=&#34;1-near-real-time-search&#34;&gt;1. Near real-time search&lt;/h3&gt;

&lt;p&gt;尽管改变不会立即可见，但是es确实提供了近实时查询，前面的内容提到，lucene的segment改变提交到磁盘是很消耗性能的。&lt;/p&gt;

&lt;p&gt;为了避免search的同时提交改变到磁盘，es在memory buffer和disk之间提供了一个 filesystem cache，memory buffer 默认1s refreshed 一次，
一个包含倒排索引的segment也会在 filesystem cache中生成。segment是开放的可以查询。&lt;/p&gt;

&lt;p&gt;filesystem cache有文件句柄而且可以打开，读写，关闭，尽管它在内存中。
Since, the refresh interval is 1 sec by default,  the changes are not visible right away ，所以是准实时。
Since, the translog is a persistent record of changes not persisted to the disk, it also helps with the near real-time aspect for CRUD operations.
在查找相关段之前，在 translog 中搜索任何最近的变化，因此，客户端可以访问近实时的所有变化。&lt;/p&gt;

&lt;p&gt;你可以每次更新后手动刷新index，但是这样会产生很多小segment，不推荐。
For a search request, all Lucene segments in a given shard of an Elasticsearch index are searched。
however, fetching all matching documents or documents deep in the resulting pages is dangerous for your Elasticsearch cluster. Let’s see why that is.&lt;/p&gt;

&lt;h3 id=&#34;2-why-deep-pagination-in-distributed-search-can-be-dangerous&#34;&gt;2. Why deep pagination in distributed search can be dangerous?&lt;/h3&gt;

&lt;p&gt;当我们查到的es中有很多匹配到的document，默认返回最相关的10条，分页操作会提供from和size两个参数，然后每个shard上面会产生from+size个结果放进优先队列，最后汇总。&lt;/p&gt;

&lt;p&gt;加入你需要的是10000到100010个结果，每个shard的优先队列会返回10010个结果排序后放进 memory中，这样将会有很大的隐患。
scroll API （&lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-scroll.html）可以让你返回所有的结果。&#34;&gt;https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-scroll.html）可以让你返回所有的结果。&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;前面说到es使用tf-idf算法，分布式系统计算idf时很麻烦的，需要有aggregate操作。es的做法是返回一个local idf：
Instead, every shard calculates a local idf to assign a relevance score to the resulting documents and returns the result for only the documents on that shard.
Similarly, all the shards return the resulting documents with relevant scores calculated using local idf and the coordinating node sorts all the results to return the top ones。
大多数情况可靠，但是数据倾斜时候就不太可靠了。&lt;/p&gt;

&lt;p&gt;对于上面的问题有两种 trade-off，都不太适合大规模数据。一种是只有一个shard，这样local idf就是 globe idf。另一种是 dfs_query_then_search，先把local idf合并成globe idf，然后在计算。&lt;/p&gt;

&lt;h2 id=&#34;what-next&#34;&gt;What next?&lt;/h2&gt;

&lt;p&gt;In the last few posts, we reviewed some of the fundamental principles of Elasticsearch which are important to understand in order to get started.&lt;/p&gt;

&lt;p&gt;接下来，我们要看看他的源码，或者看看怎么和spark、hadoop结合使用。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>hadoop网站日志分析项目架构</title>
      <link>https://dengziming.github.io/post/project/hadoop/hadoop/</link>
      <pubDate>Fri, 30 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/project/hadoop/hadoop/</guid>
      
        <description>

&lt;p&gt;项目简介：大数据涉及到的业务很多很复杂，从一开始的项目架构，再到后台的网站搭建，以及数据的收集，数据的分析，数据的迁移，业务开发，后台运维，等等。我们没办法一个实验将所有的过程都学习到。本次试验我们将会将重点放在项目架构上，后面的项目我们将重点放在每一部分的实现上。通过本次实验，你将能了解到一个大数据架构师工作的基本步骤，虽然本次实验我们也有复杂的代码分析过程，但是大家没有必要将自己的重点放在代码上面，大家应该更加站在架构师的角度，专注于整个项目每一部分的连接，每个部分具体实现的细节，大家可以不必太深入，我们后期会有专门的实验放在这上面。
有关代码我们已经实现并且提供，大家直接打开，然后阅读熟悉每部分的意义即可。
本次项目我们是架构一个日志分析，我们的要完成的任务包括后台和前端的实现，网站的搭建，nginx反向代理的搭建，etl数据清洗程序，数据分析，数据报表的实现。&lt;/p&gt;

&lt;h2 id=&#34;一-业务分析和需求文档&#34;&gt;一、业务分析和需求文档&lt;/h2&gt;

&lt;h3 id=&#34;1-业务分析概述&#34;&gt;1.业务分析概述&lt;/h3&gt;

&lt;p&gt;本次试验我们主要是分析类似淘宝等购物网站上的点击流，从而进行展示分析。在本次项目中我们分别从七个大的角度来进行分析，分别为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;用户基本信息分析模块、浏览器信息分析模块、地域信息分析模块、用户浏览深度分析模块、外链数据分析模块、订单分析模块以及事件分析模块。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意几个概念:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;用户/访客：表示同一个浏览器代表的用户。唯一标示用户
会员：表示网站的一个正常的会员用户。
会话：一段时间内的连续操作，就是一个会话中的所有操作。
Pv：访问页面的数量
在本次项目中，所有的计数都是去重过的。比如：活跃用户/访客，计算uuid的去重后的个数。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们分析数据的需求文档和最终的展示结果大概如下。&lt;/p&gt;

&lt;h3 id=&#34;2-用户基本信息分析模块&#34;&gt;2.用户基本信息分析模块&lt;/h3&gt;

&lt;p&gt;用户基本信息分析模块主要是从用户/访客和会员两个主要角度分析浏览相关信息，包括但不限于新增用户，活跃用户，总用户，新增会员，活跃会员，总会员以及会话分析等。下面就各个不同的用户信息角度来进行分析：&lt;/p&gt;

&lt;h4 id=&#34;1-用户分析&#34;&gt;(1).用户分析&lt;/h4&gt;

&lt;p&gt;该分析主要分析新增用户、活跃用户以及总用户的相关信息。
新访客:老访客(活跃访客中) =  1:7~10
&lt;img src=&#34;../images/2017-06-20-21-21-54.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;2-会员分析&#34;&gt;(2).会员分析&lt;/h4&gt;

&lt;p&gt;该分析主要分析新增会员、活跃会员以及总会员的相关信息。
&lt;img src=&#34;../images/2017-06-20-21-22-26.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;3-会话分析&#34;&gt;(3).会话分析&lt;/h4&gt;

&lt;p&gt;该分析主要分析会话个数、会话长度和平均会话长度相关的信息。
&lt;img src=&#34;../images/2017-06-20-21-23-17.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;4-hourly分析&#34;&gt;(4).Hourly分析&lt;/h4&gt;

&lt;p&gt;该分析主要分析每天每小时的用户、会话个数以及会话长度的相关信息。
&lt;img src=&#34;../images/2017-06-20-21-24-07.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;3-其他模块分析&#34;&gt;3.其他模块分析&lt;/h3&gt;

&lt;p&gt;在用户模块的基础上，我们可以添加其他的六个模块分析，我们本次试验先不展示所有的模块，只是作简单介绍，例如地域分布模块：
&lt;img src=&#34;../images/2017-06-20-21-25-43.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;上面分析的业务需求大家可能不太懂，没关系，注意在下面的项目中，时不时回头看看我们的需求，就能明白了。&lt;/p&gt;

&lt;h2 id=&#34;二-开发环境搭建&#34;&gt;二、开发环境搭建&lt;/h2&gt;

&lt;p&gt;为了方便管理，我们以后按照管理，尽量使用maven构建java和scala项目。另外我们的软件安装在D盘的&lt;code&gt;soft&lt;/code&gt;目录下，我们的开发项目放在D盘的&lt;code&gt;workspace&lt;/code&gt;目录下。&lt;/p&gt;

&lt;h3 id=&#34;1-下载安装软件&#34;&gt;1.下载安装软件&lt;/h3&gt;

&lt;p&gt;分别在&lt;code&gt;http://tomcat.apache.org&lt;/code&gt;和&lt;code&gt;http://maven.apache.org&lt;/code&gt;下载tomcat和maven，解压后放在D盘的soft目录，然后配置环境变量，需要配置的环境变量包括 &lt;code&gt;MAVEN_HOME&lt;/code&gt;和&lt;code&gt;TOMCAT_HOME&lt;/code&gt;，并且将他们的bin目录添加到&lt;code&gt;PATH&lt;/code&gt;中。安装配置完成后，在命令行输入start-up和mvn命令，检查是否安装正确。
确保无误后，我们的开发环境使用IDEA，安装好IDEA，打开，配置maven的目录，如下图的方法，搜索maven，在Maven的配置填写maven的路径。
&lt;img src=&#34;../images/2017-06-20-19-07-00.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;2-搭建服务器&#34;&gt;2.搭建服务器&lt;/h3&gt;

&lt;h4 id=&#34;1-布置开发环境&#34;&gt;(1).布置开发环境&lt;/h4&gt;

&lt;p&gt;如果大家熟悉javaEE开发，这一段就比较简单。我们搭建服务器就是新建一个javaEE项目，然后启动，这个过程需要借助tomcat实现。首先打开IDEA，IDEA中已经配置好了maven的路径。
点击File -&amp;gt;New -&amp;gt; Project ，选择java的web application，然后下一步:
&lt;img src=&#34;../images/2017-06-20-19-09-55.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在下一步我们设置项目路径，我们的项目名为&lt;code&gt;taobaopayment&lt;/code&gt;，放在D盘下的workspace目录下。然后点击完成。这时候就新建了一个web项目，我们在项目的web文件下能看到一个index.jsp文件，这个文件你可以修改为自定义的内容，例如我修改为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;%@ page contentType=&amp;quot;text/html;charset=UTF-8&amp;quot; language=&amp;quot;java&amp;quot; %&amp;gt;
&amp;lt;html&amp;gt;
  &amp;lt;head&amp;gt;
    &amp;lt;title&amp;gt;taobaopayment&amp;lt;/title&amp;gt;
  &amp;lt;/head&amp;gt;
  &amp;lt;body&amp;gt;
  支付页面
  &amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;2-本地发布项目&#34;&gt;(2).本地发布项目&lt;/h4&gt;

&lt;p&gt;菜单栏选择Run -&amp;gt;Edit Configuration或者点击右上角按钮添加tomcat的发布参数，依次点击 加号 -&amp;gt;tomcat server -&amp;gt; local ，添加tomcat：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/2017-06-20-19-18-24.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在右边的配置页面配置好名字、地址、端口：
&lt;img src=&#34;../images/2017-06-20-19-20-30.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;然后在deployment选项下面点击加号添加发布选项，然后设置你content名字，我们设置为 &lt;code&gt;taobaopayment&lt;/code&gt;：
&lt;img src=&#34;../images/2017-06-20-19-21-52.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;点击确定后，我们可以看到右上方和下方都出现了可以启动的三角形按钮，点击启动：
&lt;img src=&#34;../images/2017-06-20-19-25-24.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;启动成功后打开浏览器，输入&lt;code&gt;http://localhost:8080/taobaopayment/&lt;/code&gt;，出现我们刚刚编辑的jsp页面，剩下的操作自己实验。
&lt;img src=&#34;../images/2017-06-20-19-24-42.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;三-web服务器开发&#34;&gt;三、Web服务器开发&lt;/h2&gt;

&lt;p&gt;根据我们的需求文档，我们需要实现支付成功和退款页面。这里又分为两部分，一是前端的页面传来的请求数据，这部分代码使用JavaScript编写，另一方面是后台的服务器发送过来的代码，通过Java语言编写。&lt;/p&gt;

&lt;h3 id=&#34;1-后端开发&#34;&gt;1.后端开发&lt;/h3&gt;

&lt;h4 id=&#34;1-程序后台事件分析&#34;&gt;(1).程序后台事件分析&lt;/h4&gt;

&lt;p&gt;本项目中在程序后台会有chargeSuccess事件，本事件的主要作用是发送订单成功的信息给nginx服务器。发送格式同pc端发送方式， 也是访问同一个url来进行数据的传输。格式为:
&lt;code&gt;http://hongyahuayu.com/index.jpg?query1=spark&lt;/code&gt;
当会员最终支付成功的时候触发chargeSuccess该事件，该事件需要程序主动调用，然后向后台发送数据：
&lt;code&gt;u_mid=maomao&amp;amp;c_time=1449142044528&amp;amp;oid=orderid_1&amp;amp;ver=1&amp;amp;en=e_cs&amp;amp;pl=jdk&amp;amp;sdk=java&lt;/code&gt;，其中 &lt;code&gt;u_mid&lt;/code&gt;和&lt;code&gt;oid&lt;/code&gt;代表用户id和订单id。
前面我们分析了后端的业务，如果你不太懂，我们尅简单地说，后端程序的工作流如下：
&lt;img src=&#34;../images/2017-06-20-21-52-16.jpg&#34; alt=&#34;&#34; /&gt;
简单说，后端就是要设计方法，当&lt;code&gt;chargeSuccess&lt;/code&gt;触发的时候，我们给后台发送数据。&lt;/p&gt;

&lt;h4 id=&#34;2-后端程序开发&#34;&gt;(2).后端程序开发&lt;/h4&gt;

&lt;p&gt;程序开发有一定难度，另外由于我们本次试验的重点是后面的数据分析，这一块不作太高的要求，大家能够理解即可，核心代码如下，其余代码可以在项目中查看：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;	public static boolean onChargeSuccess(String orderId, String memberId) {
		try {
			if (isEmpty(orderId) || isEmpty(memberId)) {
				// 订单id或者memberid为空
				log.log(Level.WARNING, &amp;quot;订单id和会员id不能为空&amp;quot;);
				return false;
			}
			// 代码执行到这儿，表示订单id和会员id都不为空。
			Map&amp;lt;String, String&amp;gt; data = new HashMap&amp;lt;String, String&amp;gt;();
			data.put(&amp;quot;u_mid&amp;quot;, memberId);
			data.put(&amp;quot;oid&amp;quot;, orderId);
			data.put(&amp;quot;c_time&amp;quot;, String.valueOf(System.currentTimeMillis()));
			data.put(&amp;quot;ver&amp;quot;, version);
			data.put(&amp;quot;en&amp;quot;, &amp;quot;e_cs&amp;quot;);
			data.put(&amp;quot;pl&amp;quot;, platformName);
			data.put(&amp;quot;sdk&amp;quot;, sdkName);
			// 创建url
			String url = buildUrl(data);
			// 发送url&amp;amp;将url加入到队列
			SendDataMonitor.addSendUrl(url);
			return true;
		} catch (Throwable e) {
			log.log(Level.WARNING, &amp;quot;发送数据异常&amp;quot;, e);
		}
		return false;
	}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;注意事项&lt;/em&gt;&lt;/strong&gt; 修改代码这里url地址为自己服务器的地址：
&lt;img src=&#34;../images/2017-06-20-23-42-54.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;2-前端开发&#34;&gt;2.前端开发&lt;/h3&gt;

&lt;h4 id=&#34;1-前端事件分析&#34;&gt;(1).前端事件分析&lt;/h4&gt;

&lt;p&gt;前面我们说后端的事件主要是chargeSuccess，前端的时间处理就更复杂了。针对我们最终的不同分析模块，我们需要不同的数据，接下来分别从各个模块分析，每个模块需要的数据。
1. 用户基本信息就是用户的浏览行为信息分析，也就是我们只需要pageview事件就可以了；
2. 浏览器信息分析以及地域信息分析其实就是在用户基本信息分析的基础上添加浏览器和地域这个维度信息，其中浏览器信息我们可以通过浏览器的window.navigator.userAgent来进行分析，地域信息可以通过nginx服务器来收集用户的ip地址来进行分析，也就是说pageview事件也可以满足这两个模块的分析。
3. 外链数据分析以及用户浏览深度分析我们可以在pageview事件中添加访问页面的当前url和前一个页面的url来进行处理分析，也就是说pageview事件也可以满足这两个模块的分析。
4. 订单信息分析要求pc端发送一个订单产生的事件，那么对应这个模块的分析，我们需要一个新的事件chargeRequest。对于事件分析我们也需要一个pc端发送一个新的事件数据，我们可以定义为event。
我们要分析的模块包括：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;用户基本信息分析
浏览器信息分析
地域信息分析
外链数据分析
用户浏览深度分析
订单信息分析
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们处理的事件包括：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pageview事件
chargeRequest事件
launch事件
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;第一，Launch事件。当用户第一次访问网站的时候触发该事件，不提供对外调用的接口，只实现该事件的数据收集。
第二，Pageview事件，当用户访问页面/刷新页面的时候触发该事件。该事件会自动调用，也可以让程序员手动调用。
第三，chargeRequest事件。当用户下订单的时候触发该事件，该事件需要程序主动调用。
每次都会发送对应的数据，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;u_sd=8E9559B3-DA35-44E1-AC98-85EB37D1F263&amp;amp;c_time=1449139048231&amp;amp;oid=orderid123&amp;amp;on=%E4%BA%A7%E5%93%81%E5%90%8D%E7%A7%B0&amp;amp;cua=1000&amp;amp;cut=%E4%BA%BA%E6%B0%91%E5%B8%81&amp;amp;pt=%E6%B7%98%E5%AE%9D&amp;amp;ver=1&amp;amp;en=e_cr&amp;amp;pl=website&amp;amp;sdk=js&amp;amp;b_rst=1920*1080&amp;amp;u_ud=12BF4079-223E-4A57-AC60-C1A04D8F7A2F&amp;amp;b_iev=Mozilla%2F5.0%20(Windows%20NT%206.1%3B%20WOW64)%20AppleWebKit%2F537.1%20(KHTML%2C%20like%20Gecko)%20Chrome%2F21.0.1180.77%20Safari%2F537.1&amp;amp;l=zh-CN
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个url的字段比较多，字段词典如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;参数名称	类型	描述
en	string	事件名称, eg: e_pv
ver	string	版本号, eg: 0.0.1
pl	string	平台, eg: website
sdk	string	Sdk类型, eg: js
b_rst	string	浏览器分辨率，eg: 1800*678
b_iev	string	浏览器信息useragent
u_ud	string	用户/访客唯一标识符
l	string	客户端语言
u_mid	string	会员id，和业务系统一致
u_sd	string	会话id
c_time	string	客户端时间
p_url	string	当前页面的url
p_ref	string	上一个页面的url
tt	string	当前页面的标题
ca	string	Event事件的Category名称
ac	string	Event事件的action名称
kv_*	string	Event事件的自定义属性
du	string	Event事件的持续时间
oid	string	订单id
on	string	订单名称
cua	string	支付金额
cut	string	支付货币类型
pt	string	支付方式
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;2-前端程序开发&#34;&gt;(2).前端程序开发&lt;/h4&gt;

&lt;p&gt;前面我们简单实现了后端开发，现在前端的JavaScript代码实现可能就更复杂了，对大家来说难度略大，但是还好这不是我们的重点，我大概展示几个函数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;onPageView: function() {
				// 触发page view事件
				if (this.preCallApi()) {
					var time = new Date().getTime();
					var pageviewEvent = {};
					pageviewEvent[this.columns.eventName] = this.keys.pageView;
					pageviewEvent[this.columns.currentUrl] = window.location.href; // 设置当前url
					pageviewEvent[this.columns.referrerUrl] = document.referrer; // 设置前一个页面的url
					pageviewEvent[this.columns.title] = document.title; // 设置title
					this.setCommonColumns(pageviewEvent); // 设置公用columns
					this.sendDataToServer(this.parseParam(pageviewEvent)); // 最终发送编码后的数据ss
					this.updatePreVisitTime(time);
				}
			},

			onChargeRequest: function(orderId, name, currencyAmount, currencyType, paymentType) {
				// 触发订单产生事件
				if (this.preCallApi()) {
					if (!orderId || !currencyType || !paymentType) {
						this.log(&amp;quot;订单id、货币类型以及支付方式不能为空&amp;quot;);
						return ;
					}

					if (typeof(currencyAmount) == &amp;quot;number&amp;quot;) {
						// 金额必须是数字
						var time = new Date().getTime();
						var chargeRequestEvent = {};
						chargeRequestEvent[this.columns.eventName] = this.keys.chargeRequestEvent;
						chargeRequestEvent[this.columns.orderId] = orderId;
						chargeRequestEvent[this.columns.orderName] = name;
						chargeRequestEvent[this.columns.currencyAmount] = currencyAmount;
						chargeRequestEvent[this.columns.currencyType] = currencyType;
						chargeRequestEvent[this.columns.paymentType] = paymentType;
						this.setCommonColumns(chargeRequestEvent); // 设置公用columns
						this.sendDataToServer(this.parseParam(chargeRequestEvent)); // 最终发送编码后的数据ss
						this.updatePreVisitTime(time);
					} else {
						this.log(&amp;quot;订单金额必须是数字&amp;quot;);
						return ;
					}	
				}
			},
			
			onEventDuration: function(category, action, map, duration) {
				// 触发event事件
				if (this.preCallApi()) {
					if (category &amp;amp;&amp;amp; action) {
						var time = new Date().getTime();
						var event = {};
						event[this.columns.eventName] = this.keys.eventDurationEvent;
						event[this.columns.category] = category;
						event[this.columns.action] = action;
						if (map) {
							for (var k in map){
								if (k &amp;amp;&amp;amp; map[k]) {
									event[this.columns.kv + k] = map[k];
								}
							}
						}
						if (duration) {
							event[this.columns.duration] = duration;
						}
						this.setCommonColumns(event); // 设置公用columns
						this.sendDataToServer(this.parseParam(event)); // 最终发送编码后的数据ss
						this.updatePreVisitTime(time);
					} else {
						this.log(&amp;quot;category和action不能为空&amp;quot;);
					}
				}
			}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;完整的代码，如果有兴趣自己可以详细研究，
*** 注意事项 ***，发布的时候一定要将代码中的url改为你的服务器的url：
&lt;img src=&#34;../images/2017-06-20-23-05-24.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;3-项目发布&#34;&gt;3.项目发布&lt;/h3&gt;

&lt;h4 id=&#34;1-本地项目发布&#34;&gt;(1).本地项目发布&lt;/h4&gt;

&lt;p&gt;前面我们已经简单的实现了后台和前端的代码，首先我们在本地启动服务，方法和前面一样，只是我们将自己的代码添加进去了，点击启动按钮。
&lt;img src=&#34;../images/2017-06-20-22-14-58.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;然后去浏览器访问 &lt;code&gt;http://localhost:8080/taobaopayment/demo4.jsp&lt;/code&gt;，然后点击跳转按钮测试
&lt;img src=&#34;../images/2017-06-20-22-16-22.jpg&#34; alt=&#34;&#34; /&gt;
截止现在我们的后台项目基本完成，这里面的代码比较难，大家可以根据情况查看，不用花太多的精力。&lt;/p&gt;

&lt;h4 id=&#34;2-发布到linux的tomcat上&#34;&gt;(2).发布到linux的tomcat上&lt;/h4&gt;

&lt;p&gt;我们的项目不可能放在本地运行，需要放到Linux的集群环境才能正常运行。首先打开Xshell，连上linux节点。
第一步，在Linux上安装tomcat。
安装的过程很简单，首先要安装java配置JAVA相关环境变量，然后在tomcat的官网下载tomcat的tar.gz包，解压，然后配置tomcat相关环境变量。启动tomcat的命令是tomcat安装目录下面的bin下面的&lt;code&gt;startup.sh&lt;/code&gt;，执行就能启动tomcat，然后访问节点的8080端口，如图：
&lt;img src=&#34;../images/2017-06-20-22-27-11.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;第二步，将项目打成war包。
类似以前打jar包，点开project structure -&amp;gt; artificts，添加一个artifict，名字为taobaopayment，type选择 Web Application:Archive，设置好对应的输出目录output idrectory，一般默认即可，然后点击确定&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/2017-06-20-22-28-57.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;设置好后，点击build -&amp;gt; build artifacts，构建war包
&lt;img src=&#34;../images/2017-06-20-22-32-33.jpg&#34; alt=&#34;&#34; /&gt;
构建结束后，进入刚刚设置的输出目录，你将会在刚刚设置的目录下看到一个war包。&lt;/p&gt;

&lt;p&gt;通过xshell的xftp工具，将打出来的war包拷贝放在linux的tomcat安装目录的webapps目录下：
&lt;img src=&#34;../images/2017-06-20-22-35-41.jpg&#34; alt=&#34;&#34; /&gt;
然后tomcat会自动解压war包，我们就可以在浏览器访问刚刚发布的项目了：
&lt;img src=&#34;../images/2017-06-20-22-39-45.jpg&#34; alt=&#34;&#34; /&gt;
到这里我们的整个后端项目就发布成功了。&lt;/p&gt;

&lt;h2 id=&#34;四-数据分析系统开发&#34;&gt;四、数据分析系统开发&lt;/h2&gt;

&lt;p&gt;本次我们的重心是整个系统的搭建，这部分的开发过程比较复杂，大家酌情进行学习，代码我们已经写好，大家只要稍作理解。细节和逻辑我们后续的实验还会讲解。&lt;/p&gt;

&lt;h3 id=&#34;1-需求回顾&#34;&gt;1.需求回顾&lt;/h3&gt;

&lt;p&gt;之前我们已经做过需求分析，这时候大家再回头看看我们一开始的需求设计，我们是要完成几个关键指标的设计分析。假设我们已经配置好了tomcat和nginx，那么我们知道每次用有浏览等行为时，我们的服务器就会给我们的设置的url发送数据，然后nginx就会收到我们发送的数据。接下来就是分析nginx收集到的数据。&lt;/p&gt;

&lt;p&gt;随便选取一条数据查看：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;192.168.126.1^A1458731952.690^A192.168.126.11^A/log.gif?en=e_pv&amp;amp;p_url=http%3A%2F%2Flocalhost%3A8080%2FBIG_DATA_LOG2%2Fdemo.jsp&amp;amp;p_ref=http%3A%2F%2Flocalhost%3A8080%2FBIG_DATA_LOG2%2Fdemo.jsp&amp;amp;tt=%E6%B5%8B%E8%AF%95%E9%A1%B5%E9%9D%A21&amp;amp;ver=1&amp;amp;pl=website&amp;amp;sdk=js&amp;amp;u_ud=EAB36BC9-0347-4D33-8579-AA8C331D001A&amp;amp;u_mid=laoxiao&amp;amp;u_sd=2D24B8A2-B2EF-450C-8C86-4F8B0F3E2785&amp;amp;c_time=1458731943823&amp;amp;l=zh-CN&amp;amp;b_iev=Mozilla%2F5.0%20(Windows%20NT%2010.0%3B%20WOW64%3B%20rv%3A45.0)%20Gecko%2F20100101%20Firefox%2F45.0&amp;amp;b_rst=1366*768
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这条数据提取了很多来自url的query信息，通过^A隔开，我们需要编写代码分析这种数据。&lt;/p&gt;

&lt;h3 id=&#34;2-etl处理&#34;&gt;2.ETL处理&lt;/h3&gt;

&lt;h4 id=&#34;1-定义工具类&#34;&gt;(1).定义工具类&lt;/h4&gt;

&lt;p&gt;本次试验的工具类主要是从一个url中抽取KPI信息，我们前面的业务需要的信息包括了地址、浏览器、操作系统等，根据我们的分析，所以我们需要使用IP地址解析等工具。解析url的工具类我们放在&lt;code&gt;com.hongya.etl.util&lt;/code&gt;下面，大家自己认真分析。&lt;/p&gt;

&lt;h4 id=&#34;2-定义相关常量类&#34;&gt;(2).定义相关常量类&lt;/h4&gt;

&lt;p&gt;我们的数据分析是有时间段的，我们分析的结果放进hbase中的表中，表名、字段名、时间范围等都是需要用到的常量，我们放在&lt;code&gt;com.hongya.common&lt;/code&gt;下面，大家可以查看。&lt;/p&gt;

&lt;h4 id=&#34;3-业务代码&#34;&gt;(3).业务代码&lt;/h4&gt;

&lt;p&gt;我们的数据字段含义在前面已经讲过了，现在我们需要将数据解析后放进hbase中。ETL过程就是简单的字符串处理，只需要一个Mapper程序即可完成。相应的代码在&lt;code&gt;com.hongya.etl.mr.ald&lt;/code&gt;中，大家可以查看。&lt;/p&gt;

&lt;h4 id=&#34;4-本地测试运行&#34;&gt;(4).本地测试运行&lt;/h4&gt;

&lt;p&gt;然后我们在本地新建一个文件，将上面哪一行测试数据放进去：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;192.168.126.1^A1458731952.690^A192.168.126.11^A/log.gif?en=e_pv&amp;amp;p_url=http%3A%2F%2Flocalhost%3A8080%2FBIG_DATA_LOG2%2Fdemo.jsp&amp;amp;p_ref=http%3A%2F%2Flocalhost%3A8080%2FBIG_DATA_LOG2%2Fdemo.jsp&amp;amp;tt=%E6%B5%8B%E8%AF%95%E9%A1%B5%E9%9D%A21&amp;amp;ver=1&amp;amp;pl=website&amp;amp;sdk=js&amp;amp;u_ud=EAB36BC9-0347-4D33-8579-AA8C331D001A&amp;amp;u_mid=laoxiao&amp;amp;u_sd=2D24B8A2-B2EF-450C-8C86-4F8B0F3E2785&amp;amp;c_time=1458731943823&amp;amp;l=zh-CN&amp;amp;b_iev=Mozilla%2F5.0%20(Windows%20NT%2010.0%3B%20WOW64%3B%20rv%3A45.0)%20Gecko%2F20100101%20Firefox%2F45.0&amp;amp;b_rst=1366*768
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后我们将&lt;code&gt;com.hongya.etl.mr.ald.AnalyserLogDataRunner&lt;/code&gt;中的&lt;code&gt;setJobInputPaths&lt;/code&gt;方法的路径改为刚刚添加的文件的路径，并且在&lt;code&gt;setConf&lt;/code&gt;方法设置一下zookeeper地址，并且启动zookeeper，就可以点击运行，在本地观察结果。
&lt;img src=&#34;../images/2017-06-21-01-16-19.jpg&#34; alt=&#34;&#34; /&gt;
如图我们可以看出ETL后将这一行数据转化为了Hbase的一条Put，这里一直运行不结束是因为我没有启动Hbase，所以一直没法写进去，发布项目的时候是需要启动的。
这个Mapper读取数据格式上面有，而写出的数据格式是Hbase的Put。不知道大家是否记得Hbase的javaAPI，我们介绍过Put的使用。最后我们每一条记录将会放进Hbase的表格中，例如上面的示例数据最后会解析为一条Put数据，我们可以看出它的rowKey是带着日期的：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rowKey 1458731952690_84973288
cf: info ,key:tt value:测试页面1
cf: info ,key:country value:unknown
cf: info ,key:ver value:1
cf: info ,key:u_mid value:laoxiao
cf: info ,key:os value:Windows
cf: info ,key:city value:unknown
cf: info ,key:ip value:192.168.126.1
cf: info ,key:b_rst value:1366*768
cf: info ,key:en value:e_pv
cf: info ,key:c_time value:1458731943823
cf: info ,key:l value:zh-CN
cf: info ,key:u_sd value:2D24B8A2-B2EF-450C-8C86-4F8B0F3E2785
cf: info ,key:u_ud value:EAB36BC9-0347-4D33-8579-AA8C331D001A
cf: info ,key:os_v value:Windows
cf: info ,key:p_ref value:http://localhost:8080/BIG_DATA_LOG2/demo.jsp
cf: info ,key:province value:unknown
cf: info ,key:s_time value:1458731952690
cf: info ,key:p_url value:http://localhost:8080/BIG_DATA_LOG2/demo.jsp
cf: info ,key:browser value:Firefox
cf: info ,key:sdk value:js
cf: info ,key:pl value:website
cf: info ,key:browser_v value:45.0
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;3-关键指标分析&#34;&gt;3.关键指标分析&lt;/h3&gt;

&lt;p&gt;上面的ETL完成后我们的结果数据都放在hbase的event_logs表格的info列族中，现在我们需要运行代码分析这些数据，对我们的数据进行我们前面的设计文档中的关键指标分析。&lt;/p&gt;

&lt;h4 id=&#34;1-代码结构规范&#34;&gt;(1).代码结构规范&lt;/h4&gt;

&lt;p&gt;我们的程序需要定期分析hbase的数据，我们分析的指标有很多，我们需要从hbase中提取的数据也有很多。最后分析完每个指标后我们放进mysql中，供echart做展示。
1. 首先我们分析的指标需要即KPI需要专门的类定义好，放在&lt;code&gt;com.hongya.common.KpiType&lt;/code&gt;下面。
2. 我们需要自定义Key和Value的类型，这些类型包含了需要统计的关键维度信息，作为mapreduce任务的输入输出key，我们定义好了放在&lt;code&gt;com.hongya.transformer.model.dim&lt;/code&gt;下面。
3. 我们的hadoop任务写mysql需要有专门的OutputFormat，我们放在&lt;code&gt;com.hongya.transformer.service&lt;/code&gt;下面，由于每个唯独统计任务写mysql都不一样，所以我们通过配置文件的方式传入，在&lt;code&gt;output-collector.xml&lt;/code&gt;中有相关配置。
4. 无论是读写mysql还是hbase都有配置，我们通过配置文件的方式传入，配置文件有&lt;code&gt;query-mapping.xml&lt;/code&gt;，&lt;code&gt;transfomer-env.xml&lt;/code&gt;。&lt;/p&gt;

&lt;h4 id=&#34;2-业务代码实现&#34;&gt;(2).业务代码实现&lt;/h4&gt;

&lt;p&gt;上面分析了业务，我们开始写mapreduce程序统计分析指标。
我们知道我们现在的mapreduce读取hbase的数据，然后写进mysql中，相关代码我们放在了&lt;code&gt;com.hongya.transformer.mr&lt;/code&gt;包下面。
由于时间关系，我们的业务只实现了new user指标的统计，大家可以查看&lt;code&gt;com.hongya.transformer.mr.nu&lt;/code&gt;包下的内容。
当我们的hbase中有数据时，运行&lt;code&gt;com.hongya.transformer.mr.nu.NewInstallUserRunner&lt;/code&gt;，就能看到下面的结果。
&lt;img src=&#34;../images/2017-06-21-13-10-29.jpg&#34; alt=&#34;&#34; /&gt;
如果你map完成后reduce就失败 ，没关系，是因为你的mysql还没有配置好，我们在后面会介绍的。&lt;/p&gt;

&lt;h2 id=&#34;四-数据分析系统架构&#34;&gt;四、数据分析系统架构&lt;/h2&gt;

&lt;p&gt;我们有了服务器后，接下来的任务就是对服务器的数据进行收集处理，处理后的数据进行展示。我们需要搭建一个完整的服务器，这时候首先需要一个集群，我们在云端直接使用一个节点的centos作为服务器。首先打开Xshell，连上centos节点，我们这里节点名为&lt;code&gt;node1&lt;/code&gt;。&lt;/p&gt;

&lt;h3 id=&#34;1-部署hadoop和hbase&#34;&gt;1.部署hadoop和Hbase&lt;/h3&gt;

&lt;h4 id=&#34;1-部署hadoop&#34;&gt;(1).部署hadoop&lt;/h4&gt;

&lt;p&gt;hadoop单节点安装很简单，以前讲过。直接解压后，配置&lt;code&gt;hadoop-env.sh、core-site.xml、hdfs-site.xml&lt;/code&gt;
1. hadoop-env.sh配置 JAVA_HOME
2. core-site.xml配置&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt; &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;fs.defaultFS&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;hdfs://localhost:8020&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hadoop.tmp.dir&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;/opt/data/hadoop&amp;lt;/value&amp;gt;
      &amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后就能够格式化，启动：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hdfs namenode -format
start-dfs.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;2-配置hbase&#34;&gt;(2).配置Hbase&lt;/h4&gt;

&lt;p&gt;首先安装单节点的zookeeper，安装好后启动，这个不细说：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;zkServer.sh start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后解压Hbase安装包.
1. 配置hbase-env.sh，配置JAVA_HOME，然后将 HBASE_MANAGES_ZK改为false
2. 配置hbase-site.xm&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hbase.rootdir&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;hdfs://localhost:8020/hbase&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hbase.cluster.distributed&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hbase.zookeeper.quorum&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;localhost&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;复制hadoop配置
复制hadoop的 core-site.xml和hdf-site.xml到hbase的conf目录下
然后就能启动Hbase了。
启动hbase后使用hbase shell进入交互窗口，执行建表语句：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;create &#39;event_logs&#39; ,&#39;info&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果执行成功了就可以开始下一步了。&lt;/p&gt;

&lt;h3 id=&#34;2-部署nginx服务&#34;&gt;2.部署nginx服务&lt;/h3&gt;

&lt;p&gt;前面我们将项目发布到tomcat上面了。正常情况下我们需要通过nginx进行负载均衡，同时收集url的请求日志。
我们可以安装nginx，也可以安装淘宝开源的tengine，比一般nginx多一些功能，而且淘宝上有中文文档。&lt;/p&gt;

&lt;h4 id=&#34;1-nginx安装&#34;&gt;(1).nginx安装&lt;/h4&gt;

&lt;p&gt;步骤如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;1.安装GCC编译器等工具：
yum install -y gcc gcc-c++ autoconf automake libtool make openssl openssl-devel pcre pcre-devel
2.下载安装Nginx:
wget http://nginx.org/download/nginx-1.6.3.tar.gz
注：这里也可以下载tengine压缩包，比一般nginx多一些功能
tar -zxvf nginx-1.6.3.tar.gz 
cd nginx-1.6.3/  
./configure --prefix=/usr/local/nginx
--sbin-path=/usr/local/nginx/sbin/nginx
--conf-path=/usr/local/nginx/conf/nginx.conf
--pid-path=/usr/local/nginx/logs/nginx.pid \
--with-http_ssl_module \
--with-http_stub_status_module \
--with-http_gzip_static_module \ 
make &amp;amp;&amp;amp; make install 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果正常的话，就安装好了，然后启动nginx服务即可。记住启动之前需要先关闭之前的tomcat，因为他们两个的端口冲突了。启动命令是就是nginx，启动以后，如果不修改配置，我们可以直接打开浏览器访问8080端口，出现这样就算是成功了。
&lt;img src=&#34;../images/2017-06-21-15-52-28.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&#34;2-nginx配置&#34;&gt;(2).nginx配置&lt;/h4&gt;

&lt;p&gt;现在我们已经安装了nginx，我们要配置nginx的反向代理，让他替我们监听我们的需要监听的端口。上面我们安装的时候已经配置了配置文件的路径：&lt;code&gt;/usr/local/nginx/conf/nginx.conf&lt;/code&gt;。现在我们修改他的内容。
我们让它监听80端口，这样我们给80端口发送的数据就可以被nginx收集然后我们后期处理：
我们只需要添加下面的内容：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;log_format my_format &#39;$remote_addr^A$msec^A$http_host^A$request_uri&#39;;

location = /log.gif {
   root html;
   ## 配置日志文件保存位置
   access_log /opt/data/access.log my_format;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改后的nginx.conf内容大概是这样的，其实就是添加了一行log_format，然后修改了端口信息：
&lt;img src=&#34;../images/2017-06-21-16-06-08.jpg&#34; alt=&#34;&#34; /&gt;
然后我们通过命令让配置文件生效： sudo nginx -s reload&lt;/p&gt;

&lt;h4 id=&#34;3-测试nginx收集日志&#34;&gt;(3).测试nginx收集日志&lt;/h4&gt;

&lt;p&gt;这时候我们可以启动我们之前的web项目，启动之前记得修改代码的url为刚刚配置的地址格式：
&lt;img src=&#34;../images/2017-06-21-16-11-05.jpg&#34; alt=&#34;&#34; /&gt;
然后我们发布运行项目，或者打war包放在tomcat里面，然后启动tomcat。在浏览器输入：&lt;code&gt;http://localhost:8080/taobaopayment/demo4.jsp&lt;/code&gt; 模拟用户点击。如果你发现浏览器一直处于刷新状态，可能你需要换一个浏览器：
&lt;img src=&#34;../images/2017-06-21-16-14-49.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;你还可以运行我们的Test类，来模拟后台的数据，报错没关系，只需要手动点击停止程序：
&lt;img src=&#34;../images/2017-06-21-16-16-11.jpg&#34; alt=&#34;&#34; /&gt;
然后我们查看刚刚配置的文件，已经有了几条记录，是刚刚我们发送的，而且都是我们配置的格式：
&lt;img src=&#34;../images/2017-06-21-16-25-15.jpg&#34; alt=&#34;&#34; /&gt;
到这里就恭喜，我们的gninx基本完成。&lt;/p&gt;

&lt;h3 id=&#34;3-日志收集系统&#34;&gt;3.日志收集系统&lt;/h3&gt;

&lt;p&gt;日志手机一般有两种方式，shell实现和flume实现。
shell命令前面大家都熟悉过，flume使用在前面的SparkStreaming实验也使用过，我们不介绍过多，简单回顾一下即可。
首先安装好flume，配置JAVA_HOME和HADOOP_HOME，然后新建或者复制一个配置文件，log.cfg ，添加下面的内容：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# 配置三个组件的名字
agent.sources = r1
agent.channels = c1
agent.sinks = k1

# For each one of the sources, the type is defined
agent.sources.r1.type = exec
## 这里配置你刚刚手机日志的文件
agent.sources.r1.command = tail -F /Users/dengziming/opt/data/hongya/taobaopayment/access.log
agent.sources.r1.port = 44444

# The channel can be defined as follows.
agent.channels.c1.type = memory
agent.channels.c1.capacity = 1000
agent.channels.c1.transactionCapacity = 1000

# Each sink&#39;s type must be defined
agent.sinks.k1.type = hdfs
agent.sinks.k1.hdfs.path = hdfs://localhost:8020/flume/events/%Y-%m-%d/%H%M/
agent.sinks.k1.hdfs.filePrefix = events-
agent.sinks.k1.hdfs.round = true
agent.sinks.k1.hdfs.roundValue = 10
agent.sinks.k1.hdfs.roundUnit = minute
agent.sinks.k1.hdfs.useLocalTimeStamp = true

#Specify the channel the sink should use
agent.sources.r1.channels = c1
agent.sinks.k1.channel = c1
agent.channels.memoryChannel.capacity = 100
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置好后适用命令启动：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/flume-ng agent --conf conf --conf-file conf/log.cfg --name agent -D flume.root.logger=INFO,console
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样就会收集我们刚刚nginx的日志到hadoop的 &lt;code&gt;/flume/events/%Y-%m-%d/%H%M/&lt;/code&gt; 路径下
恭喜你，马上就要进入数据分析部分&lt;/p&gt;

&lt;h3 id=&#34;4-提交数据分析任务&#34;&gt;4.提交数据分析任务&lt;/h3&gt;

&lt;p&gt;现在我们要开始运行程序，分析数据了。&lt;/p&gt;

&lt;h4 id=&#34;1-启动zookeeper-hadoop-hbase&#34;&gt;(1).启动zookeeper、hadoop、hbase&lt;/h4&gt;

&lt;p&gt;启动命令就不说了，然后记得之前我们已经在hbase中创建了表格：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;create &#39;event_logs&#39; ,&#39;info&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;../images/2017-06-21-16-45-52.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;2-运行etl任务&#34;&gt;(2).运行ETL任务&lt;/h4&gt;

&lt;p&gt;我们的etl的任务是 &lt;code&gt;com.hongya.etl.mr.ald.AnalyserLogDataRunner&lt;/code&gt;，打开这段代码，我们修改几个路径和配置，因为是测试，我们把数据放在本地运行。修改的主要是zookeeper地址和我们刚刚的日志路径：
&lt;img src=&#34;../images/2017-06-21-16-41-30.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;然后运行程序：
&lt;img src=&#34;../images/2017-06-21-16-46-55.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;然后我们可以根据日志看到打印的rowKey，我们可以在hbase中查看这些rowKey
&lt;img src=&#34;../images/2017-06-21-16-48-51.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;3-创建mysql表格&#34;&gt;(3).创建mysql表格&lt;/h4&gt;

&lt;p&gt;我们的etl完成后数据放在hbase中，然后我们需要进行统计分析，结果放在mysql，首先是建立mysql表格，我们的表格统一放在数据库report下面，首先建库，然后按照下面的语句依次建表：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;DROP TABLE IF EXISTS `stats_user`;
CREATE TABLE `stats_user` (
  `date_dimension_id` int(11) NOT NULL,
  `platform_dimension_id` int(11) NOT NULL,
  `active_users` int(11) DEFAULT &#39;0&#39; COMMENT &#39;活跃用户数&#39;,
  `new_install_users` int(11) DEFAULT &#39;0&#39; COMMENT &#39;新增用户数&#39;,
  `total_install_users` int(11) DEFAULT &#39;0&#39; COMMENT &#39;总用户数&#39;,
  `sessions` int(11) DEFAULT &#39;0&#39; COMMENT &#39;会话个数&#39;,
  `sessions_length` int(11) DEFAULT &#39;0&#39; COMMENT &#39;会话长度&#39;,
  `total_members` int(11) unsigned DEFAULT &#39;0&#39; COMMENT &#39;总会员数&#39;,
  `active_members` int(11) unsigned DEFAULT &#39;0&#39; COMMENT &#39;活跃会员数&#39;,
  `new_members` int(11) unsigned DEFAULT &#39;0&#39; COMMENT &#39;新增会员数&#39;,
  `created` date DEFAULT NULL,
  PRIMARY KEY (`platform_dimension_id`,`date_dimension_id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8 ROW_FORMAT=COMPACT COMMENT=&#39;统计用户基本信息的统计表&#39;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这只是一个表格，由于我们的表格太多，这里不展示，我们会将所有数据库的建表语句放在文件中，大家可以参考，最终如图：
&lt;img src=&#34;../images/2017-06-21-16-55-12.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;4-运行统计任务&#34;&gt;(4).运行统计任务&lt;/h4&gt;

&lt;p&gt;新用户点击分析任务放在&lt;code&gt;com.hongya.transformer.mr.nu.NewInstallUserRunner&lt;/code&gt;中，大家可以查看代码，然后我们修改一下配置，主要是mysql的用户名密码，在DimensionConverterImpl中，另外我们还有查看核对 src/transformer-env.xml 下的内容：
如图，修改用户名密码为你的mysql设置：
&lt;img src=&#34;../images/2017-06-21-16-58-32.jpg&#34; alt=&#34;&#34; /&gt;
修改zookeeper地址和运行的起始日期，你可以设置的小一点：
&lt;img src=&#34;../images/2017-06-21-16-57-33.jpg&#34; alt=&#34;&#34; /&gt;
然后我们点击运行，我们就可以根据日志看到map和reduce执行的过程:
&lt;img src=&#34;../images/2017-06-21-17-02-13.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;执行完成后，我们查看我们刚刚的mysql的report库的stats_device_browser和stats_user表格：
&lt;img src=&#34;../images/2017-06-21-17-03-45.jpg&#34; alt=&#34;&#34; /&gt;
当然我们还可以查看dimension_browser等其他表格。
程序执行成功。&lt;/p&gt;

&lt;h3 id=&#34;4-数据转移系统&#34;&gt;4.数据转移系统&lt;/h3&gt;

&lt;p&gt;数据转移系统我们使用sqoop，由于我们的部分mapreduce任务每次运行的结果都放在hadoop或者Hbase上面，我们可能需要手动将关键指标转移到关系型数据库，然后编写代码进行展示。但是在这里，我们都写进了mysql，就暂时不适用sqoop了，也是为了减轻大家的负担。&lt;/p&gt;

&lt;h3 id=&#34;5-数据展示&#34;&gt;5.数据展示&lt;/h3&gt;

&lt;p&gt;数据展示我们使用 jquery + Echart吧。真正项目的echart展示部分一般不需要我们管，会有专门的前端高手负责，所以我们就简单的做一下吧。以浏览器维度为例，我们直接写sql语句&lt;code&gt;select name,count(*) from dimension_browser group by browser_name&lt;/code&gt;，将结果文件写到echart的option属性中：
&lt;img src=&#34;../images/2017-06-21-20-20-50.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;然后重新再浏览器段查看吧：
&lt;img src=&#34;../images/2017-06-21-20-21-47.jpg&#34; alt=&#34;&#34; /&gt;
当然我这是一种不可取的做法，因为这种方式显然是不符合企业生产环境的。真正的生产环境肯定是通过后台和数据库交互，通过ajax将数据传给前端展示，我们做的很敷衍，是因为这不是我们的重点。&lt;/p&gt;

&lt;h3 id=&#34;6-项目发布&#34;&gt;6.项目发布&lt;/h3&gt;

&lt;p&gt;这上面的所有步骤都完成了，就可以发布项目了，我们需要一套任务调度系统。azkaban是目前来说用的比较多的任务调度系统，我们推荐大家课后了解一下azkaban的安装使用。这里我们没法演示了。&lt;/p&gt;

&lt;h2 id=&#34;项目上线&#34;&gt;项目上线&lt;/h2&gt;

&lt;h3 id=&#34;1-基本步骤&#34;&gt;1.基本步骤&lt;/h3&gt;

&lt;p&gt;将整个项目设计好以后，就可以上线了，这里我们总结一下真实项目上线的过程。
1. 软件的安装
本地安装开发环境需要的东西，以及相关的依赖。服务器需要安装tomcat、nginx、zookeeper、hadoop、Hbase
2. 项目开发
这里的项目开发有三部分，服务端程序，日志分析程序，前端展示程序，其中日志分析程序我们只完成了new user 开发，剩下的业务由大家自己开发。前端展示程序我们只是简单展示，没有开发，这不是重点。
3. 搭建nginx服务器，监听80端口
nginx的安装和部署需要注意很多，安装完成后修改配置文件。
4. 启动flume，收集来自nginx的数据
flume配置完成后会收集日志文件的日志，按照时间放到hadoop上面。
5. 发布web项目
将项目打成war包，放到tomcat上，然后浏览器就可以访问，有访问时会向80端口发送数据。
6. 建表
新建hbase的表格和mysql表格
7. 定时启动hadoop任务
我们通过以前说过的方法将程序打成jar包，上传到linux，在Linux上通过&lt;code&gt;hadoop jar&lt;/code&gt;命令提交。
8. 启动展示任务
这里我们简单处理一下忽略掉。
9. 将运行和展示任务添加到定时任务进行调度
这部分比较复杂，需要专门的时间学习。&lt;/p&gt;

&lt;h3 id=&#34;2-自己动手发布程序&#34;&gt;2.自己动手发布程序&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;安装相关软件，我这里已经安装完成，大家自己检查安装。&lt;/li&gt;

&lt;li&gt;&lt;p&gt;安装hadoop、zookeeper、hbase、flume并配置
这部分不详细讲解，配置方法上面都有，配置好以后启动相应集群。启动命令分别为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;start-dfs.sh
start-yarn.sh
start-hbase.sh
bin/flume-ng agent -c ./conf -f ./conf/log.cfg -n agent 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;至少有这些java进程：
&lt;img src=&#34;../images/2017-06-21-21-38-57.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;安装nginx，启动nginx服务，修改nginx的配置文件，监听80端口，并且收集格式为： /log.gif 的url。
可以在浏览器访问linux的80端口：
&lt;img src=&#34;../images/2017-06-21-21-40-36.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;项目打war包，放到tomcat中。
打包之前，修改代码的url为你的linux节点加上log.gif
&lt;img src=&#34;../images/2017-06-21-21-57-51.jpg&#34; alt=&#34;&#34; /&gt;
然后打成war包，名字为：&lt;code&gt;taobaopayment.war&lt;/code&gt;，放在tomcat的webapps目录下，使员工startup.sh 启动tomcat，访问8080端口，然后访&lt;code&gt;http://node1:8080/taobaopayment/demo4.jsp&lt;/code&gt;：
如果这里一直在刷新，那么需要换一个浏览器：
&lt;img src=&#34;../images/2017-06-21-22-06-20.jpg&#34; alt=&#34;&#34; /&gt;
比如我用Safari浏览器，不断点击，产生数据：
&lt;img src=&#34;../images/2017-06-21-22-07-45.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;创建hbase和mysql表格
hbase创建event_logs表格，info列族，
mysql建表语句文件里有。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;执行mapreduce的ETL任务
你可以选择打jar包，或者在本地执行，执行的主类是：&lt;code&gt;com.hongya.etl.mr.ald.AnalyserLogDataRunner&lt;/code&gt;
需要修改zookeeper配置和输入路径，如果在集群上运行，这个输入路径是前面配置的flume手机日志的路径。
&lt;img src=&#34;../images/2017-06-21-22-12-59.jpg&#34; alt=&#34;&#34; /&gt;
执行完成后，可以去hbase查看数据。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;运行分析程序
分析程序基于我们刚刚的结果，主类为：&lt;code&gt;com.hongya.transformer.mr.nu.NewInstallUserRunner&lt;/code&gt;，运行之前需要在&lt;code&gt;DimensionConverterImpl&lt;/code&gt;类中设置mysql的连接信息。
&lt;img src=&#34;../images/2017-06-21-22-16-00.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;查看mysql数据库的结果，并展示
查看mysql的结果：
&lt;img src=&#34;../images/2017-06-21-22-16-54.jpg&#34; alt=&#34;&#34; /&gt;
数据展示模块，需要使用Echart，脱离了我们的实验主题，我们简单模拟，访问浏览器的：&lt;code&gt;http://node1:8080/taobaopayment/showUser.jsp&lt;/code&gt;和&lt;code&gt;http://node1:8080/taobaopayment/showBrowser.jsp&lt;/code&gt;
&lt;img src=&#34;../images/2017-06-21-22-18-47.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
      
    </item>
    
    <item>
      <title>On Disk IO</title>
      <link>https://dengziming.github.io/post/%E6%95%B0%E6%8D%AE%E5%BA%93/%E7%A3%81%E7%9B%98io/</link>
      <pubDate>Thu, 22 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/%E6%95%B0%E6%8D%AE%E5%BA%93/%E7%A3%81%E7%9B%98io/</guid>
      
        <description>

&lt;p&gt;文章来源：&lt;a href=&#34;https://medium.com/databasss/on-disk-io-part-1-flavours-of-io-8e1ace1de017&#34;&gt;https://medium.com/databasss/on-disk-io-part-1-flavours-of-io-8e1ace1de017&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;了解 IO 原理有助于工作。Network IO 相关问题经常讨论，然而和磁盘相关的问题很少讨论。一方面实现上，网络 IO 更为复杂的巧妙，Filesystem 相关 tools 很少。
另一方面，大家都使用 databases 作为存储系统，和底层打交道的事情交给了数据库设计人员，但是了解相关资料 依然重要。&lt;/p&gt;

&lt;p&gt;目录：
- Syscalls: open, write, read, fsync, sync, close
- Standard IO: fopen, fwrite, fread, fflush, fclose
- Vectored IO: writev, readv
- Memory mapped IO: open, mmap, msync, munmap&lt;/p&gt;

&lt;h2 id=&#34;buffered-io&#34;&gt;Buffered IO&lt;/h2&gt;

&lt;p&gt;“buffering”  总是让人困惑，当使用 Standard IO，可以选择  full and line buffering 或者 out from any buffering whatsoever，
这些其实和我们要讨论的 Kernel buffering (Page Cache)  没什么关系。你可以吧上面的称之为 caching。&lt;/p&gt;

&lt;h2 id=&#34;sector-block-page&#34;&gt;Sector/Block/Page&lt;/h2&gt;

&lt;p&gt;块状设备（Block Device ）是为读取提供  HDDs or SSDs 等硬件设备提供 buffered access 的特殊文件类型。
Block Devices 工作于 Sectors（一组相邻的 bytes） 之上，Most disk devices have a sector size of 512 bytes。
Sector 是 block device 之间数据传输的最小单元，读取数据不可能比 Sector 更小，但是却可以同时读 multiple adjacent segments。
File System  最小读取单元就是 Block，Block就是一组相邻的sectors，block sizes 一般为 512, 1024, 2048 and 4096 bytes。
通常 IO操作会通过 Virtual Memory，将 filesystem blocks 缓存到 memory 中作为buffer提供中间操作，
Virtual Memory 又和 pages 一起工作，pages 会 map 到 blocks，Typical page size is 4096 bytes。&lt;/p&gt;

&lt;p&gt;总结来说，Virtual Memory 和 pages 一起工作，map 到 Filesystem blocks，blocks 又 map 到 Block Device sectors。&lt;/p&gt;

&lt;h2 id=&#34;standard-io&#34;&gt;Standard IO&lt;/h2&gt;

&lt;p&gt;通过 read() and write()  方法操作文件系统。&lt;/p&gt;

&lt;p&gt;reading: 首先访问 Page Cache，如果 Page Cache 中没有需要的数据，将会触发 Page Fault 然后将需要的数据 paged in。所以读取 unmapped 文件会慢一些，尽管对用户是透明的。&lt;/p&gt;

&lt;p&gt;writes: buffer contents 首先写到 Page Cache，数据并不会立马写进磁盘，真正的 hardware write 是当 Kernel 决定进行  writeback of the dirty page.&lt;/p&gt;

&lt;h2 id=&#34;page-cache&#34;&gt;Page Cache&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/torvalds/linux/blob/master/include/linux/buffer_head.h&#34;&gt;https://github.com/torvalds/linux/blob/master/include/linux/buffer_head.h&lt;/a&gt;
将最近访问过的 fragments 存储起来，read() and write() 操作并不会触发磁盘操作而是通过Page Cache 。&lt;/p&gt;

&lt;p&gt;读的时候，读的时候会查看 Page Cache，如果有的话直接给用户，没有的话先加载再给用户，如果满了，least recently used pages 会被刷到磁盘，并且 evicted from cache。&lt;/p&gt;

&lt;p&gt;写操作仅仅是将数据放到 Page Cache，marking the written page as dirty。后续会有 flush or writeback 的进程将数据写到磁盘。&lt;/p&gt;

&lt;p&gt;标记为dirty 的page会被flush到磁盘，这个过程被称作 writeback。writeback 有缺点，例如 queuing up IO requests。&lt;/p&gt;

&lt;p&gt;Page Cache 的理论是 时间本地性原则，也就是刚刚被访问的内容很有可能被再次访问。当然还有空间本地性原则，也就是被访问的数据附近的数据很有可能被访问，prefetch 操作就是机遇这个原则。&lt;/p&gt;

&lt;p&gt;Page Cache ，保存了最近访问的和将要访问的 file chunks，read-write-read 的操作可以直接通过缓存，加快了速度。&lt;/p&gt;

&lt;h2 id=&#34;delaying-errors&#34;&gt;Delaying Errors&lt;/h2&gt;

&lt;p&gt;既然不会立即写到磁盘，就会有 可能突然出错没有写到磁盘的问题，可以查看： &lt;a href=&#34;https://lwn.net/Articles/457667/&#34;&gt;https://lwn.net/Articles/457667/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;direct-io&#34;&gt;Direct IO&lt;/h2&gt;

&lt;p&gt;当我们不想用 Kernel Page Cache。打开文件的时候使用 O_DIRECT 标志，直接读取绕过 Page Cache，写也是直接写到磁盘。
大部分使用 Direct IO 的application 都会导致性能的 下降，但是高手可以通过 fine-grained 操作提高性能，因为会实现自己的缓存，例如 PostgreSQL and MySQL use Direct IO 。&lt;/p&gt;

&lt;p&gt;例如 write-ahead-log，一般不会立即查看。同时使用 O_DIRECT 和 PageCache 打开文件会带来很多问题，不鼓励。&lt;/p&gt;

&lt;h2 id=&#34;block-alignment&#34;&gt;Block Alignment&lt;/h2&gt;

&lt;p&gt;使用 Direct IO 的时候，最好让操作都对准 sector 的边界上。也就是说，每个操作的 starting offset 是 512 的倍数，缓存大小也是 512 的倍数，
Crossing segment boundary 会导致加载多个sectors。&lt;/p&gt;

&lt;p&gt;Page Cache 会在内存自动对齐，所以不需要。&lt;/p&gt;

&lt;h2 id=&#34;memory-mapping&#34;&gt;Memory Mapping&lt;/h2&gt;

&lt;p&gt;Memory Mapping (mmap) 让你可以像整个文件都放进内存了一样访问文件。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>数据库底层原理</title>
      <link>https://dengziming.github.io/post/%E6%95%B0%E6%8D%AE%E5%BA%93/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86/</link>
      <pubDate>Thu, 22 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dengziming.github.io/post/%E6%95%B0%E6%8D%AE%E5%BA%93/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86/</guid>
      
        <description>&lt;p&gt;我不太喜欢一直把技术当黑箱使用，需要花时间理解底层原理。参考资料：&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://coding-geek.com/how-databases-work/&#34;&gt;http://coding-geek.com/how-databases-work/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://blog.jobbole.com/100349/&#34;&gt;http://blog.jobbole.com/100349/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You can google by yourself “how does a relational database work” to see how few results there are. Moreover, those articles are short.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://coding-geek.com/how-does-a-hashmap-work-in-java/&#34;&gt;http://coding-geek.com/how-does-a-hashmap-work-in-java/&lt;/a&gt;&lt;/p&gt;
</description>
      
    </item>
    
  </channel>
</rss>